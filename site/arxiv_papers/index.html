
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.39">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>持续改进自主现实世界强化学习中的移动操作</td>
<td>我们提出了一种完全自主的现实世界强化学习框架，用于移动操作，该框架能够在无需广泛仪器或人工监督的情况下学习策略。这一成果的实现得益于以下三点：1) 任务相关的自主性，它引导探索朝向物体交互，并防止在目标状态附近停滞；2) 通过利用行为先验中的基本任务知识，实现高效策略学习；3) 制定结合人类可解释语义信息与低层次、细粒度观察的通用奖励。我们的研究表明，该方法使Spot机器人能够在四个具有挑战性的移动操作任务中持续提升表现，平均成功率达到80%，比现有方法提高了3-4个百分点。相关视频可在https://continual-mobile-manip.github.io/查看。</td>
<td>Russell Mendonca</td>
<td><a href="http://arxiv.org/pdf/2409.20568v1">PDF</a></td>
<td>N/A</td>
<td>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</td>
<td>We present a fully autonomous real-world RL framework for mobile manipulation that can learn policies without extensive instrumentation or human supervision. This is enabled by 1) task-relevant autonomy, which guides exploration towards object interactions and prevents stagnation near goal states, 2) efficient policy learning by leveraging basic task knowledge in behavior priors, and 3) formulating generic rewards that combine human-interpretable semantic information with low-level, fine-grained observations. We demonstrate that our approach allows Spot robots to continually improve their performance on a set of four challenging mobile manipulation tasks, obtaining an average success rate of 80% across tasks, a 3-4 improvement over existing approaches. Videos can be found at https://continual-mobile-manip.github.io/</td>
</tr>
<tr>
<td>MM1.5：多模态大语言模型微调中的方法、分析与洞察</td>
<td>我们推出了MM1.5，这是一系列新的多模态大型语言模型（MLLMs），旨在增强在文本丰富的图像理解、视觉指称和定位以及多图像推理方面的能力。基于MM1架构，MM1.5采用了以数据为中心的模型训练方法，系统地探索了在整个模型训练生命周期中多样化数据混合的影响。这包括高质量的OCR数据和合成字幕用于持续预训练，以及优化的视觉指令调优数据混合用于有监督的微调。我们的模型参数规模从1B到30B不等，涵盖了密集型和专家混合（MoE）变体，并展示了即使在较小规模（1B和3B）下，精心设计的数据筛选和训练策略也能带来强大的性能。此外，我们引入了两个专门的变体：MM1.5-Video，专为视频理解设计，以及MM1.5-UI，专为移动用户界面理解定制。通过广泛的实证研究和消融实验，我们详细揭示了训练过程和决策，这些都构成了我们最终设计的基础，为未来MLLM开发研究提供了宝贵的指导。</td>
<td>Haotian Zhang</td>
<td><a href="http://arxiv.org/pdf/2409.20566v1">PDF</a></td>
<td>N/A</td>
<td>MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning</td>
<td>We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development.</td>
</tr>
<tr>
<td>排名优于评分：迈向可靠且稳健的LLM生成医学解释性论证自动化评估</td>
<td>评估大型语言模型（LLM）生成的文本已成为一个关键挑战，特别是在医学领域等特定领域中。本研究引入了一种新颖的评估方法，用于LLM生成的医学解释性论证，该方法依赖于代理任务和排名，以紧密符合人类评估标准，克服了通常作为评判者的LLM中存在的偏见。我们证明，所提出的评估者对对抗性攻击具有鲁棒性，包括对非论证性文本的评估。此外，训练评估者所需的人工编写的论证被最小化到每个代理任务仅一个示例。通过检查多个LLM生成的论证，我们建立了一种方法，用于确定代理任务是否适合评估LLM生成的医学解释性论证，仅需五个示例和两位人类专家。</td>
<td>Iker De la Iglesia</td>
<td><a href="http://arxiv.org/pdf/2409.20565v1">PDF</a></td>
<td>N/A</td>
<td>Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments</td>
<td>Evaluating LLM-generated text has become a key challenge, especially in domain-specific contexts like the medical field. This work introduces a novel evaluation methodology for LLM-generated medical explanatory arguments, relying on Proxy Tasks and rankings to closely align results with human evaluation criteria, overcoming the biases typically seen in LLMs used as judges. We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text. Additionally, the human-crafted arguments needed to train the evaluators are minimized to just one example per Proxy Task. By examining multiple LLM-generated arguments, we establish a methodology for determining whether a Proxy Task is suitable for evaluating LLM-generated medical explanatory arguments, requiring only five examples and two human experts.</td>
</tr>
<tr>
<td>SpaceMesh：一种用于学习流形表面网格的连续表示</td>
<td>网格在视觉计算和模拟中无处不在，然而大多数现有的机器学习技术仅间接表示网格，例如作为标量场的水平集或模板的变形，或者作为缺乏局部结构的无序三角形汤。这项工作提出了一种方案，直接生成具有复杂连通性的流形多边形网格作为神经网络的输出。我们的关键创新在于在每个网格顶点定义一个连续的潜在连通性空间，这隐含了离散网格。特别是，我们的顶点嵌入在半边网格表示中生成循环邻居关系，这保证了边流形性并能够表示一般的多边形网格。这种表示非常适合机器学习和随机优化，不受连通性或拓扑的限制。我们首先探索了这种表示的基本属性，然后使用它来拟合来自大型数据集的网格分布。生成的模型生成的网格具有从数据集群体中学习的镶嵌结构，细节简洁且网格元素质量高。在应用中，这种方法不仅从生成模型中产生高质量的输出，还直接实现了学习具有挑战性的几何处理任务，如网格修复。</td>
<td>Tianchang Shen</td>
<td><a href="http://arxiv.org/pdf/2409.20562v1">PDF</a></td>
<td>N/A</td>
<td>SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes</td>
<td>Meshes are ubiquitous in visual computing and simulation, yet most existing machine learning techniques represent meshes only indirectly, e.g. as the level set of a scalar field or deformation of a template, or as a disordered triangle soup lacking local structure. This work presents a scheme to directly generate manifold, polygonal meshes of complex connectivity as the output of a neural network. Our key innovation is to define a continuous latent connectivity space at each mesh vertex, which implies the discrete mesh. In particular, our vertex embeddings generate cyclic neighbor relationships in a halfedge mesh representation, which gives a guarantee of edge-manifoldness and the ability to represent general polygonal meshes. This representation is well-suited to machine learning and stochastic optimization, without restriction on connectivity or topology. We first explore the basic properties of this representation, then use it to fit distributions of meshes from large datasets. The resulting models generate diverse meshes with tessellation structure learned from the dataset population, with concise details and high-quality mesh elements. In applications, this approach not only yields high-quality outputs from generative models, but also enables directly learning challenging geometry processing tasks such as mesh repair.</td>
</tr>
<tr>
<td>LaMMA-P：基于语言模型驱动的PDDL规划器实现多智能体长时任务分配与规划的通用性</td>
<td>语言模型（LMs）具备强大的自然语言理解能力，使其能够有效地将人类指令转化为简单机器人任务的详细计划。然而，处理长时程任务，特别是在合作异构机器人团队的子任务识别和分配方面，仍然是一个重大挑战。为了解决这一问题，我们提出了一种语言模型驱动的多智能体PDDL规划器（LaMMA-P），这是一种新颖的多智能体任务规划框架，在长时程任务上实现了最先进的性能。LaMMA-P结合了LMs的推理能力和传统启发式搜索规划器的优势，实现了高成功率和效率，并在任务间展现了强大的泛化能力。此外，我们创建了MAT-THOR，这是一个全面的基准测试，基于AI2-THOR环境，包含两种不同复杂程度的家居任务。实验结果表明，LaMMA-P的成功率比现有的基于LM的多智能体规划器高出105%，效率高出36%。本工作的实验视频、代码、数据集以及各模块中使用的详细提示均可在https://lamma-p.github.io获取。</td>
<td>Xiaopan Zhang</td>
<td><a href="http://arxiv.org/pdf/2409.20560v1">PDF</a></td>
<td>N/A</td>
<td>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</td>
<td>Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105% higher success rate and 36% higher efficiency than existing LM-based multi-agent planners. The experimental videos, code, and datasets of this work as well as the detailed prompts used in each module are available at https://lamma-p.github.io.</td>
</tr>
<tr>
<td>监督多模态裂变学习</td>
<td>从多模态数据集中学习可以利用互补信息，并在预测任务中提高性能。处理高维数据集中特征相关性的常用策略是潜在变量方法。已经提出了几种用于多模态数据集的潜在变量方法。然而，这些方法要么侧重于提取所有模态共享的成分，要么侧重于提取共享成分以及每个模态特有的个体成分。为了填补这一空白，我们提出了一种多模态裂变学习（Multi-Modal Fission Learning, MMFL）模型，该模型同时识别多模态数据集特征中潜在的全局联合、部分联合和个体成分。与现有的潜在变量方法不同，MMFL利用响应变量的监督来识别具有预测性的潜在成分，并且可以自然地扩展以整合不完整的多模态数据。通过模拟研究，我们证明了MMFL在完整和不完整模态设置下均优于各种现有的多模态算法。我们将MMFL应用于一个实际案例研究，使用来自阿尔茨海默病神经影像学倡议（Alzheimers Disease Neuroimaging Initiative, ADNI）数据集的多模态神经影像学和基因组学数据，进行阿尔茨海默病的早期预测。与现有方法相比，MMFL提供了更准确的预测，并更好地洞察了模态内和模态间的相关性。</td>
<td>Lingchao Mao</td>
<td><a href="http://arxiv.org/pdf/2409.20559v1">PDF</a></td>
<td>N/A</td>
<td>Supervised Multi-Modal Fission Learning</td>
<td>Learning from multimodal datasets can leverage complementary information and improve performance in prediction tasks. A commonly used strategy to account for feature correlations in high-dimensional datasets is the latent variable approach. Several latent variable methods have been proposed for multimodal datasets. However, these methods either focus on extracting the shared component across all modalities or on extracting both a shared component and individual components specific to each modality. To address this gap, we propose a Multi-Modal Fission Learning (MMFL) model that simultaneously identifies globally joint, partially joint, and individual components underlying the features of multimodal datasets. Unlike existing latent variable methods, MMFL uses supervision from the response variable to identify predictive latent components and has a natural extension for incorporating incomplete multimodal data. Through simulation studies, we demonstrate that MMFL outperforms various existing multimodal algorithms in both complete and incomplete modality settings. We applied MMFL to a real-world case study for early prediction of Alzheimers Disease using multimodal neuroimaging and genomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset. MMFL provided more accurate predictions and better insights into within- and across-modality correlations compared to existing methods.</td>
</tr>
<tr>
<td>实际代码生成中的LLM幻觉：现象、机制与缓解</td>
<td>代码生成旨在根据输入需求自动生成代码，显著提升开发效率。近期基于大型语言模型（LLMs）的方法展示了令人鼓舞的结果，并彻底改变了代码生成任务。尽管性能表现出色，LLMs在生成内容时常常出现幻觉，特别是在实际开发过程中需要处理复杂上下文依赖的代码生成场景中。虽然先前的研究已经分析了基于LLM的代码生成中的幻觉现象，但这些研究仅限于独立函数的生成。本文通过实证研究，探讨了在更实际和复杂的开发上下文中，即在仓库级别生成场景下，LLM幻觉的现象、机制及其缓解方法。首先，我们手动检查了六个主流LLMs的代码生成结果，建立了LLM生成代码的幻觉分类。接着，我们详细阐述了幻觉现象，分析了其在不同模型中的分布。然后，我们分析了幻觉的原因，并识别出四个可能导致幻觉的因素。最后，我们提出了一种基于RAG的缓解方法，该方法在所有研究的LLMs中均表现出一致的有效性。包含代码、数据和实验结果的复制包可在https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination获取。</td>
<td>Ziyao Zhang</td>
<td><a href="http://arxiv.org/pdf/2409.20550v1">PDF</a></td>
<td>N/A</td>
<td>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</td>
<td>Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs. The replication package including code, data, and experimental results is available at https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination</td>
</tr>
<tr>
<td>退火流生成模型：面向高维和多模态分布的采样</td>
<td>从高维、多模态分布中采样仍然是统计贝叶斯推断和基于物理的机器学习等领域中的一个基本挑战。本文提出了一种名为“退火流”（Annealing Flow, AF）的方法，这是一种基于连续归一化流的设计，旨在从高维和多模态分布中进行采样。其核心思想是学习一个由退火引导的连续归一化流传输映射，使样本从易于采样的分布过渡到目标分布，从而促进在高维空间中对模式的有效探索。与许多现有方法不同，AF的训练不依赖于目标分布的样本。AF确保了有效且平衡的模式探索，实现了样本大小和维度的线性复杂度，并避免了低效的混合时间。通过在各种具有挑战性的分布和真实世界数据集上的广泛实验，特别是在高维和多模态设置中，我们展示了AF相对于最先进方法的优越性能。我们还强调了AF在采样最不利分布方面的潜力。</td>
<td>Dongze Wu</td>
<td><a href="http://arxiv.org/pdf/2409.20547v1">PDF</a></td>
<td>N/A</td>
<td>Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions</td>
<td>Sampling from high-dimensional, multi-modal distributions remains a fundamental challenge across domains such as statistical Bayesian inference and physics-based machine learning. In this paper, we propose Annealing Flow (AF), a continuous normalizing flow-based approach designed to sample from high-dimensional and multi-modal distributions. The key idea is to learn a continuous normalizing flow-based transport map, guided by annealing, to transition samples from an easy-to-sample distribution to the target distribution, facilitating effective exploration of modes in high-dimensional spaces. Unlike many existing methods, AF training does not rely on samples from the target distribution. AF ensures effective and balanced mode exploration, achieves linear complexity in sample size and dimensions, and circumvents inefficient mixing times. We demonstrate the superior performance of AF compared to state-of-the-art methods through extensive experiments on various challenging distributions and real-world datasets, particularly in high-dimensional and multi-modal settings. We also highlight the potential of AF for sampling the least favorable distributions.</td>
</tr>
<tr>
<td>扩展本体感受-视觉学习与异构预训练变压器</td>
<td>当前训练通用机器人模型的一个障碍是异质性。以往的机器人学习方法通常为一项特定任务收集数据以训练一个特定的实体，这种方法成本高且容易过拟合。本研究探讨了通过在不同实体和任务的大规模机器人数据上进行异质性预训练来学习策略表示的问题。我们提出了异质性预训练变压器（HPT），它预训练一个大型、可共享的策略神经网络主干，以学习与任务和实体无关的共享表示。这种通用架构将来自不同实体的特定本体感受和视觉输入对齐为一系列短标记，然后处理这些标记以映射到不同任务的机器人控制。利用最近的大规模多实体真实世界机器人数据集以及模拟、部署的机器人和人类视频数据集，我们研究了跨异质性的策略预训练。我们进行了实验，研究了训练目标的扩展行为，涉及多达52个数据集。HPT在多个模拟基准和真实世界环境中，在未见任务上的微调策略性能比几个基线方法高出超过20%。有关代码和视频，请参见项目网站（https://liruiw.github.io/hpt/）。</td>
<td>Lirui Wang</td>
<td><a href="http://arxiv.org/pdf/2409.20537v1">PDF</a></td>
<td>N/A</td>
<td>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</td>
<td>One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos.</td>
</tr>
<tr>
<td>信用评分中负责任机器学习的最佳实践</td>
<td>机器学习在信用评分中的广泛应用显著提升了风险评估和决策制定的水平。然而，这也引发了关于这些自动化系统中潜在偏见、歧视和缺乏透明度的担忧。本教程论文进行了非系统性文献回顾，旨在指导在信用评分中开发负责任的机器学习模型的最佳实践，重点关注公平性、拒绝推断和可解释性。我们讨论了定义、指标和技术，以减轻偏见并确保不同群体间的公平结果。此外，我们通过探索结合被拒绝贷款申请信息的拒绝推断方法，解决了数据代表性不足的问题。最后，我们强调了透明度和可解释性在信用模型中的重要性，讨论了提供决策过程洞察力并使个人能够理解并可能提升其信用价值的技术。通过采用这些最佳实践，金融机构可以在坚持伦理和负责任的贷款实践的同时，充分利用机器学习的力量。</td>
<td>Giovani Valdrighi</td>
<td><a href="http://arxiv.org/pdf/2409.20536v1">PDF</a></td>
<td>N/A</td>
<td>Best Practices for Responsible Machine Learning in Credit Scoring</td>
<td>The widespread use of machine learning in credit scoring has brought significant advancements in risk assessment and decision-making. However, it has also raised concerns about potential biases, discrimination, and lack of transparency in these automated systems. This tutorial paper performed a non-systematic literature review to guide best practices for developing responsible machine learning models in credit scoring, focusing on fairness, reject inference, and explainability. We discuss definitions, metrics, and techniques for mitigating biases and ensuring equitable outcomes across different groups. Additionally, we address the issue of limited data representativeness by exploring reject inference methods that incorporate information from rejected loan applications. Finally, we emphasize the importance of transparency and explainability in credit models, discussing techniques that provide insights into the decision-making process and enable individuals to understand and potentially improve their creditworthiness. By adopting these best practices, financial institutions can harness the power of machine learning while upholding ethical and responsible lending practices.</td>
</tr>
<tr>
<td>端到端保形校准用于不确定性下的优化</td>
<td>机器学习能够显著提升在广泛领域中不确定情况下的决策性能。然而，确保鲁棒性保障需要经过良好校准的不确定性估计，这在高容量预测模型（如深度神经网络）中可能难以实现。此外，在高维环境中，可能存在许多有效的不确定性估计，每个都有其自身的性能特征——即并非所有不确定性对于下游决策都具有同等价值。为了解决这一问题，本文开发了一个端到端的框架，用于学习条件鲁棒优化中的不确定性估计，该框架通过保形预测提供了鲁棒性和校准保障。此外，我们提出使用部分输入凸神经网络来表示任意凸不确定性集合，这些网络作为我们框架的一部分进行学习。我们的方法在能源存储套利和投资组合优化等具体应用中，持续优于两阶段估计后优化的基线方法。</td>
<td>Christopher Yeh</td>
<td><a href="http://arxiv.org/pdf/2409.20534v1">PDF</a></td>
<td>N/A</td>
<td>End-to-End Conformal Calibration for Optimization Under Uncertainty</td>
<td>Machine learning can significantly improve performance for decision-making under uncertainty in a wide range of domains. However, ensuring robustness guarantees requires well-calibrated uncertainty estimates, which can be difficult to achieve in high-capacity prediction models such as deep neural networks. Moreover, in high-dimensional settings, there may be many valid uncertainty estimates, each with their own performance profile - i.e., not all uncertainty is equally valuable for downstream decision-making. To address this problem, this paper develops an end-to-end framework to learn the uncertainty estimates for conditional robust optimization, with robustness and calibration guarantees provided by conformal prediction. In addition, we propose to represent arbitrary convex uncertainty sets with partially input-convex neural networks, which are learned as part of our framework. Our approach consistently improves upon two-stage estimate-then-optimize baselines on concrete applications in energy storage arbitrage and portfolio optimization.</td>
</tr>
<tr>
<td>双编码器生成对抗网络反演用于从单张图像进行高保真3D头部重建</td>
<td>3D GAN inversion旨在将单张图像投影到3D生成对抗网络（GAN）的潜在空间中，从而实现3D几何重建。虽然现有的编码器在3D GAN反转中取得了良好的效果，但它们主要基于EG3D构建，该模型擅长合成近正面视角的图像，但在从多样视角合成全面的3D场景方面存在局限。与现有方法不同，我们提出了一种基于PanoHead的新框架，该框架擅长从360度视角合成图像。为了实现输入图像的真实感3D建模，我们引入了一个双编码器系统，专门用于高保真重建和从不同视角的真实感生成。同时，我们提出了一种在三平面域上的拼接框架，以从两者中获得最佳预测。为了实现无缝拼接，尽管两个编码器专用于不同任务，它们必须输出一致的结果。为此，我们使用专门的损失仔细训练这些编码器，包括基于我们新颖的遮挡感知三平面判别器的对抗损失。实验表明，我们的方法在质量和数量上都优于现有的编码器训练方法。请访问项目页面：https://berkegokmen1.github.io/dual-enc-3d-gan-inv。</td>
<td>Bahri Batuhan Bilecen</td>
<td><a href="http://arxiv.org/pdf/2409.20530v1">PDF</a></td>
<td>N/A</td>
<td>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images</td>
<td>3D GAN inversion aims to project a single image into the latent space of a 3D Generative Adversarial Network (GAN), thereby achieving 3D geometry reconstruction. While there exist encoders that achieve good results in 3D GAN inversion, they are predominantly built on EG3D, which specializes in synthesizing near-frontal views and is limiting in synthesizing comprehensive 3D scenes from diverse viewpoints. In contrast to existing approaches, we propose a novel framework built on PanoHead, which excels in synthesizing images from a 360-degree perspective. To achieve realistic 3D modeling of the input image, we introduce a dual encoder system tailored for high-fidelity reconstruction and realistic generation from different viewpoints. Accompanying this, we propose a stitching framework on the triplane domain to get the best predictions from both. To achieve seamless stitching, both encoders must output consistent results despite being specialized for different tasks. For this reason, we carefully train these encoders using specialized losses, including an adversarial loss based on our novel occlusion-aware triplane discriminator. Experiments reveal that our approach surpasses the existing encoder training methods qualitatively and quantitatively. Please visit the project page: https://berkegokmen1.github.io/dual-enc-3d-gan-inv.</td>
</tr>
<tr>
<td>正式验证的物理信息神经控制李雅普诺夫函数</td>
<td>控制李雅普诺夫函数是设计和分析非线性系统稳定控制器的关键工具。然而，构建这些函数仍然是一个重大挑战。在本文中，我们研究了基于物理信息的学习和神经网络控制李雅普诺夫函数的正式验证。这些神经网络解决了一个变换后的哈密顿-雅可比-贝尔曼方程，该方程通过使用庞特里亚金最大原理生成的数据进行了增强。类似于Zubov方程如何表征自治系统的吸引域，该方程表征了受控系统的零可控集。数值例子表明，这种原则性的神经网络控制李雅普诺夫函数学习方法优于其他方法，如平方和和有理控制李雅普诺夫函数。作为中间步骤，我们还展示了关于二次控制李雅普诺夫函数的正式验证结果，这些函数在可满足性模理论求解器的帮助下，与更复杂的方法相比，表现出色，并且能够高效地生成零可控性的全局证书。</td>
<td>Jun Liu</td>
<td><a href="http://arxiv.org/pdf/2409.20528v1">PDF</a></td>
<td>N/A</td>
<td>Formally Verified Physics-Informed Neural Control Lyapunov Functions</td>
<td>Control Lyapunov functions are a central tool in the design and analysis of stabilizing controllers for nonlinear systems. Constructing such functions, however, remains a significant challenge. In this paper, we investigate physics-informed learning and formal verification of neural network control Lyapunov functions. These neural networks solve a transformed Hamilton-Jacobi-Bellman equation, augmented by data generated using Pontryagin's maximum principle. Similar to how Zubov's equation characterizes the domain of attraction for autonomous systems, this equation characterizes the null-controllability set of a controlled system. This principled learning of neural network control Lyapunov functions outperforms alternative approaches, such as sum-of-squares and rational control Lyapunov functions, as demonstrated by numerical examples. As an intermediate step, we also present results on the formal verification of quadratic control Lyapunov functions, which, aided by satisfiability modulo theories solvers, can perform surprisingly well compared to more sophisticated approaches and efficiently produce global certificates of null-controllability.</td>
</tr>
<tr>
<td>母语西班牙语中的词义消歧：全面的词汇评估资源</td>
<td>人类语言虽然旨在传达意义，但其本身具有固有的模糊性。这种模糊性对语音和语言处理提出了挑战，但同时也具有重要的交流功能。高效地解决模糊性既是一种期望，也是一种必要特征。在特定语境下，单词的词义可以通过词义消歧（WSD）算法自动确定，这些算法依赖于外部知识，但这些知识通常有限且偏向于英语。在将内容适配到其他语言时，自动化翻译往往不够准确，需要高度专业的专家验证来确保准确性和理解。本研究通过引入一种新的西班牙语WSD资源来解决以往的局限性。该资源包括一个语义库存和一个词汇数据集，这些数据来源于西班牙皇家语言学院维护的《西班牙语词典》。我们还回顾了当前的西班牙语资源，并通过最先进的系统报告了它们的指标。</td>
<td>Pablo Ortega</td>
<td><a href="http://arxiv.org/pdf/2409.20524v1">PDF</a></td>
<td>N/A</td>
<td>Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource</td>
<td>Human language, while aimed at conveying meaning, inherently carries ambiguity. It poses challenges for speech and language processing, but also serves crucial communicative functions. Efficiently solve ambiguity is both a desired and a necessary characteristic. The lexical meaning of a word in context can be determined automatically by Word Sense Disambiguation (WSD) algorithms that rely on external knowledge often limited and biased toward English. When adapting content to other languages, automated translations are frequently inaccurate and a high degree of expert human validation is necessary to ensure both accuracy and understanding. The current study addresses previous limitations by introducing a new resource for Spanish WSD. It includes a sense inventory and a lexical dataset sourced from the Diccionario de la Lengua Espa\~nola which is maintained by the Real Academia Espa\~nola. We also review current resources for Spanish and report metrics on them by a state-of-the-art system.</td>
</tr>
<tr>
<td>分布稳健的非动态强化学习的上下界</td>
<td>我们研究了策略训练和部署环境不同的离线强化学习（RL）。为了应对这种环境扰动，我们专注于在分布式鲁棒马尔可夫决策过程（DRMDPs）框架下学习对转移动态不确定性具有鲁棒性的策略，其中标称和扰动动态均为线性马尔可夫决策过程。我们提出了一种新颖的算法We-DRIVE-U，该算法具有平均次优性$\widetilde{\mathcal{O}}\big({d H \cdot \min {1/{\rho}, H}/\sqrt{K} }\big)$，其中$K$是剧集数，$H$是时间范围长度，$d$是特征维度，$\rho$是不确定性水平。这一结果将现有技术的水平提高了$\mathcal{O}(dH/\min{1/\rho,H})$。我们还构建了一个新的困难实例，并在此设置中推导出了第一个信息论下界，这表明我们的算法在任意不确定性水平$\rho\in(0,1]$下，几乎是最优的，误差在$\mathcal{O}(\sqrt{H})$以内。我们的算法还采用了“罕见切换”设计，因此只需要$\mathcal{O}(dH\log(1+H^2K))$次策略切换和$\mathcal{O}(d^2H\log(1+H^2K))$次对求解对偶优化问题的oracle调用，这显著提高了现有DRMDPs算法的计算效率，后者的策略切换和oracle复杂度均为$\mathcal{O}(K)$。</td>
<td>Zhishuai Liu</td>
<td><a href="http://arxiv.org/pdf/2409.20521v1">PDF</a></td>
<td>N/A</td>
<td>Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning</td>
<td>We study off-dynamics Reinforcement Learning (RL), where the policy training and deployment environments are different. To deal with this environmental perturbation, we focus on learning policies robust to uncertainties in transition dynamics under the framework of distributionally robust Markov decision processes (DRMDPs), where the nominal and perturbed dynamics are linear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that enjoys an average suboptimality $\widetilde{\mathcal{O}}\big({d H \cdot \min {1/{\rho}, H}/\sqrt{K} }\big)$, where $K$ is the number of episodes, $H$ is the horizon length, $d$ is the feature dimension and $\rho$ is the uncertainty level. This result improves the state-of-the-art by $\mathcal{O}(dH/\min{1/\rho,H})$. We also construct a novel hard instance and derive the first information-theoretic lower bound in this setting, which indicates our algorithm is near-optimal up to $\mathcal{O}(\sqrt{H})$ for any uncertainty level $\rho\in(0,1]$. Our algorithm also enjoys a 'rare-switching' design, and thus only requires $\mathcal{O}(dH\log(1+H^2K))$ policy switches and $\mathcal{O}(d^2H\log(1+H^2K))$ calls for oracle to solve dual optimization problems, which significantly improves the computational efficiency of existing algorithms for DRMDPs, whose policy switch and oracle complexities are both $\mathcal{O}(K)$.</td>
</tr>
<tr>
<td>加速非极大值抑制：图论视角</td>
<td>非极大值抑制（NMS）是目标检测中不可或缺的后处理步骤。随着网络模型的不断优化，NMS已成为提升目标检测效率的“最后一公里”。本文首次从图论的角度系统分析NMS，揭示其内在结构，并由此提出两种优化方法：QSI-NMS和BOE-NMS。前者是一种快速递归分治算法，mAP损失可忽略不计，其扩展版本（eQSI-NMS）达到最优复杂度$\mathcal{O}(n\log n)$。后者则专注于NMS的局部性，实现了恒定水平的优化，且无mAP损失。此外，为方便研究人员快速评估NMS方法，我们引入了NMS-Bench，这是首个全面评估各种NMS方法的基准。以MS COCO 2017上的YOLOv8-N模型为基准设置，我们的方法QSI-NMS在基准测试中提供了$6.2\times$的原NMS速度，mAP下降$0.1\%$。最优的eQSI-NMS在mAP仅下降$0.3\%$的情况下，实现了$10.7\times$的速度。同时，BOE-NMS在保持mAP不变的情况下，展示了$5.1\times$的速度。</td>
<td>King-Siong Si</td>
<td><a href="http://arxiv.org/pdf/2409.20520v1">PDF</a></td>
<td>N/A</td>
<td>Accelerating Non-Maximum Suppression: A Graph Theory Perspective</td>
<td>Non-maximum suppression (NMS) is an indispensable post-processing step in object detection. With the continuous optimization of network models, NMS has become the ``last mile'' to enhance the efficiency of object detection. This paper systematically analyzes NMS from a graph theory perspective for the first time, revealing its intrinsic structure. Consequently, we propose two optimization methods, namely QSI-NMS and BOE-NMS. The former is a fast recursive divide-and-conquer algorithm with negligible mAP loss, and its extended version (eQSI-NMS) achieves optimal complexity of $\mathcal{O}(n\log n)$. The latter, concentrating on the locality of NMS, achieves an optimization at a constant level without an mAP loss penalty. Moreover, to facilitate rapid evaluation of NMS methods for researchers, we introduce NMS-Bench, the first benchmark designed to comprehensively assess various NMS methods. Taking the YOLOv8-N model on MS COCO 2017 as the benchmark setup, our method QSI-NMS provides $6.2\times$ speed of original NMS on the benchmark, with a $0.1\%$ decrease in mAP. The optimal eQSI-NMS, with only a $0.3\%$ mAP decrease, achieves $10.7\times$ speed. Meanwhile, BOE-NMS exhibits $5.1\times$ speed with no compromise in mAP.</td>
</tr>
<tr>
<td>SMLE：通过嵌入超近似实现的安全机器学习</td>
<td>尽管机器学习（ML）和神经网络领域最近取得了显著进展，但如何为这些系统的行为提供形式化保证仍然是一个开放性问题，也是其在受监管或安全关键场景中应用的关键需求。我们考虑的任务是训练可微分的ML模型，这些模型能够保证满足设计者选择的属性，这些属性以输入输出蕴含的形式表述。由于严格验证和确保现代神经模型合规性的计算复杂性，这一任务非常具有挑战性。我们提出了一种基于三个组成部分的创新方法：1）一种通用、简单的架构，能够以保守的语义实现高效的验证；2）基于投影梯度法的严格训练算法；3）对寻找强反例问题的表述。所提出的框架仅在一定程度上受模型复杂性的影响，能够很好地扩展到实际应用中，并生成提供完全属性满足保证的模型。我们在回归中由线性不等式定义的属性和多标签分类中的互斥类别上评估了我们的方法。我们的方法与基线方法相比具有竞争力，基线方法包括在预处理阶段（即在训练数据上）和后处理阶段（即在模型预测上）实施属性。最后，我们的贡献建立了一个框架，开启了多个研究方向和潜在改进的可能性。</td>
<td>Matteo Francobaldi</td>
<td><a href="http://arxiv.org/pdf/2409.20517v1">PDF</a></td>
<td>N/A</td>
<td>SMLE: Safe Machine Learning via Embedded Overapproximation</td>
<td>Despite the extent of recent advances in Machine Learning (ML) and Neural Networks, providing formal guarantees on the behavior of these systems is still an open problem, and a crucial requirement for their adoption in regulated or safety-critical scenarios. We consider the task of training differentiable ML models guaranteed to satisfy designer-chosen properties, stated as input-output implications. This is very challenging, due to the computational complexity of rigorously verifying and enforcing compliance in modern neural models. We provide an innovative approach based on three components: 1) a general, simple architecture enabling efficient verification with a conservative semantic; 2) a rigorous training algorithm based on the Projected Gradient Method; 3) a formulation of the problem of searching for strong counterexamples. The proposed framework, being only marginally affected by model complexity, scales well to practical applications, and produces models that provide full property satisfaction guarantees. We evaluate our approach on properties defined by linear inequalities in regression, and on mutually exclusive classes in multilabel classification. Our approach is competitive with a baseline that includes property enforcement during preprocessing, i.e. on the training data, as well as during postprocessing, i.e. on the model predictions. Finally, our contributions establish a framework that opens up multiple research directions and potential improvements.</td>
</tr>
<tr>
<td>基于激光全场测量的数据驱动发现控制方程的集成WSINDy方法</td>
<td>本研究利用激光测振和偏微分方程的稀疏非线性动力学识别（WSINDy）的弱形式，从全场实验数据中学习宏观尺度控制方程。实验中，两个梁状试件（一个为铝材，另一个为IDOX/Estane复合材料）在低频范围内受到剪切波激励，并在试件表面以粒子速度的形式测量响应。将WSINDy算法应用于所得的时空数据，从一组潜在的偏微分方程中揭示试件的有效动力学特性。所发现的偏微分方程具有可识别的欧拉-伯努利梁模型形式，从中估计了两种材料的杨氏模量。还使用了WSINDy算法的集成版本，该版本提供了关于偏微分方程系数和杨氏模量不确定性的信息。所发现的偏微分方程还通过有限元代码进行模拟，以合理精度与实验数据进行比较。结合全场实验数据和WSINDy算法，是一种强大的无损方法，可用于学习未知的控制方程，并深入了解动态条件下的机械系统。</td>
<td>Abigail C. Schmid</td>
<td><a href="http://arxiv.org/pdf/2409.20510v1">PDF</a></td>
<td>N/A</td>
<td>Ensemble WSINDy for Data Driven Discovery of Governing Equations from Laser-based Full-field Measurements</td>
<td>This work leverages laser vibrometry and the weak form of the sparse identification of nonlinear dynamics (WSINDy) for partial differential equations to learn macroscale governing equations from full-field experimental data. In the experiments, two beam-like specimens, one aluminum and one IDOX/Estane composite, are subjected to shear wave excitation in the low frequency regime and the response is measured in the form of particle velocity on the specimen surface. The WSINDy for PDEs algorithm is applied to the resulting spatio-temporal data to discover the effective dynamics of the specimens from a family of potential PDEs. The discovered PDE is of the recognizable Euler-Bernoulli beam model form, from which the Young's modulus for the two materials are estimated. An ensemble version of the WSINDy algorithm is also used which results in information about the uncertainty in the PDE coefficients and Young's moduli. The discovered PDEs are also simulated with a finite element code to compare against the experimental data with reasonable accuracy. Using full-field experimental data and WSINDy together is a powerful non-destructive approach for learning unknown governing equations and gaining insights about mechanical systems in the dynamic regime.</td>
</tr>
<tr>
<td>基于日志的异常检测需要哪些信息？可配置Transformer方法的见解</td>
<td>日志数据源自源代码中的日志语句，为软件应用和系统的执行过程提供了深入洞察。基于日志的异常检测先进方法通常利用深度学习模型来捕捉日志数据中的语义或序列信息，并检测异常的运行时行为。然而，这些不同类型信息的影响尚不明确。此外，现有方法未捕捉日志数据中的时间戳，这些时间戳可能比序列信息提供更细粒度的时间信息。在本研究中，我们提出了一种可配置的基于Transformer的异常检测模型，该模型能够捕捉日志数据中的语义、序列和时间信息，并允许我们配置不同类型的信息作为模型的特征。此外，我们使用不同长度的日志序列训练和评估所提出的模型，从而克服了现有方法依赖固定长度或时间窗口日志序列作为输入的限制。通过所提出的模型，我们进行了一系列实验，使用不同组合的输入特征来评估不同类型信息在异常检测中的作用。在处理不同长度的日志序列时，该模型相比基线模型能够达到具有竞争力的稳定性能。结果表明，事件发生信息在识别异常中起关键作用，而序列和时间信息对所研究公共数据集中的异常检测影响不大。另一方面，研究结果也揭示了所研究公共数据集的简单性，并强调了构建包含不同类型异常的新数据集以更好地评估异常检测模型性能的重要性。</td>
<td>Xingfang Wu</td>
<td><a href="http://arxiv.org/pdf/2409.20503v1">PDF</a></td>
<td>N/A</td>
<td>What Information Contributes to Log-based Anomaly Detection? Insights from a Configurable Transformer-Based Approach</td>
<td>Log data are generated from logging statements in the source code, providing insights into the execution processes of software applications and systems. State-of-the-art log-based anomaly detection approaches typically leverage deep learning models to capture the semantic or sequential information in the log data and detect anomalous runtime behaviors. However, the impacts of these different types of information are not clear. In addition, existing approaches have not captured the timestamps in the log data, which can potentially provide more fine-grained temporal information than sequential information. In this work, we propose a configurable transformer-based anomaly detection model that can capture the semantic, sequential, and temporal information in the log data and allows us to configure the different types of information as the model's features. Additionally, we train and evaluate the proposed model using log sequences of different lengths, thus overcoming the constraint of existing methods that rely on fixed-length or time-windowed log sequences as inputs. With the proposed model, we conduct a series of experiments with different combinations of input features to evaluate the roles of different types of information in anomaly detection. When presented with log sequences of varying lengths, the model can attain competitive and consistently stable performance compared to the baselines. The results indicate that the event occurrence information plays a key role in identifying anomalies, while the impact of the sequential and temporal information is not significant for anomaly detection in the studied public datasets. On the other hand, the findings also reveal the simplicity of the studied public datasets and highlight the importance of constructing new datasets that contain different types of anomalies to better evaluate the performance of anomaly detection models.</td>
</tr>
<tr>
<td>拼贴画：利用分层潜在扩散和语言模型生成协作人机交互</td>
<td>我们提出了一种名为COLLAGE的新框架，通过利用大型语言模型（LLMs）和分层运动特定向量量化变分自编码器（VQ-VAEs）来生成协作的代理-对象-代理交互。我们的模型通过结合LLMs的知识和推理能力，指导生成扩散模型，解决了该领域数据集丰富性不足的问题。分层VQ-VAE架构在多个抽象层次上捕捉不同的运动特定特征，避免了冗余概念，并实现了高效的多分辨率表示。我们引入了一种在潜在空间中操作的扩散模型，并结合LLM生成的运动规划线索来指导去噪过程，从而实现更具控制性和多样性的提示特定运动生成。在CORE-4D和InterHuman数据集上的实验结果表明，我们的方法在生成真实且多样的协作人-对象-人交互方面具有显著效果，优于现有最先进的方法。我们的工作为在机器人学、图形学和计算机视觉等领域建模复杂交互开辟了新的可能性。</td>
<td>Divyanshu Daiya</td>
<td><a href="http://arxiv.org/pdf/2409.20502v1">PDF</a></td>
<td>N/A</td>
<td>COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models</td>
<td>We propose a novel framework COLLAGE for generating collaborative agent-object-agent interactions by leveraging large language models (LLMs) and hierarchical motion-specific vector-quantized variational autoencoders (VQ-VAEs). Our model addresses the lack of rich datasets in this domain by incorporating the knowledge and reasoning abilities of LLMs to guide a generative diffusion model. The hierarchical VQ-VAE architecture captures different motion-specific characteristics at multiple levels of abstraction, avoiding redundant concepts and enabling efficient multi-resolution representation. We introduce a diffusion model that operates in the latent space and incorporates LLM-generated motion planning cues to guide the denoising process, resulting in prompt-specific motion generation with greater control and diversity. Experimental results on the CORE-4D, and InterHuman datasets demonstrate the effectiveness of our approach in generating realistic and diverse collaborative human-object-human interactions, outperforming state-of-the-art methods. Our work opens up new possibilities for modeling complex interactions in various domains, such as robotics, graphics and computer vision.</td>
</tr>
<tr>
<td>通过知识蒸馏、多任务学习和数据增强提升罗马尼亚语攻击性语言检测</td>
<td>本文强调了自然语言处理（NLP）在人工智能中的重要性，突显了其在理解和建模人类语言方面的关键作用。NLP领域的最新进展，尤其是在对话机器人方面，已引起了开发者的广泛关注和采用。本文探讨了实现更小、更高效NLP模型的先进方法。具体而言，我们采用了三种主要方法：（1）训练基于Transformer的神经网络以检测冒犯性语言，（2）采用数据增强和知识蒸馏技术以提升性能，（3）结合多任务学习与知识蒸馏及使用多样化数据集的教师退火法以增强效率。这些方法的综合应用已取得了显著的改进成果。</td>
<td>Vlad-Cristian Matei</td>
<td><a href="http://arxiv.org/pdf/2409.20498v1">PDF</a></td>
<td>N/A</td>
<td>Enhancing Romanian Offensive Language Detection through Knowledge Distillation, Multi-Task Learning, and Data Augmentation</td>
<td>This paper highlights the significance of natural language processing (NLP) within artificial intelligence, underscoring its pivotal role in comprehending and modeling human language. Recent advancements in NLP, particularly in conversational bots, have garnered substantial attention and adoption among developers. This paper explores advanced methodologies for attaining smaller and more efficient NLP models. Specifically, we employ three key approaches: (1) training a Transformer-based neural network to detect offensive language, (2) employing data augmentation and knowledge distillation techniques to increase performance, and (3) incorporating multi-task learning with knowledge distillation and teacher annealing using diverse datasets to enhance efficiency. The culmination of these methods has yielded demonstrably improved outcomes.</td>
</tr>
<tr>
<td>预算约束下的在线决策延迟</td>
<td>机器学习（ML）模型越来越多地被用于支持或替代决策过程。在技能专家资源有限的应用场景中，减轻他们的负担并在ML模型的性能至少与专家相当时自动化决策至关重要。然而，模型通常是预先训练并固定的，而任务则是按顺序到达的，其分布可能会发生变化。在这种情况下，决策者的相应性能可能会改变，因此延迟算法必须保持适应性。我们提出了一种上下文多臂赌博机模型来解决这种在线决策问题。我们的框架包括预算约束和不同类型的部分反馈模型。除了我们算法的理论保证外，我们还提出了高效的扩展，这些扩展在现实世界的数据集上实现了显著的性能。</td>
<td>Mirabel Reid</td>
<td><a href="http://arxiv.org/pdf/2409.20489v1">PDF</a></td>
<td>N/A</td>
<td>Online Decision Deferral under Budget Constraints</td>
<td>Machine Learning (ML) models are increasingly used to support or substitute decision making. In applications where skilled experts are a limited resource, it is crucial to reduce their burden and automate decisions when the performance of an ML model is at least of equal quality. However, models are often pre-trained and fixed, while tasks arrive sequentially and their distribution may shift. In that case, the respective performance of the decision makers may change, and the deferral algorithm must remain adaptive. We propose a contextual bandit model of this online decision making problem. Our framework includes budget constraints and different types of partial feedback models. Beyond the theoretical guarantees of our algorithm, we propose efficient extensions that achieve remarkable performance on real-world datasets.</td>
</tr>
<tr>
<td>使用拉普拉斯神经流形的“什么”与“何时”工作记忆表征</td>
<td>工作记忆——即在事件不断退入过去时记住它们的能力——需要能够在任何时间延迟下表示任何刺激的能力。这一特性要求编码工作记忆的神经元表现出混合选择性，具有刺激和时间的联合感受野（RFs），形成“什么”×“何时”的表示。我们在简单的实验中研究这种工作记忆的特性，其中必须记住一个单一的刺激一段时间。联合感受野的要求使得网络的协方差矩阵能够很好地解耦，从而理解群体的低维动力学。不同的时间基函数选择会导致质上不同的动力学。我们研究了一个特定的选择——拉普拉斯空间与指数时间基函数相结合，以及一个在时间上具有限定基函数的“逆拉普拉斯”空间。我们将这种均匀平铺对数时间的基函数选择称为拉普拉斯神经流形。尽管它们通过线性投影相互关联，拉普拉斯群体显示出稳定的刺激特定子空间，而逆拉普拉斯群体显示出旋转动力学。随着时间的推移，协方差矩阵的秩的增长取决于时间基集的密度；对数平铺与数据表现出良好的一致性。我们勾勒出一个构建拉普拉斯神经流形的连续吸引子CANN。拉普拉斯空间中的吸引子表现为边缘；逆空间中的吸引子表现为凸起。这项工作提供了一个从更抽象的认知工作记忆模型到使用连续吸引子神经网络的电路级实现的路线图，并确定了支持工作记忆的神经动力学类型。</td>
<td>Aakash Sarkar</td>
<td><a href="http://arxiv.org/pdf/2409.20484v1">PDF</a></td>
<td>N/A</td>
<td>"What" x "When" working memory representations using Laplace Neural Manifolds</td>
<td>Working memory $\unicode{x2013}$ the ability to remember recent events as they recede continuously into the past $\unicode{x2013}$ requires the ability to represent any stimulus at any time delay. This property requires neurons coding working memory to show mixed selectivity, with conjunctive receptive fields (RFs) for stimuli and time, forming a representation of 'what' $\times$ 'when'. We study the properties of such a working memory in simple experiments where a single stimulus must be remembered for a short time. The requirement of conjunctive receptive fields allows the covariance matrix of the network to decouple neatly, allowing an understanding of the low-dimensional dynamics of the population. Different choices of temporal basis functions lead to qualitatively different dynamics. We study a specific choice $\unicode{x2013}$ a Laplace space with exponential basis functions for time coupled to an "Inverse Laplace" space with circumscribed basis functions in time. We refer to this choice with basis functions that evenly tile log time as a Laplace Neural Manifold. Despite the fact that they are related to one another by a linear projection, the Laplace population shows a stable stimulus-specific subspace whereas the Inverse Laplace population shows rotational dynamics. The growth of the rank of the covariance matrix with time depends on the density of the temporal basis set; logarithmic tiling shows good agreement with data. We sketch a continuous attractor CANN that constructs a Laplace Neural Manifold. The attractor in the Laplace space appears as an edge; the attractor for the inverse space appears as a bump. This work provides a map for going from more abstract cognitive models of WM to circuit-level implementation using continuous attractor neural networks, and places constraints on the types of neural dynamics that support working memory.</td>
</tr>
<tr>
<td>RecSys Challenge 2024：在新闻推荐中平衡准确性与编辑价值观</td>
<td>RecSys Challenge 2024 旨在通过解决新闻推荐系统设计中固有的技术和规范挑战，推动新闻推荐技术的发展。本文介绍了该挑战赛的目标、问题设定以及由丹麦新闻出版商 Ekstra Bladet 和 JP/Politikens Media Group（简称“Ekstra Bladet”）提供的数据集。挑战赛探讨了新闻推荐的独特方面，如基于用户行为建模偏好、考虑新闻议程对用户兴趣的影响以及管理新闻项目的快速衰减。此外，挑战赛还涉及规范复杂性，研究推荐系统对新闻流的影响及其与编辑价值观的一致性。我们总结了挑战赛的设置、数据集特征和评估指标。最后，我们宣布了获胜者并强调了他们的贡献。数据集可通过以下链接获取：https://recsys.eb.dk。</td>
<td>Johannes Kruse</td>
<td><a href="http://arxiv.org/pdf/2409.20483v1">PDF</a></td>
<td>N/A</td>
<td>RecSys Challenge 2024: Balancing Accuracy and Editorial Values in News Recommendations</td>
<td>The RecSys Challenge 2024 aims to advance news recommendation by addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. This paper describes the challenge, including its objectives, problem setting, and the dataset provided by the Danish news publishers Ekstra Bladet and JP/Politikens Media Group ("Ekstra Bladet"). The challenge explores the unique aspects of news recommendation, such as modeling user preferences based on behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Additionally, the challenge embraces normative complexities, investigating the effects of recommender systems on news flow and their alignment with editorial values. We summarize the challenge setup, dataset characteristics, and evaluation metrics. Finally, we announce the winners and highlight their contributions. The dataset is available at: https://recsys.eb.dk.</td>
</tr>
<tr>
<td>越南社交媒体中机器词汇规范化的一种弱监督数据标注框架</td>
<td>本研究引入了一种创新的自动标注框架，旨在解决越南语等低资源语言在社交媒体文本中的词汇规范化挑战。社交媒体数据丰富多样，但这些环境中使用的不断演变的多样化语言使得手动标注既费时又昂贵。为了应对这些问题，我们提出了一种结合半监督学习和弱监督技术的框架。这种方法在减少手动标注工作量的同时，提高了训练数据集的质量并扩大了其规模。我们的框架能够自动标注原始数据，将非标准词汇转换为标准形式，从而提高训练数据的准确性和一致性。实验结果表明，我们的弱监督框架在规范化越南语文本方面具有显著效果，尤其是在利用预训练语言模型时。该框架实现了令人印象深刻的82.72%的F1分数，并在保持词汇完整性方面达到了高达99.22%的准确率。此外，它在各种条件下都能有效处理无音调符号的文本。这一框架显著提升了自然语言规范化的质量，并提高了各种自然语言处理任务的准确性，平均准确率提高了1-3%。</td>
<td>Dung Ha Nguyen</td>
<td><a href="http://arxiv.org/pdf/2409.20467v1">PDF</a></td>
<td>N/A</td>
<td>A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</td>
<td>This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing Pre-trained Language Models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1-3%.</td>
</tr>
<tr>
<td>跨领域自动文本简化的西班牙语语言资源</td>
<td>本研究描述了为西班牙语文本自动简化在三个领域（金融、医学和历史研究）开发的语言资源和模型。我们在每个领域创建了多个语料库、标注和简化指南、技术性和简化医学术语词典、用于金融领域共享任务的数据集，以及两款简化工具。方法论、资源及相关出版物已在网站上公开共享：https://clara-nlp.uned.es/。</td>
<td>Antonio Moreno-Sandoval</td>
<td><a href="http://arxiv.org/pdf/2409.20466v1">PDF</a></td>
<td>N/A</td>
<td>Language Resources in Spanish for Automatic Text Simplification across Domains</td>
<td>This work describes the language resources and models developed for automatic simplification of Spanish texts in three domains: Finance, Medicine and History studies. We created several corpora in each domain, annotation and simplification guidelines, a lexicon of technical and simplified medical terms, datasets used in shared tasks for the financial domain, and two simplification tools. The methodology, resources and companion publications are shared publicly on the web-site: https://clara-nlp.uned.es/.</td>
</tr>
<tr>
<td>教师嵌入线性投影用于少类蒸馏</td>
<td>知识蒸馏（KD）作为一种有前景的方法，旨在将知识从更大、更复杂的教师模型转移到较小的学生模型中。传统上，KD涉及训练学生模型以模仿教师模型的输出概率，而更先进的技术则探索引导学生模型采用教师模型的内部表示。尽管KD在许多领域取得了广泛的成功，但在二分类和少类问题上的表现却不尽如人意。这是因为教师模型的泛化模式信息量与类别数量直接相关。此外，一些复杂的蒸馏方法可能无法普遍适用于计算机视觉以外的数据类型。因此，对于情感分析、搜索查询理解、广告与查询相关性评估等关键现实应用，有效的蒸馏技术仍然难以捉摸。考虑到这些观察，我们提出了一种从教师模型表示中蒸馏知识的新方法，称为学习嵌入线性投影（LELP）。受近期关于最终层表示结构的研究启发，LELP通过识别教师嵌入空间中的信息线性子空间，并将其分割为伪子类，来工作。然后，学生模型被训练以复制这些伪类。我们在大规模NLP基准测试（如Amazon Reviews和Sentiment140）上的实验评估表明，LELP在二分类和少类问题上始终具有竞争力，并且通常优于现有的最先进蒸馏算法，这些算法在大多数KD方法中表现不佳。</td>
<td>Noel Loo</td>
<td><a href="http://arxiv.org/pdf/2409.20449v1">PDF</a></td>
<td>N/A</td>
<td>Linear Projections of Teacher Embeddings for Few-Class Distillation</td>
<td>Knowledge Distillation (KD) has emerged as a promising approach for transferring knowledge from a larger, more complex teacher model to a smaller student model. Traditionally, KD involves training the student to mimic the teacher's output probabilities, while more advanced techniques have explored guiding the student to adopt the teacher's internal representations. Despite its widespread success, the performance of KD in binary classification and few-class problems has been less satisfactory. This is because the information about the teacher model's generalization patterns scales directly with the number of classes. Moreover, several sophisticated distillation methods may not be universally applicable or effective for data types beyond Computer Vision. Consequently, effective distillation techniques remain elusive for a range of key real-world applications, such as sentiment analysis, search query understanding, and advertisement-query relevance assessment. Taking these observations into account, we introduce a novel method for distilling knowledge from the teacher's model representations, which we term Learning Embedding Linear Projections (LELP). Inspired by recent findings about the structure of final-layer representations, LELP works by identifying informative linear subspaces in the teacher's embedding space, and splitting them into pseudo-subclasses. The student model is then trained to replicate these pseudo-classes. Our experimental evaluation on large-scale NLP benchmarks like Amazon Reviews and Sentiment140 demonstrate the LELP is consistently competitive with, and typically superior to, existing state-of-the-art distillation algorithms for binary and few-class problems, where most KD methods suffer.</td>
</tr>
<tr>
<td>POMONAG：帕累托最优多目标神经架构生成器</td>
<td>神经架构搜索（NAS）自动化了神经网络设计，减少了对人类专家的依赖。尽管NAS方法计算密集且特定于数据集，但辅助预测器减少了需要训练的模型数量，从而缩短了搜索时间。这种策略用于生成满足多个计算约束的架构。最近，可迁移的NAS应运而生，将搜索过程从数据集依赖泛化到任务依赖。在这一领域，DiffusionNAG是一种最先进的方法。这种基于扩散的方法简化了计算，生成的架构在不进行进一步适应的情况下，对未见数据集的准确性进行了优化。然而，DiffusionNAG仅关注准确性，忽略了模型复杂性、计算效率和推理延迟等其他关键目标——这些因素对于在资源受限环境中部署模型至关重要。本文介绍了Pareto-Optimal Many-Objective Neural Architecture Generator（POMONAG），通过多目标扩散过程扩展了DiffusionNAG。POMONAG同时考虑了准确性、参数数量、乘积累加运算（MACs）和推理延迟。它集成了性能预测模型来估计这些指标并指导扩散梯度。POMONAG的优化通过扩展其训练元数据集、应用Pareto前沿过滤以及细化条件生成的嵌入来增强。这些增强使得POMONAG能够生成在性能和效率上优于之前最先进的Pareto最优架构。结果在两个搜索空间——NASBench201和MobileNetV3上进行了验证，并在15个图像分类数据集上进行了评估。</td>
<td>Eugenio Lomurno</td>
<td><a href="http://arxiv.org/pdf/2409.20447v1">PDF</a></td>
<td>N/A</td>
<td>POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator</td>
<td>Neural Architecture Search (NAS) automates neural network design, reducing dependence on human expertise. While NAS methods are computationally intensive and dataset-specific, auxiliary predictors reduce the models needing training, decreasing search time. This strategy is used to generate architectures satisfying multiple computational constraints. Recently, Transferable NAS has emerged, generalizing the search process from dataset-dependent to task-dependent. In this field, DiffusionNAG is a state-of-the-art method. This diffusion-based approach streamlines computation, generating architectures optimized for accuracy on unseen datasets without further adaptation. However, by focusing solely on accuracy, DiffusionNAG overlooks other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained environments. This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG via a many-objective diffusion process. POMONAG simultaneously considers accuracy, number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these metrics and guide diffusion gradients. POMONAG's optimization is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in performance and efficiency. Results were validated on two search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.</td>
</tr>
<tr>
<td>实例自适应的零样本思维链提示</td>
<td>零样本思维链（CoT）提示作为一种简单而有效的策略，正在崭露头角，用以提升大型语言模型（LLMs）在实际推理任务中的表现。然而，单一的、任务级别的提示在整个实例中统一应用的效果本质上是有限的，因为一个提示无法成为所有实例的“好搭档”。更为恰当的方法应细致考虑提示与每个实例之间的互动。本文提出了一种实例自适应提示算法，作为零样本CoT推理的替代方案，通过自适应地区分好与坏的提示。具体而言，我们首先通过信息流的视角对LLMs进行分析，以揭示零样本CoT推理的机制。我们发现，从问题到提示以及从问题到推理的信息流动共同影响着推理结果。我们注意到，更好的零样本CoT推理需要提示从问题中获取语义信息，然后推理直接从问题中获取足够的信息，并通过提示间接获取。相反，缺乏其中任何一项都可能导致不良的推理结果。基于此，我们进一步提出了零样本CoT推理的实例自适应提示策略（IAP）。在LLaMA-2、LLaMA-3和Qwen上进行的实验，涵盖数学、逻辑和常识推理任务（如GSM8K、MMLU、因果判断），均获得了持续的改进，表明实例自适应的零样本CoT提示在性能上优于其他任务级别的方法，即使这些方法使用了精心设计的提示或复杂的程序，也凸显了我们关于零样本CoT推理机制发现的重要性。</td>
<td>Xiaosong Yuan</td>
<td><a href="http://arxiv.org/pdf/2409.20441v2">PDF</a></td>
<td>N/A</td>
<td>Instance-adaptive Zero-shot Chain-of-Thought Prompting</td>
<td>Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effective strategy for enhancing the performance of large language models (LLMs) in real-world reasoning tasks. Nonetheless, the efficacy of a singular, task-level prompt uniformly applied across the whole of instances is inherently limited since one prompt cannot be a good partner for all, a more appropriate approach should consider the interaction between the prompt and each instance meticulously. This work introduces an instance-adaptive prompting algorithm as an alternative zero-shot CoT reasoning scheme by adaptively differentiating good and bad prompts. Concretely, we first employ analysis on LLMs through the lens of information flow to detect the mechanism under zero-shot CoT reasoning, in which we discover that information flows from question to prompt and question to rationale jointly influence the reasoning results most. We notice that a better zero-shot CoT reasoning needs the prompt to obtain semantic information from the question then the rationale aggregates sufficient information from the question directly and via the prompt indirectly. On the contrary, lacking any of those would probably lead to a bad one. Stem from that, we further propose an instance-adaptive prompting strategy (IAP) for zero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwen on math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, Causal Judgement) obtain consistent improvement, demonstrating that the instance-adaptive zero-shot CoT prompting performs better than other task-level methods with some curated prompts or sophisticated procedures, showing the significance of our findings in the zero-shot CoT reasoning mechanism.</td>
</tr>
<tr>
<td>面对模糊性的乐观原则在多臂老虎机问题中的应用</td>
<td>Follow-The-Regularized-Leader (FTRL) 算法通常在对抗性和随机性多臂老虎机问题中享有最优的后悔值，并且允许进行简化的分析。然而，FTRL 算法在每次迭代中都需要解决一个优化问题，因此在计算上具有挑战性。相比之下，Follow-The-Perturbed-Leader (FTPL) 算法通过扰动臂的奖励估计来实现计算效率，但其后悔分析较为繁琐。我们提出了一种新的 FTPL 算法，能够为对抗性和随机性多臂老虎机生成最优策略。与 FTRL 类似，我们的算法允许进行统一的后悔分析，并且与 FTPL 类似，它提供了低计算成本。与现有依赖于由已知分布控制的独立加性扰动的 FTPL 算法不同，我们允许扰动由一个仅知属于给定集合的模糊分布控制，并提出了一种在面对模糊时的乐观主义原则。因此，我们的框架推广了现有的 FTPL 算法。它还将一系列 FTRL 方法作为特例包含在内，包括几种最优方法，这在当前的 FTPL 方法中似乎是不可能的。最后，我们利用离散选择理论的技术设计了一种高效的二分算法，用于计算乐观的臂采样概率。该算法的速度比每次迭代都解决优化问题的标准 FTRL 算法快达 $10^4$ 倍。我们的结果不仅解决了现有的猜想，还通过将 FTRL 映射到 FTPL，提供了关于扰动影响的新见解。</td>
<td>Mengmeng Li</td>
<td><a href="http://arxiv.org/pdf/2409.20440v1">PDF</a></td>
<td>N/A</td>
<td>Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits</td>
<td>Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret for adversarial as well as stochastic bandit problems and allow for a streamlined analysis. Nonetheless, FTRL algorithms require the solution of an optimization problem in every iteration and are thus computationally challenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve computational efficiency by perturbing the estimates of the rewards of the arms, but their regret analysis is cumbersome. We propose a new FTPL algorithm that generates optimal policies for both adversarial and stochastic multi-armed bandits. Like FTRL, our algorithm admits a unified regret analysis, and similar to FTPL, it offers low computational costs. Unlike existing FTPL algorithms that rely on independent additive disturbances governed by a \textit{known} distribution, we allow for disturbances governed by an \textit{ambiguous} distribution that is only known to belong to a given set and propose a principle of optimism in the face of ambiguity. Consequently, our framework generalizes existing FTPL algorithms. It also encapsulates a broad range of FTRL methods as special cases, including several optimal ones, which appears to be impossible with current FTPL methods. Finally, we use techniques from discrete choice theory to devise an efficient bisection algorithm for computing the optimistic arm sampling probabilities. This algorithm is up to $10^4$ times faster than standard FTRL algorithms that solve an optimization problem in every iteration. Our results not only settle existing conjectures but also provide new insights into the impact of perturbations by mapping FTRL to FTPL.</td>
</tr>
<tr>
<td>QA编码器：面向问答系统中的对齐表示学习</td>
<td>现代问答系统采用检索增强生成（RAG）技术以提供准确且可信的回答。然而，用户查询与相关文档之间的固有差距阻碍了精确匹配。受我们的圆锥分布假设启发，即潜在查询和文档在嵌入空间中形成类似圆锥的结构，我们提出了QAEncoder，一种无需训练的方法来弥合这一差距。具体而言，QAEncoder将嵌入空间中潜在查询的期望估计为文档嵌入的稳健替代，并附加文档指纹以有效区分这些嵌入。在六种语言和八个数据集上对十四个嵌入模型进行的广泛实验验证了QAEncoder的对齐能力，它提供了一种即插即用的解决方案，能够无缝集成到现有的RAG架构和基于训练的方法中。</td>
<td>Zhengren Wang</td>
<td><a href="http://arxiv.org/pdf/2409.20434v1">PDF</a></td>
<td>N/A</td>
<td>QAEncoder: Towards Aligned Representation Learning in Question Answering System</td>
<td>Modern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documents hinders precise matching. Motivated by our conical distribution hypothesis, which posits that potential queries and documents form a cone-like structure in the embedding space, we introduce QAEncoder, a training-free approach to bridge this gap. Specifically, QAEncoder estimates the expectation of potential queries in the embedding space as a robust surrogate for the document embedding, and attaches document fingerprints to effectively distinguish these embeddings. Extensive experiments on fourteen embedding models across six languages and eight datasets validate QAEncoder's alignment capability, which offers a plug-and-play solution that seamlessly integrates with existing RAG architectures and training-based methods.</td>
</tr>
<tr>
<td>多层Picard逼近和具有ReLU、leaky ReLU及softplus激活的深度神经网络在$L^p$意义下克服了维度灾难，当逼近半线性抛物型偏微分方程时。</td>
<td>我们证明了多级Picard逼近和具有ReLU、leaky ReLU以及softplus激活的深度神经网络能够在$L^\mathfrak{p}$-意义下逼近半线性Kolmogorov PDE的解，其中$\mathfrak{p}\in [2,\infty)$，在梯度无关、Lipschitz连续的非线性情况下，多级Picard逼近的计算量和神经网络所需参数的数量在最坏情况下在维度$d\in \mathbb{N}$和规定精度的倒数$\epsilon$上最多呈多项式增长。</td>
<td>Ariel Neufeld</td>
<td><a href="http://arxiv.org/pdf/2409.20431v1">PDF</a></td>
<td>N/A</td>
<td>Multilevel Picard approximations and deep neural networks with ReLU, leaky ReLU, and softplus activation overcome the curse of dimensionality when approximating semilinear parabolic partial differential equations in $L^p$-sense</td>
<td>We prove that multilevel Picard approximations and deep neural networks with ReLU, leaky ReLU, and softplus activation are capable of approximating solutions of semilinear Kolmogorov PDEs in $L^\mathfrak{p}$-sense, $\mathfrak{p}\in [2,\infty)$, in the case of gradient-independent, Lipschitz-continuous nonlinearities, while the computational effort of the multilevel Picard approximations and the required number of parameters in the neural networks grow at most polynomially in both dimension $d\in \mathbb{N}$ and reciprocal of the prescribed accuracy $\epsilon$.</td>
</tr>
<tr>
<td>HELPD：通过分层反馈学习与视觉增强惩罚解码缓解大语言模型的幻觉现象</td>
<td>大型视觉-语言模型（LVLMs）在众多视觉-语言任务中展现了卓越的性能。然而，这些模型仍存在多模态幻觉问题，即生成的对象或内容与图像不符。许多现有工作通过直接判断对象是否存在于图像中来检测幻觉，忽略了对象与语义之间的关联。为解决这一问题，我们提出了视觉增强惩罚解码的分层反馈学习（HELPD）。该框架在对象和句子语义层面上整合了幻觉反馈。值得注意的是，即使在训练程度有限的情况下，这种方法也能减少超过15%的幻觉。同时，HELPD根据图像注意力窗口对输出对数进行惩罚，以避免过度受生成文本的影响。HELPD可以无缝集成到任何LVLMs中。我们的实验表明，所提出的框架在多个幻觉基准测试中表现出色。它有效地缓解了不同LVLMs的幻觉问题，并同时提升了它们的文本生成质量。</td>
<td>Fan Yuan</td>
<td><a href="http://arxiv.org/pdf/2409.20429v1">PDF</a></td>
<td>N/A</td>
<td>HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding</td>
<td>Large Vision-Language Models (LVLMs) have shown remarkable performance on many visual-language tasks. However, these models still suffer from multimodal hallucination, which means the generation of objects or content that violates the images. Many existing work detects hallucination by directly judging whether an object exists in an image, overlooking the association between the object and semantics. To address this issue, we propose Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding (HELPD). This framework incorporates hallucination feedback at both object and sentence semantic levels. Remarkably, even with a marginal degree of training, this approach can alleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output logits according to the image attention window to avoid being overly affected by generated text. HELPD can be seamlessly integrated with any LVLMs. Our experiments demonstrate that the proposed framework yields favorable results across multiple hallucination benchmarks. It effectively mitigates hallucination for different LVLMs and concurrently improves their text generation quality.</td>
</tr>
<tr>
<td>解码来自fMRI的视觉回声：为过去的语义信息进行记忆解构</td>
<td>人类视觉系统能够处理连续的视觉信息流，但大脑在连续视觉处理过程中如何编码和检索最近的视觉记忆仍未被探索。本研究探讨了工作记忆在连续视觉刺激下保留过去信息的能力。随后，我们提出了一项新的任务——记忆解构，旨在从fMRI信号中提取和解码过去的信息。为了解决过去记忆信息干扰的问题，我们设计了一种受前摄干扰现象启发的解构对比学习方法。该方法将相邻fMRI信号之间的信息分离为当前和过去两个部分，并将它们解码为图像描述。实验结果表明，该方法有效地解构了fMRI信号中的信息。这项研究可能推动脑机接口的发展，并缓解fMRI时间分辨率低的问题。</td>
<td>Runze Xia</td>
<td><a href="http://arxiv.org/pdf/2409.20428v1">PDF</a></td>
<td>N/A</td>
<td>Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information</td>
<td>The human visual system is capable of processing continuous streams of visual information, but how the brain encodes and retrieves recent visual memories during continuous visual processing remains unexplored. This study investigates the capacity of working memory to retain past information under continuous visual stimuli. And then we propose a new task Memory Disentangling, which aims to extract and decode past information from fMRI signals. To address the issue of interference from past memory information, we design a disentangled contrastive learning method inspired by the phenomenon of proactive interference. This method separates the information between adjacent fMRI signals into current and past components and decodes them into image descriptions. Experimental results demonstrate that this method effectively disentangles the information within fMRI signals. This research could advance brain-computer interfaces and mitigate the problem of low temporal resolution in fMRI.</td>
</tr>
<tr>
<td>充分必要解释（及其间的区别）</td>
<td>随着复杂的机器学习模型在重大决策场景中的应用不断增加，我们能够解释和理解其预测结果变得至关重要。事后解释方法通过识别输入$\mathbf{x}$中对模型输出$f(\mathbf{x})$重要的特征，提供了有用的见解。在这项工作中，我们形式化并研究了两种适用于一般机器学习模型的特征重要性的精确概念：充分性和必要性。我们展示了这两种解释类型，尽管直观且简单，但在提供模型认为重要的特征的完整图景方面可能存在不足。为此，我们提出了一种统一的重要性概念，通过在必要性-充分性轴上探索连续体来规避这些局限性。我们展示的这种统一概念与基于条件独立性和基于博弈论量（如Shapley值）的特征重要性定义有着紧密的联系。重要的是，我们展示了统一视角如何使我们能够检测到仅凭前述两种方法之一可能遗漏的重要特征。</td>
<td>Beepul Bharti</td>
<td><a href="http://arxiv.org/pdf/2409.20427v1">PDF</a></td>
<td>N/A</td>
<td>Sufficient and Necessary Explanations (and What Lies in Between)</td>
<td>As complex machine learning models continue to find applications in high-stakes decision-making scenarios, it is crucial that we can explain and understand their predictions. Post-hoc explanation methods provide useful insights by identifying important features in an input $\mathbf{x}$ with respect to the model output $f(\mathbf{x})$. In this work, we formalize and study two precise notions of feature importance for general machine learning models: sufficiency and necessity. We demonstrate how these two types of explanations, albeit intuitive and simple, can fall short in providing a complete picture of which features a model finds important. To this end, we propose a unified notion of importance that circumvents these limitations by exploring a continuum along a necessity-sufficiency axis. Our unified notion, we show, has strong ties to other popular definitions of feature importance, like those based on conditional independence and game-theoretic quantities like Shapley values. Crucially, we demonstrate how a unified perspective allows us to detect important features that could be missed by either of the previous approaches alone.</td>
</tr>
<tr>
<td>从贝叶斯决策理论的角度看流级流匹配</td>
<td>流匹配（FM）是一类用于拟合连续归一化流（CNFs）的训练算法。一种标准的FM方法称为条件流匹配（CFM），它利用了CNF的边缘向量场可以通过对所谓的条件向量场进行最小二乘回归拟合来学习的特性，该条件向量场是在给定流路径的一端或两端的情况下指定的。我们展示了从贝叶斯决策理论的角度看待CFM训练的参数估计，为CFM算法的推广打开了大门。我们提出了一种这样的扩展，通过引入基于定义条件概率路径的CFM算法，该路径是根据我们称之为“流”的潜在随机路径实例给出的，这些路径连接了噪声和观测数据对。此外，我们主张使用高斯过程（GPs）来建模这些潜在流。GP的独特分布特性，特别是GP的速度仍然是GP这一事实，使得可以从结果的流增强条件概率路径中抽取样本，而无需模拟实际的流，因此CFM训练的“无模拟”特性得以保留。我们展示了这种CFM的推广可以在适度的计算成本下显著减少估计的边缘向量场的方差，从而在常见指标下提高生成样本的质量。此外，我们展示了在流上采用GP可以灵活地连接多个相关的训练数据点（例如，时间序列）并纳入额外的先验信息。我们通过模拟和应用于两个手写图像数据集的应用程序，实证验证了我们的主张。</td>
<td>Ganchao Wei</td>
<td><a href="http://arxiv.org/pdf/2409.20423v1">PDF</a></td>
<td>N/A</td>
<td>Stream-level flow matching from a Bayesian decision theoretic perspective</td>
<td>Flow matching (FM) is a family of training algorithms for fitting continuous normalizing flows (CNFs). A standard approach to FM, called conditional flow matching (CFM), exploits the fact that the marginal vector field of a CNF can be learned by fitting least-square regression to the so-called conditional vector field specified given one or both ends of the flow path. We show that viewing CFM training from a Bayesian decision theoretic perspective on parameter estimation opens the door to generalizations of CFM algorithms. We propose one such extension by introducing a CFM algorithm based on defining conditional probability paths given what we refer to as <code>streams'', instances of latent stochastic paths that connect pairs of noise and observed data. Further, we advocates the modeling of these latent streams using Gaussian processes (GPs). The unique distributional properties of GPs, and in particular the fact that the velocities of a GP is still a GP, allows drawing samples from the resulting stream-augmented conditional probability path without simulating the actual streams, and hence the</code>simulation-free" nature of CFM training is preserved. We show that this generalization of the CFM can substantially reduce the variance in the estimated marginal vector field at a moderate computational cost, thereby improving the quality of the generated samples under common metrics. Additionally, we show that adopting the GP on the streams allows for flexibly linking multiple related training data points (e.g., time series) and incorporating additional prior information. We empirically validate our claim through both simulations and applications to two hand-written image datasets.</td>
</tr>
<tr>
<td>LHC上的新型机器学习应用</td>
<td>机器学习（ML）是粒子物理领域中一个快速发展的研究领域，在CERN LHC有着广泛的应用。ML作为一种多功能工具，改变了粒子物理学家进行搜索和测量的方式，不仅改进了现有方法，还实现了全新的方法。在这些报告中，我们描述了用于改进LHC实验中分类、快速模拟、展开和异常检测的新型ML技术和最新成果。</td>
<td>Javier M. Duarte</td>
<td><a href="http://arxiv.org/pdf/2409.20413v1">PDF</a></td>
<td>N/A</td>
<td>Novel machine learning applications at the LHC</td>
<td>Machine learning (ML) is a rapidly growing area of research in the field of particle physics, with a vast array of applications at the CERN LHC. ML has changed the way particle physicists conduct searches and measurements as a versatile tool used to improve existing approaches and enable fundamentally new ones. In these proceedings, we describe novel ML techniques and recent results for improved classification, fast simulation, unfolding, and anomaly detection in LHC experiments.</td>
</tr>
<tr>
<td>连续治疗剂量反应模型的共形预测</td>
<td>理解个体在连续治疗与结果之间的剂量-反应关系，可以极大地推动决策制定，特别是在个性化药物剂量和个性化医疗干预等领域。在这些高风险环境中，点估计往往是不够的，这突显了量化不确定性的必要性，以支持明智的决策。作为不确定性量化的一种无分布假设且模型无关的方法，保形预测在连续治疗或剂量-反应模型中的应用有限。为了填补这一空白，我们提出了一种新颖的方法，将因果剂量-反应问题框架化为协变量偏移，利用加权保形预测。通过结合倾向性估计、保形预测系统和似然比，我们提供了一种实用的解决方案，用于生成剂量-反应模型的预测区间。此外，我们的方法通过在加权保形预测中应用核函数作为权重，近似估计了每个治疗值的局部覆盖率。最后，我们使用一个新的合成基准数据集来展示协变量偏移假设在实现剂量-反应模型的稳健预测区间中的重要性。</td>
<td>Jarne Verhaeghe</td>
<td><a href="http://arxiv.org/pdf/2409.20412v1">PDF</a></td>
<td>N/A</td>
<td>Conformal Prediction for Dose-Response Models with Continuous Treatments</td>
<td>Understanding the dose-response relation between a continuous treatment and the outcome for an individual can greatly drive decision-making, particularly in areas like personalized drug dosing and personalized healthcare interventions. Point estimates are often insufficient in these high-risk environments, highlighting the need for uncertainty quantification to support informed decisions. Conformal prediction, a distribution-free and model-agnostic method for uncertainty quantification, has seen limited application in continuous treatments or dose-response models. To address this gap, we propose a novel methodology that frames the causal dose-response problem as a covariate shift, leveraging weighted conformal prediction. By incorporating propensity estimation, conformal predictive systems, and likelihood ratios, we present a practical solution for generating prediction intervals for dose-response models. Additionally, our method approximates local coverage for every treatment value by applying kernel functions as weights in weighted conformal prediction. Finally, we use a new synthetic benchmark dataset to demonstrate the significance of covariate shift assumptions in achieving robust prediction intervals for dose-response models.</td>
</tr>
<tr>
<td>加速边缘设备上的PoT量化</td>
<td>非均匀量化，例如2的幂（PoT）量化，比均匀量化更好地匹配数据分布，从而减少了深度神经网络（DNN）的量化误差。PoT量化还允许用位移操作替代乘法，但关于基于位移的加速器在PoT量化中的效率的研究有限。此外，现有的在边缘设备上加速PoT量化DNN的流水线并未开源。在本文中，我们首先为不同的PoT量化方法设计了基于位移的处理单元（shift-PE），并使用合成基准评估了它们的效率。然后，我们使用我们最有效的shift-PE设计了一个基于位移的加速器，并提出了PoTAcc，一个用于资源受限边缘设备上PoT量化DNN端到端加速的开源流水线。使用PoTAcc，我们评估了我们的基于位移的加速器在三个DNN上的性能。平均而言，与基于乘法器的加速器相比，它实现了1.23倍的加速和1.24倍的能耗降低，与仅使用CPU的执行相比，实现了2.46倍的加速和1.83倍的能耗降低。我们的代码可在https://github.com/gicLAB/PoTAcc获取。</td>
<td>Rappy Saha</td>
<td><a href="http://arxiv.org/pdf/2409.20403v1">PDF</a></td>
<td>N/A</td>
<td>Accelerating PoT Quantization on Edge Devices</td>
<td>Non-uniform quantization, such as power-of-two (PoT) quantization, matches data distributions better than uniform quantization, which reduces the quantization error of Deep Neural Networks (DNNs). PoT quantization also allows bit-shift operations to replace multiplications, but there are limited studies on the efficiency of shift-based accelerators for PoT quantization. Furthermore, existing pipelines for accelerating PoT-quantized DNNs on edge devices are not open-source. In this paper, we first design shift-based processing elements (shift-PE) for different PoT quantization methods and evaluate their efficiency using synthetic benchmarks. Then we design a shift-based accelerator using our most efficient shift-PE and propose PoTAcc, an open-source pipeline for end-to-end acceleration of PoT-quantized DNNs on resource-constrained edge devices. Using PoTAcc, we evaluate the performance of our shift-based accelerator across three DNNs. On average, it achieves a 1.23x speedup and 1.24x energy reduction compared to a multiplier-based accelerator, and a 2.46x speedup and 1.83x energy reduction compared to CPU-only execution. Our code is available at https://github.com/gicLAB/PoTAcc</td>
</tr>
<tr>
<td>反刻板印象的预测文本建议并不能可靠地产生反刻板印象的写作</td>
<td>基于人工智能的系统，如语言模型，能够复制并放大其训练数据中反映的社会偏见。除了其他值得质疑的行为外，这可能导致语言模型生成的文本——以及文本建议——包含规范上不适当的刻板印象关联。在本文中，我们探讨了在预测文本场景中，如何对语言模型进行“去偏”处理会影响人们使用该模型撰写故事的情况。我们发现（n=414），在某些情况下，符合常见社会刻板印象的语言模型建议更有可能被人类作者接受。相反，尽管反刻板印象的语言模型建议有时会提高反刻板印象故事的生成率，但这种影响远不足以导致“完全去偏”的故事。</td>
<td>Connor Baumler</td>
<td><a href="http://arxiv.org/pdf/2409.20390v1">PDF</a></td>
<td>N/A</td>
<td>Anti-stereotypical Predictive Text Suggestions Do Not Reliably Yield Anti-stereotypical Writing</td>
<td>AI-based systems such as language models can replicate and amplify social biases reflected in their training data. Among other questionable behavior, this can lead to LM-generated text--and text suggestions--that contain normatively inappropriate stereotypical associations. In this paper, we consider the question of how "debiasing" a language model impacts stories that people write using that language model in a predictive text scenario. We find that (n=414), in certain scenarios, language model suggestions that align with common social stereotypes are more likely to be accepted by human authors. Conversely, although anti-stereotypical language model suggestions sometimes lead to an increased rate of anti-stereotypical stories, this influence is far from sufficient to lead to "fully debiased" stories.</td>
</tr>
<tr>
<td>等等，但泰诺就是对乙酰氨基酚...研究并提升语言模型抵抗错误信息请求的能力</td>
<td>背景：大型语言模型（LLMs）被训练来遵循指示，但这引入了一个漏洞，即盲目服从用户请求，即使这些请求会产生错误信息。在医学领域，这可能会加速生成影响人类福祉的错误信息。  目标/方法：我们分析了在模型知道请求不合逻辑的情况下，生成关于药物的误导性内容的服从情况。我们研究了通过上下文方向和指令调整LLMs，使其优先考虑逻辑推理而非服从，是否能降低错误信息的风险。  结果：尽管所有前沿LLMs都服从了生成错误信息的请求，但基于提示和基于参数的方法都能提高对请求中逻辑缺陷的检测，并防止医疗错误信息的传播。  结论：将LLMs调整为优先考虑逻辑而非服从，可能会减少被利用来生成医疗错误信息的风险。</td>
<td>Shan Chen</td>
<td><a href="http://arxiv.org/pdf/2409.20385v1">PDF</a></td>
<td>N/A</td>
<td>Wait, but Tylenol is Acetaminophen... Investigating and Improving Language Models' Ability to Resist Requests for Misinformation</td>
<td>Background: Large language models (LLMs) are trained to follow directions, but this introduces a vulnerability to blindly comply with user requests even if they generate wrong information. In medicine, this could accelerate the generation of misinformation that impacts human well-being.   Objectives/Methods: We analyzed compliance to requests to generate misleading content about medications in settings where models know the request is illogical. We investigated whether in-context directions and instruction-tuning of LLMs to prioritize logical reasoning over compliance reduced misinformation risk.   Results: While all frontier LLMs complied with misinformation requests, both prompt-based and parameter-based approaches can improve the detection of logic flaws in requests and prevent the dissemination of medical misinformation.   Conclusion: Shifting LLMs to prioritize logic over compliance could reduce risks of exploitation for medical misinformation.</td>
</tr>
<tr>
<td>超越PINN的衍生病理学：变量分裂策略与收敛性分析</td>
<td>物理信息神经网络（PINNs）最近作为一种解决各种问题中偏微分方程（PDEs）的有效方法而出现。大量研究集中在PINNs的失败模式上，因为它们在预测中经常出现不准确的情况。然而，大多数研究都是基于这样一个前提，即将损失函数最小化到零会导致网络收敛到控制PDE的解。在本研究中，我们证明了PINNs遇到了一个根本问题，即该前提是无效的。我们还揭示了这个问题源于无法调控预测解的导数行为。受PINNs的“导数病理”启发，我们提出了一种“变量分裂”策略，通过将解的梯度参数化为辅助变量来解决这个问题。我们证明，使用辅助变量通过直接监控和调控预测解的梯度，可以规避导数病理。此外，我们证明了所提出的方法保证了对二阶线性PDE的广义解的收敛，表明其在各种问题中的适用性。</td>
<td>Yesom Park</td>
<td><a href="http://arxiv.org/pdf/2409.20383v1">PDF</a></td>
<td>N/A</td>
<td>Beyond Derivative Pathology of PINNs: Variable Splitting Strategy with Convergence Analysis</td>
<td>Physics-informed neural networks (PINNs) have recently emerged as effective methods for solving partial differential equations (PDEs) in various problems. Substantial research focuses on the failure modes of PINNs due to their frequent inaccuracies in predictions. However, most are based on the premise that minimizing the loss function to zero causes the network to converge to a solution of the governing PDE. In this study, we prove that PINNs encounter a fundamental issue that the premise is invalid. We also reveal that this issue stems from the inability to regulate the behavior of the derivatives of the predicted solution. Inspired by the \textit{derivative pathology} of PINNs, we propose a \textit{variable splitting} strategy that addresses this issue by parameterizing the gradient of the solution as an auxiliary variable. We demonstrate that using the auxiliary variable eludes derivative pathology by enabling direct monitoring and regulation of the gradient of the predicted solution. Moreover, we prove that the proposed method guarantees convergence to a generalized solution for second-order linear PDEs, indicating its applicability to various problems.</td>
</tr>
<tr>
<td>跨语言文本到语音系统中的词级声调模型</td>
<td>本文提出了一种针对俄语的单词级语调模型，并展示了如何将其推广到其他语言。所提出的模型适用于自动数据标注，并可扩展应用于文本到语音系统。它还可以通过使用基于规则的算法或通过语言模型预测轮廓来实现语调轮廓建模。关键思想是部分消除与单词中重音音节不同位置相关的变异性。这是通过同时应用音高简化与动态时间规整聚类实现的。所提出的模型可以用作语调研究的工具，或作为文本到语音系统中韵律描述的骨干。作为模型的优势，我们展示了它与现有语调系统的关系，以及使用语言模型进行韵律预测的可能性。最后，我们展示了系统对参数变化具有鲁棒性的一些实际证据。</td>
<td>Tomilov A. A.</td>
<td><a href="http://arxiv.org/pdf/2409.20374v1">PDF</a></td>
<td>N/A</td>
<td>Word-wise intonation model for cross-language TTS systems</td>
<td>In this paper we propose a word-wise intonation model for Russian language and show how it can be generalized for other languages. The proposed model is suitable for automatic data markup and its extended application to text-to-speech systems. It can also be implemented for an intonation contour modeling by using rule-based algorithms or by predicting contours with language models. The key idea is a partial elimination of the variability connected with different placements of a stressed syllable in a word. It is achieved with simultaneous applying of pitch simplification with a dynamic time warping clustering. The proposed model could be used as a tool for intonation research or as a backbone for prosody description in text-to-speech systems. As the advantage of the model, we show its relations with the existing intonation systems as well as the possibility of using language models for prosody prediction. Finally, we demonstrate some practical evidence of the system robustness to parameter variations.</td>
</tr>
<tr>
<td>非平稳时间序列预测的频率自适应归一化</td>
<td>时间序列预测通常需要处理具有演变趋势和季节性模式的非平稳数据。为了解决非平稳性问题，最近提出了可逆实例归一化方法，通过某些统计量（如均值和方差）来减轻趋势的影响。尽管这些方法展示了提高的预测准确性，但它们仅限于表达基本趋势，无法处理季节性模式。为了克服这一局限性，本文提出了一种新的实例归一化解决方案，称为频率自适应归一化（Frequency Adaptive Normalization, FAN），该方法扩展了实例归一化在处理动态趋势和季节性模式方面的能力。具体而言，我们采用傅里叶变换来识别覆盖大多数非平稳因素的实例主导频率成分。此外，输入和输出之间这些频率成分的差异被显式建模为一个预测任务，使用简单的多层感知机（MLP）模型进行处理。FAN是一种与模型无关的方法，可以应用于任意预测模型。我们在四个广泛使用的预测模型上实例化了FAN，并在八个基准数据集上评估了其预测性能的提升。FAN展示了显著的性能提升，在均方误差（MSE）上实现了7.76%到37.90%的平均改进。</td>
<td>Weiwei Ye</td>
<td><a href="http://arxiv.org/pdf/2409.20371v1">PDF</a></td>
<td>N/A</td>
<td>Frequency Adaptive Normalization For Non-stationary Time Series Forecasting</td>
<td>Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns. To address the non-stationarity, reversible instance normalization has been recently proposed to alleviate impacts from the trend with certain statistical measures, e.g., mean and variance. Although they demonstrate improved predictive accuracy, they are limited to expressing basic trends and are incapable of handling seasonal patterns. To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns. Specifically, we employ the Fourier transform to identify instance-wise predominant frequent components that cover most non-stationary factors. Furthermore, the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model. FAN is a model-agnostic method that can be applied to arbitrary predictive backbones. We instantiate FAN on four widely used forecasting models as the backbone and evaluate their prediction performance improvements on eight benchmark datasets. FAN demonstrates significant performance advancement, achieving 7.76% ~ 37.90% average improvements in MSE.</td>
</tr>
<tr>
<td>完美融合：用混合评判重新定义RLHF</td>
<td>从人类反馈中进行强化学习（RLHF）已成为微调大型语言模型（LLM）的主导方法。然而，RLHF在多任务学习（MTL）中存在局限性，主要挑战包括奖励作弊和极端多目标优化（即多个有时相互冲突的目标之间的权衡）。目前，将RLHF应用于MTL需要仔细调整奖励模型和数据组合的权重，这通常依赖于人类的直觉，且不具备普遍性。在本研究中，我们引入了一种新的训练后范式，称为约束生成策略优化（CGPO）。CGPO的核心是混合评判者（Mixture of Judges, MoJ），结合成本高效的约束策略优化与分层技术，能够以系统化的方式识别RLHF中的完美组合。CGPO在理论保证下展现出强大的实证结果，无需广泛的超级参数调优，并且可以即插即用于常见的训练后流程。通过这些方法，CGPO能够检测并缓解奖励作弊行为，同时在大量目标中达到帕累托最优点。我们的实证评估表明，CGPO在多种任务中显著优于标准的RLHF算法，如PPO和DPO，这些任务包括通用聊天、STEM问题、指令跟随和编码。具体而言，CGPO在AlpacaEval-2（通用聊天）中提升了7.4%，在Arena-Hard（STEM与推理）中提升了12.5%，并在数学和编码等领域持续取得进展。值得注意的是，尽管PPO常用，但在流行的编码基准测试中容易出现严重的奖励作弊问题，而CGPO成功解决了这一问题。这一RLHF的突破不仅解决了奖励作弊和极端多目标优化的挑战，还推动了通用LLM在多样化应用中的对齐技术的发展。</td>
<td>Tengyu Xu</td>
<td><a href="http://arxiv.org/pdf/2409.20370v1">PDF</a></td>
<td>N/A</td>
<td>The Perfect Blend: Redefining RLHF with Mixture of Judges</td>
<td>Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.   Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM &amp; reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.</td>
</tr>
<tr>
<td>通过任务驱动的表示解开新加坡式英语话语粒子</td>
<td>新加坡式英语（Singlish），或正式称为新加坡英语口语，是一种源自东南亚国家新加坡的基于英语的克里奥尔语。该语言融合了汉语方言、马来语、泰米尔语等多种语言的影响。理解新加坡式英语的一个基本任务是首先理解其话语标记的语用功能，新加坡式英语在很大程度上依赖这些话语标记来传达意义。本研究通过任务驱动的表征学习，初步尝试解构新加坡式英语的话语标记（lah、meh 和 hor）。解构后，我们将这些话语标记进行聚类，以区分其语用功能，并进行新加坡式英语到英语的机器翻译。我们的工作提供了一种计算方法来理解新加坡式英语的话语标记，并为更深入地理解该语言及其用法开辟了途径。</td>
<td>Linus Tze En Foo</td>
<td><a href="http://arxiv.org/pdf/2409.20366v1">PDF</a></td>
<td>N/A</td>
<td>Disentangling Singlish Discourse Particles with Task-Driven Representation</td>
<td>Singlish, or formally Colloquial Singapore English, is an English-based creole language originating from the SouthEast Asian country Singapore. The language contains influences from Sinitic languages such as Chinese dialects, Malay, Tamil and so forth. A fundamental task to understanding Singlish is to first understand the pragmatic functions of its discourse particles, upon which Singlish relies heavily to convey meaning. This work offers a preliminary effort to disentangle the Singlish discourse particles (lah, meh and hor) with task-driven representation learning. After disentanglement, we cluster these discourse particles to differentiate their pragmatic functions, and perform Singlish-to-English machine translation. Our work provides a computational method to understanding Singlish discourse particles, and opens avenues towards a deeper comprehension of the language and its usage.</td>
</tr>
<tr>
<td>旋转运行时平滑：无需训练的激活平滑器，用于精确的INT4推理</td>
<td>大规模语言模型在扩展参数规模后展现出了显著的能力。然而，由于其庞大的规模，服务这些大型语言模型会产生巨大的计算和内存移动成本。量化方法被用来降低服务成本和延迟。然而，激活中的异常值阻碍了INT4权重-激活量化的进展。现有方法将异常值和正常值分离到两个矩阵中，或将异常值从激活迁移到权重，但这些方法存在高延迟或精度下降的问题。基于对大规模语言模型激活的观察，异常值可以分为通道间异常值和尖峰异常值。在这项工作中，我们提出了旋转运行时平滑（Rotated Runtime Smooth, RRS），这是一种即插即用的量化激活平滑器，由运行时平滑和旋转操作组成。运行时平滑（Runtime Smooth, RS）通过在运行时使用通道最大值平滑激活来消除通道间异常值。旋转操作可以缩小尖峰异常值与正常值之间的差距，减轻通道间平滑导致的受害者效应。所提出的方法在LLaMA和Qwen系列模型中优于最先进的方法，并将INT4推理的WikiText-2困惑度从57.33提高到6.66。</td>
<td>Ke Yi</td>
<td><a href="http://arxiv.org/pdf/2409.20361v1">PDF</a></td>
<td>N/A</td>
<td>Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference</td>
<td>Large language models have demonstrated promising capabilities upon scaling up parameters. However, serving large language models incurs substantial computation and memory movement costs due to their large scale. Quantization methods have been employed to reduce service costs and latency. Nevertheless, outliers in activations hinder the development of INT4 weight-activation quantization. Existing approaches separate outliers and normal values into two matrices or migrate outliers from activations to weights, suffering from high latency or accuracy degradation. Based on observing activations from large language models, outliers can be classified into channel-wise and spike outliers. In this work, we propose Rotated Runtime Smooth (RRS), a plug-and-play activation smoother for quantization, consisting of Runtime Smooth and the Rotation operation. Runtime Smooth (RS) is introduced to eliminate channel-wise outliers by smoothing activations with channel-wise maximums during runtime. The rotation operation can narrow the gap between spike outliers and normal values, alleviating the effect of victims caused by channel-wise smoothing. The proposed method outperforms the state-of-the-art method in the LLaMA and Qwen families and improves WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference.</td>
</tr>
<tr>
<td>使用神经量子核的卫星图像分类</td>
<td>尽管在理论上付出了巨大的努力，量子机器学习在短期内应用于现实世界场景的实际应用仍然难以捉摸。图像分类是经典模型的常见任务，已被用于使用简单数据集来基准测试量子算法，但只有少数研究解决了复杂的真实数据分类挑战。在这项工作中，我们通过专注于卫星图像分类来填补这一空白，这对地球观测（EO）行业特别感兴趣。我们首先通过降低其维度对选定的复杂数据集进行预处理。随后，我们使用神经量子核（NQKs）——由训练好的量子神经网络（QNNs）构建的嵌入量子核（EQKs）——来分类包含太阳能电池板的图像。我们探索了$1$-to-$n$和$n$-to-$n$ NQKs。在前者中，单量子比特QNN的训练参数构建了一个$n$量子比特EQK，使用三个特征实现了超过86%的平均测试准确率。在后者中，我们迭代地训练一个$n$量子比特QNN以确保可扩展性，使用结果架构直接形成一个$n$量子比特EQK。在这种情况下，对于三个特征和8个量子比特，测试准确率超过88%。此外，我们展示了结果对QNN次优训练的鲁棒性。</td>
<td>Pablo Rodriguez-Grasa</td>
<td><a href="http://arxiv.org/pdf/2409.20356v1">PDF</a></td>
<td>N/A</td>
<td>Satellite image classification with neural quantum kernels</td>
<td>A practical application of quantum machine learning in real-world scenarios in the short term remains elusive, despite significant theoretical efforts. Image classification, a common task for classical models, has been used to benchmark quantum algorithms with simple datasets, but only few studies have tackled complex real-data classification challenges. In this work, we address such a gap by focusing on the classification of satellite images, a task of particular interest to the earth observation (EO) industry. We first preprocess the selected intrincate dataset by reducing its dimensionality. Subsequently, we employ neural quantum kernels (NQKs)- embedding quantum kernels (EQKs) constructed from trained quantum neural networks (QNNs)- to classify images which include solar panels. We explore both $1$-to-$n$ and $n$-to-$n$ NQKs. In the former, parameters from a single-qubit QNN's training construct an $n$-qubit EQK achieving a mean test accuracy over 86% with three features. In the latter, we iteratively train an $n$-qubit QNN to ensure scalability, using the resultant architecture to directly form an $n$-qubit EQK. In this case, a test accuracy over 88% is obtained for three features and 8 qubits. Additionally, we show that the results are robust against a suboptimal training of the QNN.</td>
</tr>
<tr>
<td>CableInspect-AD：一个专家标注的异常检测数据集</td>
<td>机器学习模型越来越多地被部署在现实世界的场景中。然而，关于这些模型在特定和关键应用中的可迁移性的系统研究在研究文献中却相对不足。一个重要的例子是用于机器人电力线路巡检的视觉异常检测（VAD）。尽管现有的VAD方法在受控环境中表现良好，但现实世界中存在多样且不可预见的异常情况，这些情况是当前数据集无法捕捉的。为了填补这一空白，我们引入了$\textit{CableInspect-AD}$，这是一个由加拿大公共事业公司Hydro-Québec的领域专家创建和标注的高质量、公开可用的数据集。该数据集包含高分辨率图像，涵盖了具有不同严重程度的真实世界异常情况。为了应对收集多样化异常和正常样本以设定检测阈值的挑战，我们对著名的PatchCore算法进行了改进。这一改进使其能够在标签数据有限的情况下使用。我们还提出了一种基于交叉验证的综合评估协议，以评估模型的性能。我们评估了我们的$\textit{Enhanced-PatchCore}$在少样本和多样本检测中的表现，以及视觉语言模型在零样本检测中的表现。尽管这些模型显示出一定的潜力，但它们在检测所有异常方面仍面临困难，这突显了该数据集作为对更广泛研究社区具有挑战性的基准的价值。项目页面：https://mila-iqia.github.io/cableinspect-ad/。</td>
<td>Akshatha Arodi</td>
<td><a href="http://arxiv.org/pdf/2409.20353v1">PDF</a></td>
<td>N/A</td>
<td>CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset</td>
<td>Machine learning models are increasingly being deployed in real-world contexts. However, systematic studies on their transferability to specific and critical applications are underrepresented in the research literature. An important example is visual anomaly detection (VAD) for robotic power line inspection. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. To address this gap, we introduce $\textit{CableInspect-AD}$, a high-quality, publicly available dataset created and annotated by domain experts from Hydro-Qu\'ebec, a Canadian public utility. This dataset includes high-resolution images with challenging real-world anomalies, covering defects with varying severity levels. To address the challenges of collecting diverse anomalous and nominal examples for setting a detection threshold, we propose an enhancement to the celebrated PatchCore algorithm. This enhancement enables its use in scenarios with limited labeled data. We also present a comprehensive evaluation protocol based on cross-validation to assess models' performances. We evaluate our $\textit{Enhanced-PatchCore}$ for few-shot and many-shot detection, and Vision-Language Models for zero-shot detection. While promising, these models struggle to detect all anomalies, highlighting the dataset's value as a challenging benchmark for the broader research community. Project page: https://mila-iqia.github.io/cableinspect-ad/.</td>
</tr>
<tr>
<td>基于对比学习的GAN多阶段渐进微调SNN与基于RL的外部优化增强</td>
<td>深度学习在癌症研究中的应用，特别是在早期诊断、病例理解和治疗策略设计方面，强调了高质量数据的必要性。生成式人工智能，尤其是生成对抗网络（GANs），已成为解决类别不平衡、鲁棒学习和模型训练等挑战的主要方案，同时解决了患者隐私和真实数据稀缺带来的问题。尽管GANs具有潜力，但它们面临着一些固有的和与组织病理学数据相关的特定挑战。固有问题包括训练不平衡、模式崩溃、由于判别器反馈不足导致的线性学习，以及由于严格反馈导致的硬边界收敛。组织病理学数据因其复杂的表示、高空间分辨率和多尺度特征而提出了独特的挑战。为了应对这些挑战，我们提出了一种由两个部分组成的框架。首先，我们引入了一种基于对比学习的分阶段渐进微调孪生神经网络（MFT-SNN），用于评估组织病理学补丁之间的相似性。其次，我们在GAN训练循环中实现了一个基于强化学习的外部优化器（RL-EO），作为奖励信号生成器。修改后的判别器损失函数结合了加权奖励，指导GAN在最小化损失的同时最大化该奖励。这种方法为判别器提供了外部优化指导，防止生成器过拟合并确保平滑收敛。我们提出的解决方案已与最先进的（SOTA）GANs和去噪扩散概率模型进行了基准测试，在包括FID分数、KID分数、感知路径长度和下游分类任务在内的各种指标上均优于之前的SOTA。</td>
<td>Osama Mustafa</td>
<td><a href="http://arxiv.org/pdf/2409.20340v2">PDF</a></td>
<td>N/A</td>
<td>Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization</td>
<td>The application of deep learning in cancer research, particularly in early diagnosis, case understanding, and treatment strategy design, emphasizes the need for high-quality data. Generative AI, especially Generative Adversarial Networks (GANs), has emerged as a leading solution to challenges like class imbalance, robust learning, and model training, while addressing issues stemming from patient privacy and the scarcity of real data. Despite their promise, GANs face several challenges, both inherent and specific to histopathology data. Inherent issues include training imbalance, mode collapse, linear learning from insufficient discriminator feedback, and hard boundary convergence due to stringent feedback. Histopathology data presents a unique challenge with its complex representation, high spatial resolution, and multiscale features. To address these challenges, we propose a framework consisting of two components. First, we introduce a contrastive learning-based Multistage Progressive Finetuning Siamese Neural Network (MFT-SNN) for assessing the similarity between histopathology patches. Second, we implement a Reinforcement Learning-based External Optimizer (RL-EO) within the GAN training loop, serving as a reward signal generator. The modified discriminator loss function incorporates a weighted reward, guiding the GAN to maximize this reward while minimizing loss. This approach offers an external optimization guide to the discriminator, preventing generator overfitting and ensuring smooth convergence. Our proposed solution has been benchmarked against state-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model, outperforming previous SOTA across various metrics, including FID score, KID score, Perceptual Path Length, and downstream classification tasks.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.525ec568.min.js"></script>
      
    
  </body>
</html>