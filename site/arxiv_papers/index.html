
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../biorxiv_papers/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.38">
    
    
      
        <title>Arxiv Papers - Arxiv Daily</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#arxiv-papers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Arxiv Daily" class="md-header__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arxiv Daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Arxiv Papers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Arxiv Daily" class="md-nav__button md-logo" aria-label="Arxiv Daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Arxiv Daily
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Arxiv Papers
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../biorxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BioRxiv Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../medrxiv_papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MedRxiv Papers
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="arxiv-papers">Arxiv Papers</h1>
<table>
<thead>
<tr>
<th>标题</th>
<th>摘要</th>
<th>作者</th>
<th>PDF链接</th>
<th>代码仓库</th>
<th>Title</th>
<th>Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>Molmo 和 PixMo：为最先进的跨模态模型提供开放权重和开放数据</td>
<td>当今最先进的多模态模型仍然是专有的。最强大的开源权重模型在很大程度上依赖于来自专有视觉语言模型（VLM）的合成数据以实现良好性能，实际上是将这些封闭模型提炼为开放模型。因此，社区仍然缺乏关于如何从头构建高性能VLM的基础知识。我们提出了Molmo，这是一个在其开放性类别中处于最先进水平的VLM新家族。我们的关键创新是一种全新、高度详细的全由人类标注者使用基于语音的描述收集的图像描述数据集。为了支持广泛的用户交互，我们还引入了一个多样化的微调数据集混合，包括自然环境中的问答和创新的二维指向数据。我们方法的成功依赖于对模型架构细节的精心选择、经过良好调优的训练流程，以及最关键的，我们新收集数据集的质量，这些都将被发布。Molmo家族中最佳的72B模型不仅在开放权重和数据模型类别中表现优于其他模型，而且在学术基准和人类评估中与GPT-4o、Claude 3.5和Gemini 1.5等专有系统相比也表现出色。我们将在不久的将来发布所有模型权重、描述和微调数据以及源代码。选定的模型权重、推理代码和演示可在https://molmo.allenai.org获取。</td>
<td>Matt Deitke</td>
<td><a href="http://arxiv.org/pdf/2409.17146v1">PDF</a></td>
<td>N/A</td>
<td>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models</td>
<td>Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&amp;A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org.</td>
</tr>
<tr>
<td>DreamWaltz-G：从骨骼引导的2D扩散生成富有表现力的3D高斯头像</td>
<td>利用预训练的二维扩散模型和分数蒸馏采样（SDS），最近的方法在文本到3D头像生成方面展示了有前景的结果。然而，生成能够进行表情动画的高质量3D头像仍然具有挑战性。在这项工作中，我们提出了DreamWaltz-G，一种从文本生成可动画3D头像的新型学习框架。该框架的核心在于骨骼引导的分数蒸馏和混合3D高斯头像表示。具体来说，所提出的骨骼引导分数蒸馏将3D人体模板中的骨骼控制整合到2D扩散模型中，增强了SDS监督在视角和人体姿态方面的一致性。这有助于生成高质量的头像，缓解了多面、多余肢体和模糊等问题。所提出的混合3D高斯头像表示基于高效的3D高斯，结合了神经隐式场和参数化3D网格，以实现实时渲染、稳定的SDS优化和表情动画。广泛的实验表明，DreamWaltz-G在生成和动画3D头像方面非常有效，在视觉质量和动画表现力方面优于现有方法。我们的框架还支持多样化的应用，包括人类视频重演和多主体场景合成。</td>
<td>Yukun Huang</td>
<td><a href="http://arxiv.org/pdf/2409.17145v1">PDF</a></td>
<td>N/A</td>
<td>DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</td>
<td>Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.</td>
</tr>
<tr>
<td>差分隐私正则化：通过损失函数正则化保护训练数据</td>
<td>基于神经网络的机器学习模型训练需要大量数据集，这些数据集可能包含敏感信息。然而，这些模型不应泄露数据集中的私人信息。差分隐私随机梯度下降（DP-SGD）算法需要对标准随机梯度下降（SGD）算法进行修改，以便训练新模型。在本文中，我们提出了一种新的正则化策略，以更高效的方式实现相同的目标。</td>
<td>Francisco Aguilera-Martínez</td>
<td><a href="http://arxiv.org/pdf/2409.17144v1">PDF</a></td>
<td>N/A</td>
<td>Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization</td>
<td>Training machine learning models based on neural networks requires large datasets, which may contain sensitive information. The models, however, should not expose private information from these datasets. Differentially private SGD [DP-SGD] requires the modification of the standard stochastic gradient descent [SGD] algorithm for training new models. In this short paper, a novel regularization strategy is proposed to achieve the same goal in a more efficient manner.</td>
</tr>
<tr>
<td>FineZip：推动大型语言模型在实际无损文本压缩中的极限</td>
<td>尽管语言建模目标已被证明与压缩有着深刻的联系，但令人惊讶的是，现代大型语言模型（LLMs）并未被应用于实际的文本压缩系统中。本文对基于神经网络和变换器的压缩技术进行了深入分析，以解答这一问题。我们比较了传统的文本压缩系统与基于神经网络和LLM的文本压缩方法。尽管基于LLM的系统在性能上显著优于传统的压缩方法，但它们在实际应用中极为不切实际。具体而言，最近的一个文本压缩系统LLMZip使用了Llama3-8B模型，尽管在压缩比率上有巨大提升，但压缩10 MB的文本需要9.5天的时间。为了克服这一问题，我们提出了FineZip——一种结合了在线记忆和动态上下文概念的新型基于LLM的文本压缩系统，极大地减少了压缩时间。与LLMZip相比，FineZip能在约4小时内完成上述语料的压缩，比LLMZip快了54倍，且性能相当。FineZip在压缩比率上大幅超越了传统的算法压缩方法，提升了约50%。通过这项工作，我们迈出了使基于LLM的无损文本压缩成为现实的第一步。尽管FineZip在这方面迈出了重要一步，但LLMs仍未成为大规模文本压缩的可行解决方案。我们希望我们的工作能为未来的研究和创新铺平道路，以解决这一问题。</td>
<td>Fazal Mittu</td>
<td><a href="http://arxiv.org/pdf/2409.17141v1">PDF</a></td>
<td>N/A</td>
<td>FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression</td>
<td>While the language modeling objective has been shown to be deeply connected with compression, it is surprising that modern LLMs are not employed in practical text compression systems. In this paper, we provide an in-depth analysis of neural network and transformer-based compression techniques to answer this question. We compare traditional text compression systems with neural network and LLM-based text compression methods. Although LLM-based systems significantly outperform conventional compression methods, they are highly impractical. Specifically, LLMZip, a recent text compression system using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with huge improvements in compression ratios. To overcome this, we present FineZip - a novel LLM-based text compression system that combines ideas of online memorization and dynamic context to reduce the compression time immensely. FineZip can compress the above corpus in approximately 4 hours compared to 9.5 days, a 54 times improvement over LLMZip and comparable performance. FineZip outperforms traditional algorithmic compression methods with a large margin, improving compression ratios by approximately 50\%. With this work, we take the first step towards making lossless text compression with LLMs a reality. While FineZip presents a significant step in that direction, LLMs are still not a viable solution for large-scale text compression. We hope our work paves the way for future research and innovation to solve this problem.</td>
</tr>
<tr>
<td>动态学习：基于动态无人机团队的无人机通信网络自主调节</td>
<td>基于无人机的通信网络（UCN）是未来移动网络的关键组成部分。为了应对UCN中动态环境的变化，强化学习（RL）因其强大的自适应决策能力而成为一种有前景的解决方案，且无需依赖环境模型。然而，大多数现有的基于RL的研究主要集中在控制策略设计上，假设无人机数量是固定的。很少有研究探讨当服务无人机动态变化时，如何自适应地调节UCN。本文讨论了在动态无人机集合情况下，基于RL的策略设计以实现UCN的自适应调节，涵盖了通用UCN中的反应性策略和太阳能UCN中的前瞻性策略。首先概述了UCN和RL框架，然后详细阐述了潜在的研究方向及其关键挑战和可能的解决方案。最后，以我们最近的一些工作为例，展示了如何利用不同的RL算法处理动态无人机团队，以激发创新的方法。</td>
<td>Ran Zhang</td>
<td><a href="http://arxiv.org/pdf/2409.17139v1">PDF</a></td>
<td>N/A</td>
<td>Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew</td>
<td>Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key component in future mobile networking. To handle the dynamic environments in UCNs, reinforcement learning (RL) has been a promising solution attributed to its strong capability of adaptive decision-making free of the environment models. However, most existing RL-based research focus on control strategy design assuming a fixed set of UAVs. Few works have investigated how UCNs should be adaptively regulated when the serving UAVs change dynamically. This article discusses RL-based strategy design for adaptive UCN regulation given a dynamic UAV set, addressing both reactive strategies in general UCNs and proactive strategies in solar-powered UCNs. An overview of the UCN and the RL framework is first provided. Potential research directions with key challenges and possible solutions are then elaborated. Some of our recent works are presented as case studies to inspire innovative ways to handle dynamic UAV crew with different RL algorithms.</td>
</tr>
<tr>
<td>有限时间马尔可夫决策过程（MDPs）中具有一般状态和动作的政策优化景观</td>
<td>策略梯度方法在强化学习中得到了广泛应用。然而，策略优化的非凸性给理解策略梯度方法的全局收敛性带来了重大挑战。对于一类具有一般状态和动作空间的有限时域马尔可夫决策过程（MDPs），我们开发了一个框架，该框架提供了一组易于验证的假设，以确保策略优化的Kurdyka-Lojasiewicz（KL）条件。利用KL条件，尽管策略优化是非凸的，策略梯度方法仍能以非渐近速率收敛到全局最优策略。我们的研究结果在各种控制和运营模型中得到了应用，包括熵正则化的表格MDPs、线性二次调节器（LQR）问题、随机库存模型和随机现金余额问题，我们证明了通过随机策略梯度方法，可以在$\tilde{\mathcal{O}}(\epsilon^{-1})$的样本量和关于规划时域的多项式时间内获得$\epsilon$-最优策略。我们的研究结果在文献中首次建立了具有马尔可夫调制需求的多元库存系统和随机现金余额问题的样本复杂度。</td>
<td>Xin Chen</td>
<td><a href="http://arxiv.org/pdf/2409.17138v1">PDF</a></td>
<td>N/A</td>
<td>Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action</td>
<td>Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy optimization. Leveraging the KL condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an $\epsilon$-optimal policy can be obtained using a sample size in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature.</td>
</tr>
<tr>
<td>PACE：将参数高效微调中的泛化与一致性正则化相结合</td>
<td>参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）有效地将预训练的视觉变换器适应于下游任务。然而，为了提升任务性能的优化往往以微调模型泛化能力的牺牲为代价。为了解决这一问题，我们从理论上将训练过程中较小的权重梯度范数和较大的数据集与改进的模型泛化能力联系起来。受此联系启发，我们提出通过减小梯度范数来增强泛化能力，并使微调模型与预训练模型对齐，以保留大规模预训练数据中的知识。然而，简单的对齐并不能保证梯度的减小，反而可能导致梯度爆炸，增加管理梯度的难度。为了解决这些问题，我们提出了PACE，将参数高效微调的泛化能力与一致性正则化相结合。我们通过乘性噪声扰动从适配器学习到的特征，并确保微调模型在不同扰动下对同一样本保持一致。理论分析表明，PACE不仅隐式地对梯度进行正则化以增强泛化能力，还隐式地对齐了微调模型和预训练模型，以保留知识。实验证据支持了我们的理论。在四个视觉适应任务中，PACE优于现有的PEFT方法：VTAB-1k、FGVC、少样本学习和领域适应。代码将在https://github.com/MaxwellYaoNi/PACE提供。</td>
<td>Yao Ni</td>
<td><a href="http://arxiv.org/pdf/2409.17137v1">PDF</a></td>
<td>N/A</td>
<td>PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization</td>
<td>Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE outperforms existing PEFT methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and domain adaptation. Code will be available at https://github.com/MaxwellYaoNi/PACE</td>
</tr>
<tr>
<td>评估孟加拉社交媒体评论中对不同群体的毒性程度：一项全面调查</td>
<td>社交媒体平台在现代社会中扮演着至关重要的角色，它们是沟通、思想交流和建立网络的渠道。然而，通过有毒评论（从冒犯性言论到仇恨言论）滥用这些平台是一个令人担忧的问题。本研究专注于识别针对三个特定群体——跨性别者、原住民和移民——的孟加拉语有毒评论，这些评论来自多个社交媒体来源。研究深入探讨了识别和分类有毒语言的复杂过程，同时考虑了不同程度的有毒性：高、中、低。研究方法包括创建数据集、手动注释，并使用预训练的转换器模型如Bangla-BERT、bangla-bert-base、distil-BERT和Bert-base-multilingual-cased进行分类。采用多种评估指标，如准确率、召回率、精确率和F1分数，来评估模型的有效性。实验结果显示，Bangla-BERT优于其他模型，达到了0.8903的F1分数。这项研究揭示了孟加拉社交媒体对话中有毒性的复杂性，揭示了其对不同人口群体的不同影响。</td>
<td>Mukaffi Bin Moin</td>
<td><a href="http://arxiv.org/pdf/2409.17130v1">PDF</a></td>
<td>N/A</td>
<td>Assessing the Level of Toxicity Against Distinct Groups in Bangla Social Media Comments: A Comprehensive Investigation</td>
<td>Social media platforms have a vital role in the modern world, serving as conduits for communication, the exchange of ideas, and the establishment of networks. However, the misuse of these platforms through toxic comments, which can range from offensive remarks to hate speech, is a concerning issue. This study focuses on identifying toxic comments in the Bengali language targeting three specific groups: transgender people, indigenous people, and migrant people, from multiple social media sources. The study delves into the intricate process of identifying and categorizing toxic language while considering the varying degrees of toxicity: high, medium, and low. The methodology involves creating a dataset, manual annotation, and employing pre-trained transformer models like Bangla-BERT, bangla-bert-base, distil-BERT, and Bert-base-multilingual-cased for classification. Diverse assessment metrics such as accuracy, recall, precision, and F1-score are employed to evaluate the model's effectiveness. The experimental findings reveal that Bangla-BERT surpasses alternative models, achieving an F1-score of 0.8903. This research exposes the complexity of toxicity in Bangla social media dialogues, revealing its differing impacts on diverse demographic groups.</td>
</tr>
<tr>
<td>Blox-Net：利用VLM监督、物理模拟和具备重置功能的机器人进行机器人组装的生成式设计</td>
<td>生成式人工智能系统在生成文本、代码和图像方面展现了令人瞩目的能力。受到工业领域“装配设计”研究丰富历史的启发，我们提出了一项新问题：机器人装配生成设计（Generative Design-for-Robot-Assembly, GDfRA）。该任务要求根据自然语言提示（例如“长颈鹿”）和可用物理组件的图像（如3D打印的积木块）生成一个装配体。输出结果包括这些组件的空间排列以及机器人构建该装配体的指令。输出必须满足两个条件：1) 与请求的对象相似；2) 能够被带有吸盘夹具的6自由度机器人手臂可靠地装配。接着，我们介绍了Blox-Net，这是一个结合了生成视觉语言模型与计算机视觉、仿真、扰动分析、运动规划和物理机器人实验中成熟方法的GDfRA系统，能够在最小人类监督下解决一类GDfRA问题。Blox-Net在其设计的装配体的“可识别性”方面达到了63.5%的Top-1准确率（例如，由视觉语言模型判断其设计是否像长颈鹿）。这些设计在经过自动扰动重新设计后，能够被机器人可靠地装配，在10次连续装配迭代中几乎完美成功，仅在装配前重置时需要人工干预。令人惊讶的是，从文本词（“长颈鹿”）到可靠的物理装配的整个设计过程完全无需人工干预。</td>
<td>Andrew Goldberg</td>
<td><a href="http://arxiv.org/pdf/2409.17126v1">PDF</a></td>
<td>N/A</td>
<td>Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset</td>
<td>Generative AI systems have shown impressive capabilities in creating text, code, and images. Inspired by the rich history of research in industrial ''Design for Assembly'', we introduce a novel problem: Generative Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on a natural language prompt (e.g., ''giraffe'') and an image of available physical components, such as 3D-printed blocks. The output is an assembly, a spatial arrangement of these components, and instructions for a robot to build this assembly. The output must 1) resemble the requested object and 2) be reliably assembled by a 6 DoF robot arm with a suction gripper. We then present Blox-Net, a GDfRA system that combines generative vision language models with well-established methods in computer vision, simulation, perturbation analysis, motion planning, and physical robot experimentation to solve a class of GDfRA problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of 63.5% in the ''recognizability'' of its designed assemblies (eg, resembling giraffe as judged by a VLM). These designs, after automated perturbation redesign, were reliably assembled by a robot, achieving near-perfect success across 10 consecutive assembly iterations with human intervention only during reset prior to assembly. Surprisingly, this entire design process from textual word (''giraffe'') to reliable physical assembly is performed with zero human intervention.</td>
</tr>
<tr>
<td>深度学习和机器学习，推动大数据分析与管理：实用入门</td>
<td>本书探讨了人工智能（AI）、机器学习（ML）和深度学习（DL）在大数据分析和管理中的推动作用。本书专注于简化深度学习背后的复杂数学概念，通过直观的可视化展示和实际案例研究，帮助读者理解神经网络以及卷积神经网络（CNNs）等技术的工作原理。书中介绍了Transformer、GPT、ResNet、BERT和YOLO等多个经典模型和技术，并强调了它们在自然语言处理、图像识别和自动驾驶等领域的应用。此外，本书还强调了预训练模型的重要性，以及它们如何提升模型性能和准确性，并提供了在各种现实场景中应用这些模型的指导。同时，本书概述了SQL和NoSQL数据库等关键大数据管理技术，以及Apache Hadoop和Spark等分布式计算框架，解释了它们在管理和处理海量数据中的重要性。最终，本书强调了掌握深度学习和大数据管理技能作为未来劳动力关键工具的价值，使其成为初学者和有经验专业人士的必备资源。</td>
<td>Benji Peng</td>
<td><a href="http://arxiv.org/pdf/2409.17120v1">PDF</a></td>
<td>N/A</td>
<td>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</td>
<td>This book explores the role of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in driving the progress of big data analytics and management. The book focuses on simplifying the complex mathematical concepts behind deep learning, offering intuitive visualizations and practical case studies to help readers understand how neural networks and technologies like Convolutional Neural Networks (CNNs) work. It introduces several classic models and technologies such as Transformers, GPT, ResNet, BERT, and YOLO, highlighting their applications in fields like natural language processing, image recognition, and autonomous driving. The book also emphasizes the importance of pre-trained models and how they can enhance model performance and accuracy, with instructions on how to apply these models in various real-world scenarios. Additionally, it provides an overview of key big data management technologies like SQL and NoSQL databases, as well as distributed computing frameworks such as Apache Hadoop and Spark, explaining their importance in managing and processing vast amounts of data. Ultimately, the book underscores the value of mastering deep learning and big data management skills as critical tools for the future workforce, making it an essential resource for both beginners and experienced professionals.</td>
</tr>
<tr>
<td>编程每个示例：大规模提升预训练数据质量，如同专家般精准</td>
<td>传统上，大型语言模型的预训练依赖于人类专家设计启发式方法来提升语料库质量，由此产生了众多规则。然而，这些规则缺乏灵活性，无法有效应对每个示例的独特特征。同时，为每个示例应用定制规则对人类专家来说是不切实际的。本文展示，即使是仅有0.3亿参数的小型语言模型，也能展现出可与人类专家相媲美的显著数据精炼能力。我们提出了“编程每个示例”（ProX）这一创新框架，将数据精炼视为编程任务，使模型能够通过生成和执行细粒度操作（如字符串归一化）来大规模精炼每个单独示例的语料库。实验结果表明，在ProX精炼数据上预训练的模型在各种下游基准测试中，其表现均优于原始数据或其他筛选方法处理的数据，提升幅度超过2%。ProX的有效性涵盖了不同模型规模和预训练语料库，包括C4、RedPajama-V2和FineWeb。此外，ProX在特定领域的持续预训练中展现出巨大潜力：在没有特定领域设计的情况下，ProX精炼的OpenWebMath数据训练的模型优于基于人类手工规则的方法，Mistral-7B的平均准确率提高了7.6%，Llama-2-7B提高了14.6%，CodeLlama-7B提高了20.3%，这些提升均在100亿标记内，可与在2000亿标记上训练的Llemma-7B等模型相媲美。进一步分析表明，ProX显著节省了训练的浮点运算次数（FLOPs），为高效的大型语言模型（LLM）预训练提供了有前景的路径。我们正在开源ProX，包括超过1000亿标记的语料库、模型，并分享所有训练和实现细节，以促进可重复研究和未来创新。代码链接：https://github.com/GAIR-NLP/ProX</td>
<td>Fan Zhou</td>
<td><a href="http://arxiv.org/pdf/2409.17115v1">PDF</a></td>
<td>N/A</td>
<td>Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale</td>
<td>Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training.We are open-sourcing ProX with &gt;100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX</td>
</tr>
<tr>
<td>描述LLMs残差流中的稳定区域</td>
<td>我们在Transformer的残差流中识别出“稳定区域”，在这些区域内，模型的输出对小的激活变化不敏感，但在区域边界处表现出高敏感性。这些区域在训练过程中出现，并随着训练的进行或模型规模的增大而变得更加明确。这些区域似乎比先前研究的凸多面体大得多。我们的分析表明，这些稳定区域与语义区分相吻合，其中相似的提示在区域内聚集，而来自同一区域的激活会导致相似的下一个词预测。</td>
<td>Jett Janiak</td>
<td><a href="http://arxiv.org/pdf/2409.17113v1">PDF</a></td>
<td>N/A</td>
<td>Characterizing stable regions in the residual stream of LLMs</td>
<td>We identify "stable regions" in the residual stream of Transformers, where the model's output remains insensitive to small activation changes, but exhibits high sensitivity at region boundaries. These regions emerge during training and become more defined as training progresses or model size increases. The regions appear to be much larger than previously studied polytopes. Our analysis suggests that these stable regions align with semantic distinctions, where similar prompts cluster within regions, and activations from the same region lead to similar next token predictions.</td>
</tr>
<tr>
<td>非渐近收敛性分析的随机梯度哈密顿蒙特卡罗算法，应用于具有不连续随机梯度的ReLU神经网络训练</td>
<td>本文对随机梯度哈密顿蒙特卡罗（SGHMC）算法在Wasserstein-1和Wasserstein-2距离下向目标测度的收敛性进行了非渐近分析。与现有关于SGHMC的文献相比，我们允许其随机梯度具有不连续性。这使得我们能够为具有不连续随机梯度的非凸随机优化问题的预期超额风险提供显式上界，这些上界可以被控制得任意小，包括但不限于使用ReLU激活函数训练神经网络。为了说明我们主要结果的适用性，我们考虑了分位数估计和涉及金融和人工智能中相关的ReLU神经网络的几个优化问题的数值实验。</td>
<td>Luxu Liang</td>
<td><a href="http://arxiv.org/pdf/2409.17107v1">PDF</a></td>
<td>N/A</td>
<td>Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks</td>
<td>In this paper, we provide a non-asymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the existing literature on SGHMC, we allow its stochastic gradient to be discontinuous. This allows us to provide explicit upper bounds, which can be controlled to be arbitrarily small, for the expected excess risk of non-convex stochastic optimization problems with discontinuous stochastic gradients, including, among others, the training of neural networks with ReLU activation function. To illustrate the applicability of our main results, we consider numerical experiments on quantile estimation and on several optimization problems involving ReLU neural networks relevant in finance and artificial intelligence.</td>
</tr>
<tr>
<td>累加器感知的后训练量化</td>
<td>最近的几项研究探讨了低精度累加，报告了在各种平台上吞吐量、功耗和面积的改进。然而，伴随的提案仅考虑了量化感知训练（QAT）范式，其中模型在量化过程中进行微调或从头开始训练。随着模型规模的不断扩大，QAT技术变得越来越昂贵，这促使了近期对训练后量化（PTQ）研究的激增。据我们所知，我们的研究标志着首次在PTQ设置下对累加器感知量化进行正式研究。为了填补这一空白，我们引入了AXE，这是一个实用的累加器感知扩展框架，旨在为现有的逐层PTQ算法赋予溢出避免保证。我们从理论上推动AXE的发展，并通过在两种最先进的PTQ算法（GPFQ和OPTQ）之上实现它来展示其灵活性。我们进一步将AXE推广以首次支持多阶段累加，为全面数据路径优化和扩展到大型语言模型（LLMs）打开了大门。我们在图像分类和语言生成模型上评估了AXE，并观察到在累加器位宽与模型精度之间的权衡方面，相对于基线方法有显著改进。</td>
<td>Ian Colbert</td>
<td><a href="http://arxiv.org/pdf/2409.17092v1">PDF</a></td>
<td>N/A</td>
<td>Accumulator-Aware Post-Training Quantization</td>
<td>Several recent studies have investigated low-precision accumulation, reporting improvements in throughput, power, and area across various platforms. However, the accompanying proposals have only considered the quantization-aware training (QAT) paradigm, in which models are fine-tuned or trained from scratch with quantization in the loop. As models continue to grow in size, QAT techniques become increasingly more expensive, which has motivated the recent surge in post-training quantization (PTQ) research. To the best of our knowledge, ours marks the first formal study of accumulator-aware quantization in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework of accumulator-aware extensions designed to endow overflow avoidance guarantees to existing layer-wise PTQ algorithms. We theoretically motivate AXE and demonstrate its flexibility by implementing it on top of two state-of-the-art PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage accumulation for the first time, opening the door for full datapath optimization and scaling to large language models (LLMs). We evaluate AXE across image classification and language generation models, and observe significant improvements in the trade-off between accumulator bit width and model accuracy over baseline methods.</td>
</tr>
<tr>
<td>Ctrl-GenAug：面向医学序列分类的可控生成增强</td>
<td>在医疗领域，大规模数据集的稀缺和劳动密集型的标注过程限制了深度模型的性能。基于扩散的生成增强方法为这一问题提供了有前景的解决方案，已被证明在推进下游医疗识别任务中有效。然而，现有工作在处理具有挑战性的视频/3D序列生成时，缺乏足够的语义和顺序可控性，并且忽视了对合成样本噪声质量的控制，导致合成数据库的不可靠性，严重限制了下游任务的性能。在此工作中，我们提出了Ctrl-GenAug，一种新颖且通用的生成增强框架，能够实现高度语义和顺序定制的序列合成，并抑制错误合成的样本，以辅助医疗序列分类。具体而言，我们首先设计了一个多模态条件引导的序列生成器，用于可控地合成促进诊断的样本。接着，我们集成一个顺序增强模块，以增强生成样本的时间/立体一致性。然后，我们提出了一种噪声合成数据过滤器，以在语义和顺序层面上抑制不可靠的案例。在三个医疗数据集上，使用三种范式训练的11个网络进行了广泛的实验，全面分析了Ctrl-GenAug的有效性和通用性，特别是在代表性不足的高风险人群和域外条件下。</td>
<td>Xinrui Zhou</td>
<td><a href="http://arxiv.org/pdf/2409.17091v1">PDF</a></td>
<td>N/A</td>
<td>Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification</td>
<td>In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions.</td>
</tr>
<tr>
<td>通过快速近端梯度下降实现局部正则化的稀疏图</td>
<td>通过稀疏表示构建的稀疏图已被证明在聚类高维数据方面是有效的。尽管其具有令人信服的实证表现，但传统的稀疏图在单独对每个数据点进行稀疏表示时忽略了数据的几何信息。为了获得与数据局部几何结构相一致的稀疏图，我们提出了一种新的支持正则化稀疏图（Support Regularized Sparse Graph，简称SRSG）用于数据聚类。SRSG通过一个定义良好的支持正则化项，鼓励相邻数据点邻域的局部平滑性。我们提出了一种快速的近端梯度下降方法来解决SRSG的非凸优化问题，其收敛速度与Nesterov在光滑且具有Lipschitz连续梯度的凸目标函数上的最优一阶方法收敛率相匹配。在各种真实数据集上的广泛实验结果表明，SRSG优于其他竞争性聚类方法。</td>
<td>Dongfang Sun</td>
<td><a href="http://arxiv.org/pdf/2409.17090v1">PDF</a></td>
<td>N/A</td>
<td>Locally Regularized Sparse Graph by Fast Proximal Gradient Descent</td>
<td>Sparse graphs built by sparse representation has been demonstrated to be effective in clustering high-dimensional data. Albeit the compelling empirical performance, the vanilla sparse graph ignores the geometric information of the data by performing sparse representation for each datum separately. In order to obtain a sparse graph aligned with the local geometric structure of data, we propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data clustering. SRSG encourages local smoothness on the neighborhoods of nearby data points by a well-defined support regularization term. We propose a fast proximal gradient descent method to solve the non-convex optimization problem of SRSG with the convergence matching the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Extensive experimental results on various real data sets demonstrate the superiority of SRSG over other competing clustering methods.</td>
</tr>
<tr>
<td>SEN12-WATER：一个新的水文应用数据集及其基准测试</td>
<td>气候变化和日益加剧的干旱给全球水资源管理带来了重大挑战。这些问题导致严重的水资源短缺，威胁生态系统、农业和人类社区。为了推进应对这些挑战的斗争，我们提出了一种新的数据集，SEN12-WATER，并结合一个新颖的端到端深度学习（DL）框架，用于主动干旱相关分析的基准测试。该数据集被识别为时空数据立方体，整合了SAR极化、高程、坡度和多光谱光学波段。我们的DL框架能够分析和估算感兴趣水库中的水资源损失随时间的变化，通过检查水体积等物理量的时间变化，揭示了干旱分析中水动态的重要见解。我们的方法利用了所提出数据集的多时间和多模态特性，实现了强大的泛化能力，并推进了对干旱的理解，有助于气候变化适应性和可持续水资源管理。所提出的框架包括多个组件，如从SAR数据中去除斑点噪声、通过U-Net架构进行水体分割、时间序列分析以及时间分布卷积神经网络（TD-CNN）的预测能力。结果通过地面实况数据进行验证，这些数据通过专用传感器在地面上获取，并使用（定制的）指标进行评估，如精确度、召回率、交并比、均方误差、结构相似性指数和峰值信噪比。</td>
<td>Luigi Russo</td>
<td><a href="http://arxiv.org/pdf/2409.17087v1">PDF</a></td>
<td>N/A</td>
<td>SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking</td>
<td>Climate change and increasing droughts pose significant challenges to water resource management around the world. These problems lead to severe water shortages that threaten ecosystems, agriculture, and human communities. To advance the fight against these challenges, we present a new dataset, SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL) framework for proactive drought-related analysis. The dataset, identified as a spatiotemporal datacube, integrates SAR polarization, elevation, slope, and multispectral optical bands. Our DL framework enables the analysis and estimation of water losses over time in reservoirs of interest, revealing significant insights into water dynamics for drought analysis by examining temporal changes in physical quantities such as water volume. Our methodology takes advantage of the multitemporal and multimodal characteristics of the proposed dataset, enabling robust generalization and advancing understanding of drought, contributing to climate change resilience and sustainable water resource management. The proposed framework involves, among the several components, speckle noise removal from SAR data, a water body segmentation through a U-Net architecture, the time series analysis, and the predictive capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results are validated through ground truth data acquired on-ground via dedicated sensors and (tailored) metrics, such as Precision, Recall, Intersection over Union, Mean Squared Error, Structural Similarity Index Measure and Peak Signal-to-Noise Ratio.</td>
</tr>
<tr>
<td>视觉语言模型能否从模糊空间推理的视觉演示中学习？</td>
<td>大型视觉语言模型（VLMs）已成为许多计算机视觉任务的最新技术，其中上下文学习（ICL）是一种流行的适应新任务的策略。但是，VLMs能否仅从视觉演示中学习新概念，还是它们仅限于适应ICL示例的输出格式？我们提出了一个新的基准，我们称之为空间视觉歧义任务（SVAT），它挑战最先进的VLMs在上下文中学习新的视觉空间任务。我们发现，VLMs在零样本情况下无法完成这些任务，有时在微调后仍然失败。然而，通过课程学习向训练中添加更简单的数据，可以提高ICL的表现。</td>
<td>Bowen Zhao</td>
<td><a href="http://arxiv.org/pdf/2409.17080v1">PDF</a></td>
<td>N/A</td>
<td>Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?</td>
<td>Large vision-language models (VLMs) have become state-of-the-art for many computer vision tasks, with in-context learning (ICL) as a popular adaptation strategy for new ones. But can VLMs learn novel concepts purely from visual demonstrations, or are they limited to adapting to the output format of ICL examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks (SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks in-context. We find that VLMs fail to do this zero-shot, and sometimes continue to fail after finetuning. However, adding simpler data to the training by curriculum learning leads to improved ICL performance.</td>
</tr>
<tr>
<td>利用Transformer实现高效特征交互：提升游戏用户消费倾向预测</td>
<td>Dream11是一个梦幻体育平台，允许用户为现实生活中的体育赛事创建自己的虚拟团队。我们为超过2亿的用户群体举办多种体育赛事。在这种真实货币游戏（RMG）环境中，用户支付参与费用来参加我们为用户提供的各种竞赛产品。在我们的当前工作中，我们讨论了预测用户在游戏回合中的消费倾向的问题，以便可以将其用于各种下游应用，例如根据用户的消费倾向微调激励措施来提升用户消费，或根据用户的消费倾向个性化产品列表。我们的目标是基于过去的交易数据为每个用户建模消费倾向。在本文中，我们基准测试了在结构化数据上表现良好的基于树的模型和深度学习模型，并提出了一种新的架构变化，专门设计用于捕捉输入特征之间的丰富交互。我们展示了我们提出的架构在预测用户在游戏回合中的消费倾向任务上优于现有模型。我们的新Transformer模型超越了最先进的FT-Transformer，将MAE提高了2.5%，MSE提高了21.8%。</td>
<td>Ved Prakash</td>
<td><a href="http://arxiv.org/pdf/2409.17077v1">PDF</a></td>
<td>N/A</td>
<td>Efficient Feature Interactions with Transformers: Improving User Spending Propensity Predictions in Gaming</td>
<td>Dream11 is a fantasy sports platform that allows users to create their own virtual teams for real-life sports events. We host multiple sports and matches for our 200M+ user base. In this RMG (real money gaming) setting, users pay an entry amount to participate in various contest products that we provide to users. In our current work, we discuss the problem of predicting the user's propensity to spend in a gaming round, so it can be utilized for various downstream applications. e.g. Upselling users by incentivizing them marginally as per their spending propensity, or personalizing the product listing based on the user's propensity to spend.   We aim to model the spending propensity of each user based on past transaction data. In this paper, we benchmark tree-based and deep-learning models that show good results on structured data, and we propose a new architecture change that is specifically designed to capture the rich interactions among the input features. We show that our proposed architecture outperforms the existing models on the task of predicting the user's propensity to spend in a gaming round. Our new transformer model surpasses the state-of-the-art FT-Transformer, improving MAE by 2.5\% and MSE by 21.8\%.</td>
</tr>
<tr>
<td>通过粗粒度答案分解增强长文档理解中的事后归因</td>
<td>准确地将回答文本归因于其来源文档对于构建可靠的问答系统至关重要。然而，对于长文档的归因问题仍未得到充分探索。事后归因系统旨在将回答文本映射回源文档，但这种映射的粒度问题尚未得到解决。此外，一个关键问题浮现：究竟应该归因什么，重点在于识别回答中需要依据的信息单元？本文中，我们提出并研究了一种新颖的生成回答事实分解方法，用于归因，采用基于模板的上下文学习。为此，我们利用问题，并在少样本上下文学习中结合负采样进行分解。这种方法增强了抽象和抽取回答的语义理解。我们通过全面考察各种归因方法，从基于检索的技术到基于大型语言模型的归因器，来检验回答分解的影响。</td>
<td>Pritika Ramu</td>
<td><a href="http://arxiv.org/pdf/2409.17073v1">PDF</a></td>
<td>N/A</td>
<td>Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition</td>
<td>Accurately attributing answer text to its source document is crucial for developing a reliable question-answering system. However, attribution for long documents remains largely unexplored. Post-hoc attribution systems are designed to map answer text back to the source document, yet the granularity of this mapping has not been addressed. Furthermore, a critical question arises: What precisely should be attributed, with an emphasis on identifying the information units within an answer that necessitate grounding? In this paper, we propose and investigate a novel approach to the factual decomposition of generated answers for attribution, employing template-based in-context learning. To accomplish this, we utilize the question and integrate negative sampling during few-shot in-context learning for decomposition. This approach enhances the semantic understanding of both abstractive and extractive answers. We examine the impact of answer decomposition by providing a thorough examination of various attribution approaches, ranging from retrieval-based techniques to LLM-based attributors.</td>
</tr>
<tr>
<td>感知度量对音乐流派分类中音乐表示学习的影响</td>
<td>自然信号的主观质量可以通过客观的感知度量来近似。设计用来近似人类观察者的感知行为，感知度量通常反映了自然信号和神经通路中发现的结构。使用感知度量作为损失函数的模型可以捕捉到这些度量中包含的结构所具有的感知上有意义的特征。我们展示了，使用从经过感知损失训练的自编码器中提取的特征，可以在音乐理解任务（如风格分类）中提高性能，相比于直接将这些度量作为距离来训练分类器。这一结果表明，在表示学习中使用感知度量作为损失函数时，对新信号的泛化能力有所提高。</td>
<td>Tashi Namgyal</td>
<td><a href="http://arxiv.org/pdf/2409.17069v1">PDF</a></td>
<td>N/A</td>
<td>The Effect of Perceptual Metrics on Music Representation Learning for Genre Classification</td>
<td>The subjective quality of natural signals can be approximated with objective perceptual metrics. Designed to approximate the perceptual behaviour of human observers, perceptual metrics often reflect structures found in natural signals and neurological pathways. Models trained with perceptual metrics as loss functions can capture perceptually meaningful features from the structures held within these metrics. We demonstrate that using features extracted from autoencoders trained with perceptual losses can improve performance on music understanding tasks, i.e. genre classification, over using these metrics directly as distances when learning a classifier. This result suggests improved generalisation to novel signals when using perceptual metrics as loss functions for representation learning.</td>
</tr>
<tr>
<td>在计算病理学中基准测试领域泛化算法</td>
<td>深度学习模型在计算病理学（CPath）任务中展现了巨大的潜力，但当应用于未见过的数据时，由于领域偏移，其性能常常受到影响。解决这一问题需要领域泛化（DG）算法。然而，在CPath背景下对DG算法的系统评估尚缺乏。本研究旨在通过7,560次交叉验证运行，对30种DG算法在3个难度不同的CPath任务上的有效性进行基准测试。我们使用一个统一且稳健的平台来评估这些算法，结合了特定模态技术及如预训练基础模型等最新进展。广泛的交叉验证实验为我们提供了各种DG策略相对性能的深入见解。我们观察到，自监督学习和染色增强方法始终优于其他方法，突显了预训练模型和数据增强的潜力。此外，我们引入了一个新的泛癌肿瘤检测数据集（HISTOPANTUM）作为未来研究的基准。本研究为研究人员在选择适用于CPath任务的适当DG方法时提供了宝贵的指导。</td>
<td>Neda Zamanitajeddin</td>
<td><a href="http://arxiv.org/pdf/2409.17063v1">PDF</a></td>
<td>N/A</td>
<td>Benchmarking Domain Generalization Algorithms in Computational Pathology</td>
<td>Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks.</td>
</tr>
<tr>
<td>DRIM：从不完整的多模态医疗数据中学习解耦表示</td>
<td>现实生活中的医疗数据通常是多模态且不完整的，这促使了对能够高效整合这些数据的高级深度学习模型的需求不断增长。利用包括组织病理学切片、核磁共振成像（MRI）和基因数据在内的多种模态，为改善预后预测和揭示新的治疗途径提供了前所未有的机会。对比学习广泛用于从多模态任务中的配对数据中提取表示，它假设不同的视图包含相同的与任务相关的信息，并仅利用共享信息。然而，在处理医疗数据时，这种假设变得具有限制性，因为每种模态还包含与下游任务相关的特定知识。我们引入了DRIM，这是一种新的多模态方法，能够在数据稀疏的情况下捕捉这些共享和独特的表示。更具体地说，给定一组模态，我们的目标是编码每个模态的表示，该表示可以分为两个部分：一部分封装了跨模态的与患者相关的共同信息，另一部分封装了模态特定的细节。通过增加不同患者模态之间的共享信息，同时最小化每个模态内共享和独特组件之间的重叠，我们实现了这一目标。我们的方法在胶质瘤患者的生存预测任务中优于最先进的算法，同时对缺失模态具有鲁棒性。为了促进可重复性，代码已在https://github.com/Lucas-rbnt/DRIM公开发布。</td>
<td>Lucas Robinet</td>
<td><a href="http://arxiv.org/pdf/2409.17055v1">PDF</a></td>
<td>N/A</td>
<td>DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data</td>
<td>Real-life medical data is often multimodal and incomplete, fueling the growing need for advanced deep learning models capable of integrating them efficiently. The use of diverse modalities, including histopathology slides, MRI, and genetic data, offers unprecedented opportunities to improve prognosis prediction and to unveil new treatment pathways. Contrastive learning, widely used for deriving representations from paired data in multimodal tasks, assumes that different views contain the same task-relevant information and leverages only shared information. This assumption becomes restrictive when handling medical data since each modality also harbors specific knowledge relevant to downstream tasks. We introduce DRIM, a new multimodal method for capturing these shared and unique representations, despite data sparsity. More specifically, given a set of modalities, we aim to encode a representation for each one that can be divided into two components: one encapsulating patient-related information common across modalities and the other, encapsulating modality-specific details. This is achieved by increasing the shared information among different patient modalities while minimizing the overlap between shared and unique components within each modality. Our method outperforms state-of-the-art algorithms on glioma patients survival prediction tasks, while being robust to missing modalities. To promote reproducibility, the code is made publicly available at https://github.com/Lucas-rbnt/DRIM</td>
</tr>
<tr>
<td>利用大型语言模型（LLM）对印度尼西亚ePuskesmas中医患互动进行实时转录和总结</td>
<td>导致卫生中心(Puskesmas)效率低下的关键问题之一是医生与患者互动耗时较长。医生需要进行全面的咨询，包括诊断患者的病情、提供治疗建议以及将详细记录转录到病历中。在语言背景多样的地区，医生通常需要提出澄清问题，进一步延长了这一过程。虽然诊断至关重要，但转录和总结通常可以利用人工智能自动化，以提高时间效率，并帮助医生提高护理质量，实现早期诊断和干预。本文提出了一种解决方案，使用本地化的大型语言模型(LLM)来转录、翻译和总结医生与患者之间的对话。我们利用Whisper模型进行转录，并使用GPT-3将其总结为ePuskemas医疗记录格式。该系统作为现有网页浏览器扩展的附加组件实现，允许医生在交谈时填写患者表格。通过利用这一解决方案进行实时转录、翻译和总结，医生可以缩短患者护理的周转时间，同时提高记录质量，使其在未来就诊时更加详细和有洞察力。这一创新解决了印尼医疗机构拥挤和行政负担重等挑战。我们相信，这一解决方案将帮助医生节省时间，提供更好的护理，并生成更准确的医疗记录，标志着向现代化医疗迈出了重要一步，确保患者即使在资源有限的地区也能及时获得高质量的护理。</td>
<td>Azmul Asmar Irfan</td>
<td><a href="http://arxiv.org/pdf/2409.17054v1">PDF</a></td>
<td>N/A</td>
<td>Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia</td>
<td>One of the key issues contributing to inefficiency in Puskesmas is the time-consuming nature of doctor-patient interactions. Doctors need to conduct thorough consultations, which include diagnosing the patient's condition, providing treatment advice, and transcribing detailed notes into medical records. In regions with diverse linguistic backgrounds, doctors often have to ask clarifying questions, further prolonging the process. While diagnosing is essential, transcription and summarization can often be automated using AI to improve time efficiency and help doctors enhance care quality and enable early diagnosis and intervention. This paper proposes a solution using a localized large language model (LLM) to transcribe, translate, and summarize doctor-patient conversations. We utilize the Whisper model for transcription and GPT-3 to summarize them into the ePuskemas medical records format. This system is implemented as an add-on to an existing web browser extension, allowing doctors to fill out patient forms while talking. By leveraging this solution for real-time transcription, translation, and summarization, doctors can improve the turnaround time for patient care while enhancing the quality of records, which become more detailed and insightful for future visits. This innovation addresses challenges like overcrowded facilities and the administrative burden on healthcare providers in Indonesia. We believe this solution will help doctors save time, provide better care, and produce more accurate medical records, representing a significant step toward modernizing healthcare and ensuring patients receive timely, high-quality care, even in resource-constrained settings.</td>
</tr>
<tr>
<td>使用图Koopman自编码器对抗多无人机监视的预测隐蔽通信</td>
<td>低检测概率（LPD）通信旨在掩盖无线电频率（RF）信号的存在，以逃避监控。在使用无人机（UAV）进行移动监控的背景下，由于无人机的快速连续移动具有未知的非线性动力学特性，实现LPD通信面临重大挑战。因此，准确预测无人机的未来位置对于实现实时LPD通信至关重要。本文提出了一种新的框架，称为预测隐蔽通信，旨在在多无人机监控下的地面自组织网络中最小化检测概率。我们的数据驱动方法将图神经网络（GNN）与Koopman理论相结合，以建模多无人机网络中的复杂交互，并通过线性化动力学实现长期预测，即使在有限的历史数据下也能实现。广泛的仿真结果证实，与现有的最先进基线方法相比，使用我们的方法预测的轨迹导致检测概率至少降低了63%-75%，显示出在实际场景中实现低延迟隐蔽操作的潜力。</td>
<td>Sivaram Krishnan</td>
<td><a href="http://arxiv.org/pdf/2409.17048v1">PDF</a></td>
<td>N/A</td>
<td>Predictive Covert Communication Against Multi-UAV Surveillance Using Graph Koopman Autoencoder</td>
<td>Low Probability of Detection (LPD) communication aims to obscure the presence of radio frequency (RF) signals to evade surveillance. In the context of mobile surveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD communication presents significant challenges due to the UAVs' rapid and continuous movements, which are characterized by unknown nonlinear dynamics. Therefore, accurately predicting future locations of UAVs is essential for enabling real-time LPD communication. In this paper, we introduce a novel framework termed predictive covert communication, aimed at minimizing detectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our data-driven method synergistically integrates graph neural networks (GNN) with Koopman theory to model the complex interactions within a multi-UAV network and facilitating long-term predictions by linearizing the dynamics, even with limited historical data. Extensive simulation results substantiate that the predicted trajectories using our method result in at least 63%-75% lower probability of detection when compared to well-known state-of-the-art baseline approaches, showing promise in enabling low-latency covert operations in practical scenarios.</td>
</tr>
<tr>
<td>检测问题中的时间模糊性</td>
<td>检测和回答模糊问题是开放领域问答中的一个挑战性任务。模糊问题根据其解释的不同可能有不同的答案，并且可以采取多种形式。时间模糊问题是这类问题中最常见的一种。在本文中，我们介绍了TEMPAMBIQA，这是一个手动标注的时间模糊问答数据集，包含从现有数据集中提取的8,162个开放领域问题。我们的标注重点在于捕捉时间模糊性，以研究检测时间模糊问题的任务。我们提出了一种新颖的方法，通过使用基于问题消歧版本的多样化搜索策略。我们还引入了非搜索的竞争性基线，并使用零样本和少样本方法测试了检测时间模糊性的能力。</td>
<td>Bhawna Piryani</td>
<td><a href="http://arxiv.org/pdf/2409.17046v1">PDF</a></td>
<td>N/A</td>
<td>Detecting Temporal Ambiguity in Questions</td>
<td>Detecting and answering ambiguous questions has been a challenging task in open-domain question answering. Ambiguous questions have different answers depending on their interpretation and can take diverse forms. Temporally ambiguous questions are one of the most common types of such questions. In this paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA dataset consisting of 8,162 open-domain questions derived from existing datasets. Our annotations focus on capturing temporal ambiguity to study the task of detecting temporally ambiguous questions. We propose a novel approach by using diverse search strategies based on disambiguated versions of the questions. We also introduce and test non-search, competitive baselines for detecting temporal ambiguity using zero-shot and few-shot approaches.</td>
</tr>
<tr>
<td>如何将语音基础模型与大型语言模型连接起来？哪些因素重要，哪些不重要？</td>
<td>大型语言模型（LLM）的显著表现推动了研究工作，以利用它们处理广泛的任务和输入模式。在语音转文本（S2T）任务中，新兴的解决方案包括通过适配器模块将语音基础模型（SFM）的编码器输出投影到LLM的嵌入空间中。然而，目前尚无研究探讨下游任务性能在多大程度上依赖于每个组件（SFM、适配器、LLM），也没有研究最佳适配器设计是否取决于所选的SFM和LLM。为了填补这一空白，我们评估了5种适配器模块、2种LLM（Mistral和Llama）以及2种SFM（Whisper和SeamlessM4T）在两种广泛应用的S2T任务（即自动语音识别和语音翻译）中的组合效果。我们的研究结果表明，SFM在下游性能中起着关键作用，而适配器的选择影响适中，并依赖于SFM和LLM。</td>
<td>Francesco Verdini</td>
<td><a href="http://arxiv.org/pdf/2409.17044v1">PDF</a></td>
<td>N/A</td>
<td>How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not</td>
<td>The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.</td>
</tr>
<tr>
<td>大语言模型中的反事实令牌生成</td>
<td>当然，我很乐意为您生成一个故事：莱拉船长站在她值得信赖的船只“旋风之怒”的舵轮前，凝视着无边无际的大海。[...] 莱拉意识到残酷的真相，眼中涌出泪水——她为了短暂的财富牺牲了一切，失去了船员的爱、家人的爱，甚至失去了自我。尽管这个由大型语言模型生成的故事引人入胜，但人们可能会好奇——如果模型选择“梅芙船长”作为主角，故事会如何展开？我们无从得知。最先进的大型语言模型是无状态的——它们不保留任何内部记忆或状态。给定一个提示，它们使用自回归过程生成一系列标记作为输出。因此，它们无法对过去生成的标记进行反事实推理。在这项工作中，我们的目标是赋予它们这种功能。为此，我们开发了一种基于Gumbel-Max结构因果模型的标记生成因果模型。我们的模型允许任何大型语言模型以几乎不增加成本的方式进行反事实标记生成，实现起来非常简单，并且不需要任何微调或提示工程。我们在Llama 3 8B-instruct上实现了我们的模型，并对反事实生成的文本进行了定性和定量分析。最后，我们通过一个反事实标记生成的示范应用来检测偏见，揭示了大型语言模型构建的世界模型中的有趣见解。</td>
<td>Ivi Chatzi</td>
<td><a href="http://arxiv.org/pdf/2409.17027v1">PDF</a></td>
<td>N/A</td>
<td>Counterfactual Token Generation in Large Language Models</td>
<td>"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...] Lyra's eyes welled up with tears as she realized the bitter truth - she had sacrificed everything for fleeting riches, and lost the love of her crew, her family, and herself." Although this story, generated by a large language model, is captivating, one may wonder -- how would the story have unfolded if the model had chosen "Captain Maeve" as the protagonist instead? We cannot know. State-of-the-art large language models are stateless -- they maintain no internal memory or state. Given a prompt, they generate a sequence of tokens as an output using an autoregressive process. As a consequence, they cannot reason about counterfactual alternatives to tokens they have generated in the past. In this work, our goal is to enhance them with this functionality. To this end, we develop a causal model of token generation that builds upon the Gumbel-Max structural causal model. Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation, it is embarrassingly simple to implement, and it does not require any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-instruct and conduct both qualitative and quantitative analyses of counterfactually generated text. We conclude with a demonstrative application of counterfactual token generation for bias detection, unveiling interesting insights about the model of the world constructed by large language models.</td>
</tr>
<tr>
<td>CombU：一种用于神经网络拟合数学表达式的组合单元激活方法</td>
<td>激活函数是神经网络的基础，因为它们引入了数据关系中的非线性，从而使深度网络能够近似复杂的数据关系。现有的提高神经网络性能的努力主要集中在开发新的数学函数上。然而，我们发现，在神经网络中巧妙地组合现有的激活函数也能实现这一目标。在本文中，我们介绍了组合单元激活（CombU），它在不同层级的不同维度上使用不同的激活函数。这种方法在理论上可以证明能够准确拟合大多数数学表达式。在四个数学表达式数据集上进行的实验，与六种最先进的（SOTA）激活函数算法相比，结果表明CombU在16项指标中的10项上优于所有SOTA算法，并在其余6项指标中排名前三。</td>
<td>Jiayu Li</td>
<td><a href="http://arxiv.org/pdf/2409.17021v1">PDF</a></td>
<td>N/A</td>
<td>CombU: A Combined Unit Activation for Fitting Mathematical Expressions with Neural Networks</td>
<td>The activation functions are fundamental to neural networks as they introduce non-linearity into data relationships, thereby enabling deep networks to approximate complex data relations. Existing efforts to enhance neural network performance have predominantly focused on developing new mathematical functions. However, we find that a well-designed combination of existing activation functions within a neural network can also achieve this objective. In this paper, we introduce the Combined Units activation (CombU), which employs different activation functions at various dimensions across different layers. This approach can be theoretically proven to fit most mathematical expressions accurately. The experiments conducted on four mathematical expression datasets, compared against six State-Of-The-Art (SOTA) activation function algorithms, demonstrate that CombU outperforms all SOTA algorithms in 10 out of 16 metrics and ranks in the top three for the remaining six metrics.</td>
</tr>
<tr>
<td>CNN深度混合</td>
<td>我们引入了卷积神经网络（CNNs）的深度混合（Mixture-of-Depths, MoD），这是一种新颖的方法，通过根据通道与当前预测的相关性来选择性处理通道，从而提高CNNs的计算效率。该方法通过在卷积块（Conv-Blocks）内动态选择关键通道进行集中处理，同时跳过不太相关的通道，优化计算资源。与需要动态计算图的条件计算方法不同，CNN MoD使用具有固定张量大小的静态计算图，从而提高硬件效率。它加速了训练和推理过程，无需定制的CUDA内核、独特的损失函数或微调。CNN MoD要么在减少推理时间、GMACs和参数的情况下匹配传统CNN的性能，要么在保持相似推理时间、GMACs和参数的情况下超越其性能。例如，在ImageNet上，ResNet86-MoD在CPU上加速6%、GPU上加速5%的情况下，性能超过了标准的ResNet50 0.45%。此外，ResNet75-MoD在CPU上加速25%、GPU上加速15%的情况下，达到了与ResNet50相同的性能。</td>
<td>Rinor Cakaj</td>
<td><a href="http://arxiv.org/pdf/2409.17016v1">PDF</a></td>
<td>N/A</td>
<td>CNN Mixture-of-Depths</td>
<td>We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks (CNNs), a novel approach that enhances the computational efficiency of CNNs by selectively processing channels based on their relevance to the current prediction. This method optimizes computational resources by dynamically selecting key channels in feature maps for focused processing within the convolutional blocks (Conv-Blocks), while skipping less relevant channels. Unlike conditional computation methods that require dynamic computation graphs, CNN MoD uses a static computation graph with fixed tensor sizes which improve hardware efficiency. It speeds up the training and inference processes without the need for customized CUDA kernels, unique loss functions, or finetuning. CNN MoD either matches the performance of traditional CNNs with reduced inference times, GMACs, and parameters, or exceeds their performance while maintaining similar inference times, GMACs, and parameters. For example, on ImageNet, ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a 6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same performance as ResNet50 with a 25% speedup on CPU and 15% on GPU.</td>
</tr>
<tr>
<td>LLM-CARD：迈向大型语言模型描述与全景图</td>
<td>随着自然语言处理（NLP）领域的迅速发展，大量的大型语言模型（LLMs）不断涌现，用于各种NLP任务。随着越来越多的论文被发表，研究人员和开发者面临着信息过载的挑战。因此，开发一个能够自动从学术论文中提取和组织关于LLMs的关键信息的系统（即\textbf{LLM模型卡片}）显得尤为重要。本工作旨在通过使用命名实体识别（\textbf{NER}）和关系提取（\textbf{RE}）方法，开发这样一个开创性的系统，自动从论文中提取大型语言模型的关键信息，帮助研究人员高效获取关于LLMs的信息。这些特征包括模型的\textit{许可证}、模型的\textit{名称}以及模型的\textit{应用}。通过这些特征，我们可以为每篇论文生成一个模型卡片。在\textbf{数据贡献}方面，通过对106篇学术论文进行处理，定义了三个字典——LLMs名称、许可证和应用。通过字典查找提取了11,051个句子，并通过人工审查最终选定的129个句子（这些句子中模型名称与许可证之间存在关联）和106个句子（这些句子中模型名称与应用之间存在关联）构建了数据集。</td>
<td>Shengwei Tian</td>
<td><a href="http://arxiv.org/pdf/2409.17011v1">PDF</a></td>
<td>N/A</td>
<td>LLM-CARD: Towards a Description and Landscape of Large Language Models</td>
<td>With the rapid growth of the Natural Language Processing (NLP) field, a vast variety of Large Language Models (LLMs) continue to emerge for diverse NLP tasks. As an increasing number of papers are presented, researchers and developers face the challenge of information overload. Thus, it is particularly important to develop a system that can automatically extract and organise key information about LLMs from academic papers (\textbf{LLM model card}). This work is to develop such a pioneer system by using Named Entity Recognition (\textbf{NER}) and Relation Extraction (\textbf{RE}) methods that automatically extract key information about large language models from the papers, helping researchers to efficiently access information about LLMs. These features include model \textit{licence}, model \textit{name}, and model \textit{application}. With these features, we can form a model card for each paper. \textbf{Data-contribution} wise, 106 academic papers were processed by defining three dictionaries - LLMs name, licence, and application. 11,051 sentences were extracted through dictionary lookup, and the dataset was constructed through manual review of the final selection of 129 sentences that have a link between the name and the licence, and 106 sentences that have a link between the model name and the application.</td>
</tr>
<tr>
<td>模型能够并且应该接纳人类生成数学的交流性质</td>
<td>数学是由人类为人类构建的：正如自然语料库不仅反映命题，还反映语言使用者的交流目标一样，模型所训练的数学数据不仅反映理想化的数学实体，还反映丰富的交流意图。尽管以纯粹符号的方式处理数学有其重要优势，但我们在此假设，将数学视为情境化的语言交流具有好处，并且语言模型非常适合实现这一目标，尽管这一点尚未得到充分认识。我们通过两个案例研究来说明这些观点。首先，我们进行了一项实验，发现语言模型以类人的方式解释等号——为同一基础方程以不同方式排列生成系统上不同的文字题。其次，我们发现语言模型倾向于证明以自然的方式排序，即使其他顺序在逻辑上是等价的。我们主张开发从人类生成的数学中学习并体现其中潜在交流意图的AI系统。</td>
<td>Sasha Boguraev</td>
<td><a href="http://arxiv.org/pdf/2409.17005v1">PDF</a></td>
<td>N/A</td>
<td>Models Can and Should Embrace the Communicative Nature of Human-Generated Math</td>
<td>Math is constructed by people for people: just as natural language corpora reflect not just propositions but the communicative goals of language users, the math data that models are trained on reflects not just idealized mathematical entities but rich communicative intentions. While there are important advantages to treating math in a purely symbolic manner, we here hypothesize that there are benefits to treating math as situated linguistic communication and that language models are well suited for this goal, in ways that are not fully appreciated. We illustrate these points with two case studies. First, we ran an experiment in which we found that language models interpret the equals sign in a humanlike way -- generating systematically different word problems for the same underlying equation arranged in different ways. Second, we found that language models prefer proofs to be ordered in naturalistic ways, even though other orders would be logically equivalent. We advocate for AI systems that learn from and represent the communicative intentions latent in human-generated math.</td>
</tr>
<tr>
<td>PitRSDNet：预测内镜下垂体手术中手术剩余时间</td>
<td>准确的术中剩余手术时间（RSD）预测使麻醉师能够更精确地决定何时给予麻醉剂和药物，并通知医院工作人员传送下一位患者。因此，RSD在通过高效排程提升患者护理和降低手术室成本方面发挥着重要作用。在内窥镜垂体手术中，由于存在多种可选步骤导致的工作流程序列变化，使得手术时间的预测尤为困难。本文提出了PitRSDNet，用于预测垂体手术中的RSD，这是一种时空神经网络模型，从历史数据中学习，专注于工作流程序列。PitRSDNet通过两种方式将工作流程知识融入RSD预测：1）多任务学习，同时预测步骤和RSD；2）在时间学习和推理中将先前步骤作为上下文纳入。PitRSDNet在一个包含88个视频的新内窥镜垂体手术数据集上进行训练和评估，展示了相较于以往统计和机器学习方法的竞争性性能提升。研究结果还突显了PitRSDNet如何利用先前步骤的知识提高异常值案例的RSD精度。</td>
<td>Anjana Wijekoon</td>
<td><a href="http://arxiv.org/pdf/2409.16998v1">PDF</a></td>
<td>N/A</td>
<td>PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in Endoscopic Pituitary Surgery</td>
<td>Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow for anaesthetists to more accurately decide when to administer anaesthetic agents and drugs, as well as to notify hospital staff to send in the next patient. Therefore RSD plays an important role in improving patient care and minimising surgical theatre costs via efficient scheduling. In endoscopic pituitary surgery, it is uniquely challenging due to variable workflow sequences with a selection of optional steps contributing to high variability in surgery duration. This paper presents PitRSDNet for predicting RSD during pituitary surgery, a spatio-temporal neural network model that learns from historical data focusing on workflow sequences. PitRSDNet integrates workflow knowledge into RSD prediction in two forms: 1) multi-task learning for concurrently predicting step and RSD; and 2) incorporating prior steps as context in temporal learning and inference. PitRSDNet is trained and evaluated on a new endoscopic pituitary surgery dataset with 88 videos to show competitive performance improvements over previous statistical and machine learning methods. The findings also highlight how PitRSDNet improve RSD precision on outlier cases utilising the knowledge of prior steps.</td>
</tr>
<tr>
<td>INT-FlashAttention：为INT8量化启用Flash Attention</td>
<td>作为大型语言模型（LLMs）的基础，自注意力模块面临着与序列长度相关的二次时间与内存复杂度的挑战。FlashAttention通过利用GPU内存层次结构加速了注意力计算并减少了其内存使用。一个有前景的研究方向是将FlashAttention与量化方法结合。本文介绍了INT-FlashAttention，这是首个与FlashAttention前向工作流程兼容的INT8量化架构，显著提升了FlashAttention在Ampere GPU上的推理速度。我们通过完全INT8激活和通用矩阵乘法（GEMM）内核实现了INT-FlashAttention原型，使其成为首个完全INT8输入的注意力操作符。作为一个通用的token级后训练量化框架，INT-FlashAttention还兼容其他数据格式，如INT4等。实验结果表明，与使用FP16和FP8数据格式的标准FlashAttention相比，INT-FlashAttention的推理速度提高了72%，量化误差减少了82%。</td>
<td>Shimao Chen</td>
<td><a href="http://arxiv.org/pdf/2409.16997v1">PDF</a></td>
<td>N/A</td>
<td>INT-FlashAttention: Enabling Flash Attention for INT8 Quantization</td>
<td>As the foundation of large language models (LLMs), self-attention module faces the challenge of quadratic time and memory complexity with respect to sequence length. FlashAttention accelerates attention computation and reduces its memory usage by leveraging the GPU memory hierarchy. A promising research direction is to integrate FlashAttention with quantization methods. This paper introduces INT-FlashAttention, the first INT8 quantization architecture compatible with the forward workflow of FlashAttention, which significantly improves the inference speed of FlashAttention on Ampere GPUs. We implement our INT-FlashAttention prototype with fully INT8 activations and general matrix-multiplication (GEMM) kernels, making it the first attention operator with fully INT8 input. As a general token-level post-training quantization framework, INT-FlashAttention is also compatible with other data formats like INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster inference speed and 82% smaller quantization error compared to standard FlashAttention with FP16 and FP8 data format.</td>
</tr>
<tr>
<td>慢特征分析（Slow Feature Analysis）与后继表示（Successor Representation）之间的关系是什么？</td>
<td>对慢特征分析（SFA）和后继表示（SR）进行了分析比较。尽管SFA和SR源自机器学习的不同领域，但它们在数学性质和敏感信息类型方面具有重要的共性。本研究沿着这两个轴线探讨了它们的联系。特别是，分析了SFA算法的多个变体，并将其应用于MDP（马尔可夫决策过程）环境中，从而引出了一系列涉及SR及其他相关量的特征值问题。随后，在网格世界的玩具环境中展示了这些特征值问题，并证明通常与SR相关的位置和网格状场域同样可以通过SFA生成。</td>
<td>Eddie Seabrook</td>
<td><a href="http://arxiv.org/pdf/2409.16991v1">PDF</a></td>
<td>N/A</td>
<td>What is the relationship between Slow Feature Analysis and the Successor Representation?</td>
<td>(This is a work in progress. Feedback is welcome) An analytical comparison is made between slow feature analysis (SFA) and the successor representation (SR). While SFA and the SR stem from distinct areas of machine learning, they share important properties, both in terms of their mathematics and the types of information they are sensitive to. This work studies their connection along these two axes. In particular, multiple variants of the SFA algorithm are explored analytically and then applied to the setting of an MDP, leading to a family of eigenvalue problems involving the SR and other related quantities. These resulting eigenvalue problems are then illustrated in the toy setting of a gridworld, where it is demonstrated that the place- and grid-like fields often associated to the SR can equally be generated using SFA.</td>
</tr>
<tr>
<td>AXCEL：使用LLMs进行自动化可解释一致性评估</td>
<td>大型语言模型（LLMs）在工业界和学术界被广泛应用于各种任务，然而评估生成文本响应的一致性仍然是一个挑战。传统的评估指标如ROUGE和BLEU与人类判断的相关性较弱。使用自然语言推理（NLI）的更复杂指标虽然显示出改进的相关性，但其实现复杂，由于跨领域泛化能力差而需要领域特定的训练，并且缺乏可解释性。最近，基于提示的评估指标使用LLMs作为评估者已经出现；尽管它们更容易实现，但仍然缺乏可解释性，并且依赖于任务特定的提示，这限制了它们的通用性。这项工作引入了使用LLMs的自动化可解释一致性评估（AXCEL），这是一种基于提示的一致性评估指标，通过提供详细的推理和指出不一致的文本片段来解释一致性评分。AXCEL还是一个可泛化的指标，可以应用于多个任务而无需更改提示。在检测不一致性方面，AXCEL在总结任务中比非提示和基于提示的最先进（SOTA）指标高出8.7%，在自由文本生成任务中高出6.2%，在数据到文本转换任务中高出29.4%。我们还评估了底层LLMs对基于提示的评估指标性能的影响，并使用最新的LLMs对SOTA基于提示的指标进行重新校准，以进行公平比较。此外，我们展示了AXCEL在使用开源LLMs时表现出强大的性能。</td>
<td>P Aditya Sreekar</td>
<td><a href="http://arxiv.org/pdf/2409.16984v1">PDF</a></td>
<td>N/A</td>
<td>AXCEL: Automated eXplainable Consistency Evaluation using LLMs</td>
<td>Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated eXplainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs.</td>
</tr>
<tr>
<td>面向用户的训练数据归因研究：以人为本的可解释人工智能</td>
<td>尽管可解释人工智能（XAI）旨在使人工智能对人类来说既可理解又有用，但它因过于依赖形式主义和解决方案主义而受到批评，更关注数学上的合理性而非用户需求。我们提出了一种受设计思维启发的替代自下而上的方法：XAI研究社区应采用自上而下、以用户为中心的视角，以确保用户相关性。我们通过XAI中一个相对年轻的子领域——训练数据归属（TDA）来阐释这一点。随着TDA研究的激增和竞争的加剧，该领域存在重复解决方案主义模式的风险。我们与一群多样化的AI从业者进行了需求发现研究，以识别与TDA相关的潜在用户需求。通过访谈（N=10）和系统的调查（N=31），我们发现了目前很大程度上被忽视的新TDA任务。我们邀请TDA和XAI社区考虑这些新颖任务，并提升其研究成果的用户相关性。</td>
<td>Elisa Nguyen</td>
<td><a href="http://arxiv.org/pdf/2409.16978v1">PDF</a></td>
<td>N/A</td>
<td>Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI</td>
<td>While Explainable AI (XAI) aims to make AI understandable and useful to humans, it has been criticised for relying too much on formalism and solutionism, focusing more on mathematical soundness than user needs. We propose an alternative to this bottom-up approach inspired by design thinking: the XAI research community should adopt a top-down, user-focused perspective to ensure user relevance. We illustrate this with a relatively young subfield of XAI, Training Data Attribution (TDA). With the surge in TDA research and growing competition, the field risks repeating the same patterns of solutionism. We conducted a needfinding study with a diverse group of AI practitioners to identify potential user needs related to TDA. Through interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks that are currently largely overlooked. We invite the TDA and XAI communities to consider these novel tasks and improve the user relevance of their research outcomes.</td>
</tr>
<tr>
<td>解码大型语言模型：社会技术影响、约束及新兴问题的系统概述</td>
<td>近年来，大型语言模型（LLMs）的能力取得了快速进展，极大地革新了自然语言处理（NLP）和人工智能（AI）领域，使其能够理解和与人类语言互动。因此，在本研究中，我们对文献进行了系统的调查，以识别LLM发展、影响和局限性的主要主题和方向。我们的研究结果展示了LLM研究的目</td>
<td>Zeyneb N. Kaya</td>
<td><a href="http://arxiv.org/pdf/2409.16974v1">PDF</a></td>
<td>N/A</td>
<td>Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions</td>
<td>There have been rapid advancements in the capabilities of large language models (LLMs) in recent years, greatly revolutionizing the field of natural language processing (NLP) and artificial intelligence (AI) to understand and interact with human language. Therefore, in this work, we conduct a systematic investigation of the literature to identify the prominent themes and directions of LLM developments, impacts, and limitations. Our findings illustrate the aims, methodologies, limitations, and future directions of LLM research. It includes responsible development considerations, algorithmic improvements, ethical challenges, and societal implications of LLM development. Overall, this paper provides a rigorous and comprehensive overview of current research in LLM and identifies potential directions for future development. The article highlights the application areas that could have a positive impact on society along with the ethical considerations.</td>
</tr>
<tr>
<td>自适应自监督学习策略用于动态设备上大型语言模型个性化</td>
<td>大型语言模型（LLMs）已经彻底改变了我们与技术的互动方式，但它们对个人用户偏好的个性化仍然是一个重大挑战，尤其是在设备上的应用中。传统方法往往严重依赖标注数据集，且资源消耗大。为了解决这些问题，我们提出了自适应自监督学习策略（ASLS），该策略利用自监督学习技术来动态个性化LLMs。该框架包括一个用户画像层，用于收集互动数据，以及一个神经适应层，用于实时模型微调。这种创新方法使得模型能够从用户反馈中持续学习，从而生成与用户特定情境高度一致的响应。ASLS的自适应机制最小化了计算需求，并提高了个性化效率。在各种用户场景下的实验结果表明，ASLS在提升用户参与度和满意度方面表现卓越，突显了其在设备上重新定义LLMs为高度响应和情境感知系统的潜力。</td>
<td>Rafael Mendoza</td>
<td><a href="http://arxiv.org/pdf/2409.16973v1">PDF</a></td>
<td>N/A</td>
<td>Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization</td>
<td>Large language models (LLMs) have revolutionized how we interact with technology, but their personalization to individual user preferences remains a significant challenge, particularly in on-device applications. Traditional methods often depend heavily on labeled datasets and can be resource-intensive. To address these issues, we present Adaptive Self-Supervised Learning Strategies (ASLS), which utilizes self-supervised learning techniques to personalize LLMs dynamically. The framework comprises a user profiling layer for collecting interaction data and a neural adaptation layer for real-time model fine-tuning. This innovative approach enables continuous learning from user feedback, allowing the model to generate responses that align closely with user-specific contexts. The adaptive mechanisms of ASLS minimize computational demands and enhance personalization efficiency. Experimental results across various user scenarios illustrate the superior performance of ASLS in boosting user engagement and satisfaction, highlighting its potential to redefine LLMs as highly responsive and context-aware systems on-device.</td>
</tr>
<tr>
<td>无线人工智能范式的硬件在环真实环境桥梁</td>
<td>如今，许多用于改进车载自组织网络（VANET）中无线标准IEEE802.11p的机器学习（ML）解决方案通常在模拟环境中进行评估。与此同时，与现实世界测试相比，这种方法可能更具成本效益，因为车辆的成本较高。当这些解决方案在现实世界中实施时，存在意外结果的风险，可能导致资源浪费。为了应对这一挑战，硬件在环（Hardware-in-the-Loop）是前进的方向，因为它提供了在现实世界和模拟世界中共同测试的机会。因此，我们开发了我们认为是在模拟和现实世界环境中测试人工智能、多服务和高清地图数据（LiDAR）的开创性硬件在环系统。</td>
<td>Jeffrey Redondo</td>
<td><a href="http://arxiv.org/pdf/2409.16968v1">PDF</a></td>
<td>N/A</td>
<td>Bridge to Real Environment with Hardware-in-the-loop for Wireless Artificial Intelligence Paradigms</td>
<td>Nowadays, many machine learning (ML) solutions to improve the wireless standard IEEE802.11p for Vehicular Adhoc Network (VANET) are commonly evaluated in the simulated world. At the same time, this approach could be cost-effective compared to real-world testing due to the high cost of vehicles. There is a risk of unexpected outcomes when these solutions are implemented in the real world, potentially leading to wasted resources. To mitigate this challenge, the hardware-in-the-loop is the way to move forward as it enables the opportunity to test in the real world and simulated worlds together. Therefore, we have developed what we believe is the pioneering hardware-in-the-loop for testing artificial intelligence, multiple services, and HD map data (LiDAR), in both simulated and real-world settings.</td>
</tr>
<tr>
<td>ABCFair：一种用于比较公平方法的自适应基准方法</td>
<td>已经实现了许多方法，通过减轻机器学习中的偏见来追求对敏感特征的公平性。然而，每种方法所解决的问题设置存在显著差异，包括干预阶段、敏感特征的构成、公平性概念以及输出的分布。即使在二分类中，这些细微的差异也使得基准测试公平方法变得非常复杂，因为它们的表现可能强烈依赖于偏见缓解问题最初是如何构建的。因此，我们引入了ABCFair，这是一种基准方法，能够适应现实世界问题设置的需求，使得在任何用例中方法之间的比较更加合理。我们将ABCFair应用于一系列预处理、处理中和后处理方法，既包括大规模的传统数据集，也包括双标签（有偏和无偏）数据集，以规避公平性与准确性之间的权衡。</td>
<td>MaryBeth Defrance</td>
<td><a href="http://arxiv.org/pdf/2409.16965v1">PDF</a></td>
<td>N/A</td>
<td>ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods</td>
<td>Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.   Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. We apply ABCFair to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off.</td>
</tr>
<tr>
<td>求解方程组的元启发式方法</td>
<td>本研究探讨了遗传算法（GA）在求解线性及非线性方程组中的有效性，并将其性能与传统的解法如高斯消元法、牛顿法及列文伯格-马夸尔特法进行了比较。遗传算法在各种测试案例中均能提供精确的解，展现了其稳健性和灵活性。遗传算法的一个关键优势在于其能够广泛地探索解空间，揭示多组解——这是传统方法（通常仅收敛于单一解）所无法实现的。这一特性在复杂的非线性系统中尤为有益，因为这些系统往往存在多个有效解，从而凸显了遗传算法在应对复杂解空间时的优越性。</td>
<td>Samson Odan</td>
<td><a href="http://arxiv.org/pdf/2409.16958v1">PDF</a></td>
<td>N/A</td>
<td>Metaheuristic Method for Solving Systems of Equations</td>
<td>This study investigates the effectiveness of Genetic Algorithms (GAs) in solving both linear and nonlinear systems of equations, comparing their performance to traditional methods such as Gaussian Elimination, Newton's Method, and Levenberg-Marquardt. The GA consistently delivered accurate solutions across various test cases, demonstrating its robustness and flexibility. A key advantage of the GA is its ability to explore the solution space broadly, uncovering multiple sets of solutions -- a feat that traditional methods, which typically converge to a single solution, cannot achieve. This feature proved especially beneficial in complex nonlinear systems, where multiple valid solutions exist, highlighting the GA's superiority in navigating intricate solution landscapes.</td>
</tr>
<tr>
<td>信息化的深度分层分类：一种受非标准分析启发的非传统方法</td>
<td>本研究提出了一种新颖的深度层次分类任务方法，即根据多个标签在严格的父子结构中组织的数据分类问题。该方法采用了一种多输出深度神经网络，并在每个输出层前配备了特定的投影算子。这种被称为词典混合深度神经网络（LH-DNN）的架构设计，是通过结合来自不同且相当遥远研究领域的工具实现的：词典多目标优化、非标准分析和深度学习。为了评估该方法的有效性，我们将由此产生的网络与专门为层次分类任务设计的卷积神经网络（B-CNN）进行了比较，测试基准包括CIFAR10、CIFAR100（该网络最初在此提出，并最近被应用于多个实际场景并进行了调整）和Fashion-MNIST。证据表明，LH-DNN在大幅减少学习参数、训练周期和计算时间的情况下，仍能实现相当甚至更优的性能，特别是在学习层次关系方面，且无需专门设计的损失函数加权值。</td>
<td>Lorenzo Fiaschi</td>
<td><a href="http://arxiv.org/pdf/2409.16956v1">PDF</a></td>
<td>N/A</td>
<td>Informed deep hierarchical classification: a non-standard analysis inspired approach</td>
<td>This work proposes a novel approach to the deep hierarchical classification task, i.e., the problem of classifying data according to multiple labels organized in a rigid parent-child structure. It consists in a multi-output deep neural network equipped with specific projection operators placed before each output layer. The design of such an architecture, called lexicographic hybrid deep neural network (LH-DNN), has been possible by combining tools from different and quite distant research fields: lexicographic multi-objective optimization, non-standard analysis, and deep learning. To assess the efficacy of the approach, the resulting network is compared against the B-CNN, a convolutional neural network tailored for hierarchical classification tasks, on the CIFAR10, CIFAR100 (where it has been originally and recently proposed before being adopted and tuned for multiple real-world applications) and Fashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable if not superior performance, especially in the learning of the hierarchical relations, in the face of a drastic reduction of the learning parameters, training epochs, and computational time, without the need for ad-hoc loss functions weighting values.</td>
</tr>
<tr>
<td>多语言语音识别中低资源语言的加权交叉熵</td>
<td>本文探讨了将低资源语言整合到多语言自动语音识别（ASR）系统中的挑战。我们提出了一种新颖的应用，即加权交叉熵（通常用于不平衡数据集），以促进低资源语言在持续多语言学习背景下融入预训练的多语言ASR模型。我们对Whisper多语言ASR模型在五种高资源语言和一种低资源语言上进行了微调，采用了语言加权的动态交叉熵和数据增强技术。结果显示，与未应用我们方法的微调模型相比，低资源语言的词错误率（WER）显著降低了6.69%，与原始Whisper模型相比降低了48.86%。此外，我们的方法在六种语言中平均降低了3.29%的WER，且对高资源语言没有造成性能下降。</td>
<td>Andrés Piñeiro-Martín</td>
<td><a href="http://arxiv.org/pdf/2409.16954v1">PDF</a></td>
<td>N/A</td>
<td>Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition</td>
<td>This paper addresses the challenge of integrating low-resource languages into multilingual automatic speech recognition (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to facilitate the integration of low-resource languages into pre-trained multilingual ASR models within the context of continual multilingual learning. We fine-tune the Whisper multilingual ASR model on five high-resource languages and one low-resource language, employing language-weighted dynamic cross-entropy and data augmentation. The results show a remarkable 6.69% word error rate (WER) reduction for the low-resource language compared to the fine-tuned model without applying our approach, and a 48.86% WER reduction compared to the original Whisper model. In addition, our approach yields an average WER reduction of 3.29% across the six languages, showing no degradation for the high-resource languages.</td>
</tr>
<tr>
<td>基于不确定性的自适应规划与扩散的动态障碍物规避</td>
<td>通过将强化学习框架为序列建模问题，最近的研究使得生成模型，如扩散模型，能够用于规划。尽管这些模型在确定性环境中预测长时程状态轨迹方面表现有效，但它们在存在移动障碍物的动态环境中面临挑战。有效的碰撞避免需要持续监控和适应性决策。尽管在每个时间步重新规划可以确保安全，但由于重复预测重叠状态序列，这引入了大量的计算开销——尤其是对于以密集迭代采样过程著称的扩散模型来说，这一过程尤为昂贵。我们提出了一种自适应生成规划方法，该方法根据动作预测的不确定性动态调整重新规划的频率。我们的方法在保持强大的碰撞避免性能的同时，最小化了频繁、计算昂贵且冗余的重新规划需求。在实验中，我们获得了平均轨迹长度增加13.5%和长时程规划中平均奖励增加12.7%的效果，这表明碰撞率降低，并且在环境中安全导航的能力得到提升。</td>
<td>Vineet Punyamoorty</td>
<td><a href="http://arxiv.org/pdf/2409.16950v1">PDF</a></td>
<td>N/A</td>
<td>Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning with Diffusion</td>
<td>By framing reinforcement learning as a sequence modeling problem, recent work has enabled the use of generative models, such as diffusion models, for planning. While these models are effective in predicting long-horizon state trajectories in deterministic environments, they face challenges in dynamic settings with moving obstacles. Effective collision avoidance demands continuous monitoring and adaptive decision-making. While replanning at every timestep could ensure safety, it introduces substantial computational overhead due to the repetitive prediction of overlapping state sequences -- a process that is particularly costly with diffusion models, known for their intensive iterative sampling procedure. We propose an adaptive generative planning approach that dynamically adjusts replanning frequency based on the uncertainty of action predictions. Our method minimizes the need for frequent, computationally expensive, and redundant replanning while maintaining robust collision avoidance performance. In experiments, we obtain a 13.5% increase in the mean trajectory length and a 12.7% increase in mean reward over long-horizon planning, indicating a reduction in collision rates and an improved ability to navigate the environment safely.</td>
</tr>
<tr>
<td>基于多视角伪标签的语音半监督认知状态分类</td>
<td>在语音分类任务中，缺乏标注数据是一个常见的挑战，尤其是在需要大量主观评估的任务中，如认知状态分类。在这项工作中，我们提出了一种半监督学习（Semi-Supervised Learning, SSL）框架，引入了一种新颖的多视角伪标签方法，该方法利用声学和语言特征来选择最可信的数据用于训练分类模型。在声学方面，使用多个音频编码器生成的嵌入计算弗雷歇音频距离，将未标注数据与标注数据进行比较。在语言方面，大型语言模型被提示修正自动语音识别的转录文本，并根据我们提出的任务特定知识预测标签。当来自两个来源的伪标签一致时，识别出高置信度数据，而当伪标签不一致时，则视为低置信度数据。然后，训练一个双模态分类器，迭代地标注低置信度数据，直到达到预定义的标准。我们在情绪识别和痴呆检测任务上评估了我们的SSL框架。实验结果表明，与仅使用30%标注数据的全监督学习相比，我们的方法实现了具有竞争力的性能，并且显著优于两个选定的基线方法。</td>
<td>Yuanchao Li</td>
<td><a href="http://arxiv.org/pdf/2409.16937v1">PDF</a></td>
<td>N/A</td>
<td>Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling</td>
<td>The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.</td>
</tr>
<tr>
<td>探究OCR敏感神经元以提升历史文档中的实体识别</td>
<td>本文探讨了Transformer架构中存在的OCR敏感神经元及其对历史文档命名实体识别（NER）性能的影响。通过分析在干净和噪声文本输入下神经元的激活模式，我们识别并中和了OCR敏感神经元，从而提升了模型性能。基于两个开放访问的大型语言模型（Llama2和Mistral），实验证明了OCR敏感区域的存在，并展示了在历史报纸和古典评论上的NER性能提升，突显了针对神经元进行调制以提高模型在噪声文本上性能的潜力。</td>
<td>Emanuela Boros</td>
<td><a href="http://arxiv.org/pdf/2409.16934v1">PDF</a></td>
<td>N/A</td>
<td>Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents</td>
<td>This paper investigates the presence of OCR-sensitive neurons within the Transformer architecture and their influence on named entity recognition (NER) performance on historical documents. By analysing neuron activation patterns in response to clean and noisy text inputs, we identify and then neutralise OCR-sensitive neurons to improve model performance. Based on two open access large language models (Llama2 and Mistral), experiments demonstrate the existence of OCR-sensitive regions and show improvements in NER performance on historical newspapers and classical commentaries, highlighting the potential of targeted neuron modulation to improve models' performance on noisy text.</td>
</tr>
<tr>
<td>利用不变映射分解等变映射：对称性下通用逼近的应用</td>
<td>本文中，我们发展了一个关于群 ( G ) 的不变映射与等变映射之间关系的理论。随后，我们在具有群对称性的深度神经网络的背景下利用这一理论，以获得对其机制的新见解。更确切地说，我们建立了等变映射与某些不变映射之间的一一对应关系。这使我们能够将等变映射的论证简化为不变映射的论证，反之亦然。作为应用，我们提出了一种基于通用不变网络构建通用等变架构的方法。进而，我们解释了由我们的构造产生的通用架构与已知的通用标准等变架构之间的差异。此外，我们探讨了在自由参数数量方面的模型复杂性，并讨论了不变网络与等变网络复杂性之间的关系。最后，我们还给出了有限群 ( G ) 下带有 ReLU 激活函数的 ( G )-等变深度神经网络的逼近率。</td>
<td>Akiyoshi Sannai</td>
<td><a href="http://arxiv.org/pdf/2409.16922v1">PDF</a></td>
<td>N/A</td>
<td>Decomposition of Equivariant Maps via Invariant Maps: Application to Universal Approximation under Symmetry</td>
<td>In this paper, we develop a theory about the relationship between invariant and equivariant maps with regard to a group $G$. We then leverage this theory in the context of deep neural networks with group symmetries in order to obtain novel insight into their mechanisms. More precisely, we establish a one-to-one relationship between equivariant maps and certain invariant maps. This allows us to reduce arguments for equivariant maps to those for invariant maps and vice versa. As an application, we propose a construction of universal equivariant architectures built from universal invariant networks. We, in turn, explain how the universal architectures arising from our construction differ from standard equivariant architectures known to be universal. Furthermore, we explore the complexity, in terms of the number of free parameters, of our models, and discuss the relation between invariant and equivariant networks' complexity. Finally, we also give an approximation rate for G-equivariant deep neural networks with ReLU activation functions for finite group G.</td>
</tr>
<tr>
<td>跨语言语音情感识别：人类 vs. 自监督模型</td>
<td>利用自监督学习（SSL）模型进行语音情感识别（SER）已被证明是有效的，但针对跨语言场景的研究仍较为有限。本研究对人类表现与SSL模型进行了比较分析，首先从逐层分析入手，探讨了在单语言、跨语言和迁移学习情境下的参数高效微调策略。我们进一步比较了模型和人类在话语级和分段级上的SER能力。此外，我们通过人类评估研究了方言对跨语言SER的影响。研究结果表明，在适当的知识迁移下，模型能够适应目标语言并达到与母语者相当的性能。我们还展示了方言对没有先前语言和副语言背景的个体的SER具有显著影响。此外，人类和模型在不同情感上表现出不同的行为特征。这些结果为SSL模型的跨语言SER能力提供了新的见解，突显了它们在情感感知上与人类的相似性和差异性。</td>
<td>Zhichen Han</td>
<td><a href="http://arxiv.org/pdf/2409.16920v1">PDF</a></td>
<td>N/A</td>
<td>Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models</td>
<td>Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.</td>
</tr>
<tr>
<td>使用令牌内聚性进行零样本检测的LLM生成文本</td>
<td>随着大型语言模型（LLMs）的能力不断提升和广泛应用，自动检测LLM生成文本的需求日益凸显。零样本检测器因其无需训练的特性，受到了广泛关注并取得了显著成功。本文中，我们识别了一种新的特征——词元内聚性，这对零样本检测非常有用，并证明LLM生成的文本往往比人类书写的文本表现出更高的词元内聚性。基于这一观察，我们设计了TOCSIN，一种通用的双通道检测范式，利用词元内聚性作为即插即用模块来改进现有的零样本检测器。为了计算词元内聚性，TOCSIN仅需要进行几轮随机词元删除和语义差异测量，这使其特别适用于生成模型不可访问的实际黑箱场景。在多种数据集、源模型和评估设置下，对四个最先进的基线检测器进行的广泛实验证明了所提出方法的有效性和通用性。代码可在以下链接获取：\url{https://github.com/Shixuan-Ma/TOCSIN}。</td>
<td>Shixuan Ma</td>
<td><a href="http://arxiv.org/pdf/2409.16914v1">PDF</a></td>
<td>N/A</td>
<td>Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness</td>
<td>The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of LLM-generated text. Zero-shot detectors, due to their training-free nature, have received considerable attention and notable success. In this paper, we identify a new feature, token cohesiveness, that is useful for zero-shot detection, and we demonstrate that LLM-generated text tends to exhibit higher token cohesiveness than human-written text. Based on this observation, we devise TOCSIN, a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors. To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting where the source model used for generation is not accessible. Extensive experiments with four state-of-the-art base detectors on various datasets, source models, and evaluation settings demonstrate the effectiveness and generality of the proposed approach. Code available at: \url{https://github.com/Shixuan-Ma/TOCSIN}.</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d6f25eb3.min.js"></script>
      
    
  </body>
</html>