{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract PhysGen\uff1a\u57fa\u4e8e\u521a\u4f53\u7269\u7406\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210 \u6211\u4eec\u63d0\u51fa\u4e86PhysGen\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u5b83\u5c06\u5355\u5f20\u56fe\u50cf\u548c\u8f93\u5165\u6761\u4ef6\uff08\u4f8b\u5982\uff0c\u65bd\u52a0\u5728\u56fe\u50cf\u4e2d\u7269\u4f53\u4e0a\u7684\u529b\u548c\u626d\u77e9\uff09\u8f6c\u6362\u4e3a\u751f\u6210\u903c\u771f\u3001\u7269\u7406\u4e0a\u5408\u7406\u4e14\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u9891\u3002\u6211\u4eec\u7684\u6838\u5fc3\u89c1\u89e3\u662f\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u7269\u7406\u6a21\u62df\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u5408\u7406\u7684\u56fe\u50cf\u7a7a\u95f4\u52a8\u529b\u5b66\u3002\u6211\u4eec\u7cfb\u7edf\u7684\u5fc3\u810f\u662f\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\uff08i\uff09\u4e00\u4e2a\u56fe\u50cf\u7406\u89e3\u6a21\u5757\uff0c\u6709\u6548\u5730\u6355\u6349\u56fe\u50cf\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u6750\u6599\u548c\u7269\u7406\u53c2\u6570\uff1b\uff08ii\uff09\u4e00\u4e2a\u56fe\u50cf\u7a7a\u95f4\u52a8\u529b\u5b66\u6a21\u62df\u6a21\u578b\uff0c\u5229\u7528\u521a\u4f53\u7269\u7406\u5b66\u548c\u63a8\u65ad\u7684\u53c2\u6570\u6765\u6a21\u62df\u73b0\u5b9e\u884c\u4e3a\uff1b\uff08iii\uff09\u4e00\u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684\u6e32\u67d3\u548c\u7ec6\u5316\u6a21\u5757\uff0c\u5229\u7528\u751f\u6210\u6027\u89c6\u9891\u6269\u6563\u6280\u672f\u751f\u6210\u5305\u542b\u6a21\u62df\u8fd0\u52a8\u7684\u903c\u771f\u89c6\u9891\u7247\u6bb5\u3002\u751f\u6210\u7684\u89c6\u9891\u5728\u7269\u7406\u548c\u5916\u89c2\u4e0a\u90fd\u6781\u4e3a\u903c\u771f\uff0c\u751a\u81f3\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\uff0c\u901a\u8fc7\u5b9a\u91cf\u6bd4\u8f83\u548c\u5168\u9762\u7684\u7528\u6237\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u9a71\u52a8\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u7684\u5353\u8d8a\u7ed3\u679c\u3002PhysGen\u751f\u6210\u7684\u89c6\u9891\u53ef\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\uff0c\u4f8b\u5982\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u903c\u771f\u52a8\u753b\u6216\u5141\u8bb8\u7528\u6237\u4e0e\u56fe\u50cf\u4e92\u52a8\u5e76\u521b\u5efa\u5404\u79cd\u52a8\u6001\u6548\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://stevenlsw.github.io/physgen/ Shaowei Liu PDF N/A PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation We present PhysGen, a novel image-to-video generation method that converts a single image and an input condition (e.g., force and torque applied to an object in the image) to produce a realistic, physically plausible, and temporally consistent video. Our key insight is to integrate model-based physical simulation with a data-driven video generation process, enabling plausible image-space dynamics. At the heart of our system are three core components: (i) an image understanding module that effectively captures the geometry, materials, and physical parameters of the image; (ii) an image-space dynamics simulation model that utilizes rigid-body physics and inferred parameters to simulate realistic behaviors; and (iii) an image-based rendering and refinement module that leverages generative video diffusion to produce realistic video footage featuring the simulated motion. The resulting videos are realistic in both physics and appearance and are even precisely controllable, showcasing superior results over existing data-driven image-to-video generation works through quantitative comparison and comprehensive user study. PhysGen's resulting videos can be used for various downstream applications, such as turning an image into a realistic animation or allowing users to interact with the image and create various dynamics. Project page: https://stevenlsw.github.io/physgen/ \u63a2\u7d22\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u4ee4\u724c\u526a\u679d \u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u76f8\u8f83\u4e8e\u53d8\u538b\u5668\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5177\u6709\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5df2\u88ab\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002\u53d7\u5230\u89c6\u89c9\u53d8\u538b\u5668\uff08ViTs\uff09\u4e2d\u6700\u7ec8\u9884\u6d4b\u4ec5\u57fa\u4e8e\u6700\u4fe1\u606f\u4e30\u5bcc\u7684\u5b50\u96c6\u7684\u89c2\u5bdf\u542f\u53d1\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u901a\u8fc7\u57fa\u4e8e\u6807\u8bb0\u7684\u526a\u679d\u6765\u63d0\u9ad8\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u6548\u7387\u7684\u65b0\u6b65\u9aa4\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528\u4e3aViTs\u8bbe\u8ba1\u7684\u73b0\u6709\u6807\u8bb0\u526a\u679d\u6280\u672f\u672a\u80fd\u5e26\u6765\u826f\u597d\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u5e7f\u6cdb\u5fae\u8c03\u540e\u4e5f\u662f\u5982\u6b64\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86SSMs\u72ec\u7279\u7684\u8ba1\u7b97\u7279\u6027\uff0c\u5e76\u53d1\u73b0\u7b80\u5355\u5e94\u7528\u4f1a\u7834\u574f\u6807\u8bb0\u7684\u987a\u5e8f\u4f4d\u7f6e\u3002\u8fd9\u4e00\u53d1\u73b0\u4fc3\u4f7f\u6211\u4eec\u4e3a\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u8bbe\u8ba1\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u6807\u8bb0\u526a\u679d\u65b9\u6cd5\u3002\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u4e00\u79cd\u526a\u679d\u611f\u77e5\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u7a33\u5b9a\u5269\u4f59\u6807\u8bb0\u7684\u90bb\u57df\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6211\u4eec\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8eSSM\u6a21\u578b\u7684\u6807\u8bb0\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u6307\u5bfc\u6807\u8bb0\u526a\u679d\u3002\u901a\u8fc7\u9ad8\u6548\u7684\u5b9e\u73b0\u548c\u5b9e\u9645\u52a0\u901f\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5e26\u6765\u4e86\u5b9e\u9645\u7684\u52a0\u901f\u6548\u679c\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5728ImageNet\u4e0a\u5b9e\u73b0\u4e8681.7%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06\u526a\u679d\u540e\u7684PlainMamba-L3\u7684FLOPs\u51cf\u5c11\u4e8641.6%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u7406\u89e3\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u6a21\u578b\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 Zheng Zhan PDF N/A Exploring Token Pruning in Vision State Space Models State Space Models (SSMs) have the advantage of keeping linear computational complexity compared to attention modules in transformers, and have been applied to vision tasks as a new type of powerful vision foundation model. Inspired by the observations that the final prediction in vision transformers (ViTs) is only based on a subset of most informative tokens, we take the novel step of enhancing the efficiency of SSM-based vision models through token-based pruning. However, direct applications of existing token pruning techniques designed for ViTs fail to deliver good performance, even with extensive fine-tuning. To address this issue, we revisit the unique computational characteristics of SSMs and discover that naive application disrupts the sequential token positions. This insight motivates us to design a novel and general token pruning method specifically for SSM-based vision models. We first introduce a pruning-aware hidden state alignment method to stabilize the neighborhood of remaining tokens for performance enhancement. Besides, based on our detailed analysis, we propose a token importance evaluation method adapted for SSM models, to guide the token pruning. With efficient implementation and practical acceleration methods, our method brings actual speedup. Extensive experiments demonstrate that our approach can achieve significant computation reduction with minimal impact on performance across different tasks. Notably, we achieve 81.7\\% accuracy on ImageNet with a 41.6\\% reduction in the FLOPs for pruned PlainMamba-L3. Furthermore, our work provides deeper insights into understanding the behavior of SSM-based vision models for future research. \u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u6269\u6563\u6982\u7387\u6a21\u578b\u7684$O(d/T)$\u6536\u655b\u7406\u8bba \u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u9006\u8f6c\u4e00\u4e2a\u6269\u6563\u8fc7\u7a0b\u6765\u751f\u6210\u65b0\u6570\u636e\uff0c\u8be5\u8fc7\u7a0b\u5c06\u6765\u81ea\u76ee\u6807\u5206\u5e03\u7684\u6570\u636e\u6270\u52a8\u6210\u566a\u58f0\uff0c\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u5c3d\u7ba1\u5b83\u4eec\u5728\u5b9e\u8bc1\u8868\u73b0\u4e0a\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u4f46\u73b0\u6709\u7684\u7406\u8bba\u4fdd\u8bc1\u5f80\u5f80\u53d7\u5230\u4e25\u683c\u5047\u8bbe\u6216\u6b21\u4f18\u6536\u655b\u7387\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5728\u6700\u5c11\u7684\u5047\u8bbe\u4e0b\u4e3a\u4e00\u79cd\u6d41\u884c\u7684\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u7684\u91c7\u6837\u5668\u5efa\u7acb\u4e86\u5feb\u901f\u6536\u655b\u7406\u8bba\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5728\u63d0\u4f9b$\\ell_{2}$\u7cbe\u786e\u7684\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u76ee\u6807\u5206\u5e03\u4e0e\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\u88ab\u4e0a\u754c\u4e3a$O(d/T)$\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\uff0c\u5176\u4e2d$d$\u662f\u6570\u636e\u7ef4\u5ea6\uff0c$T$\u662f\u6b65\u6570\u3002\u8fd9\u4e00\u7ed3\u679c\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u6709\u9650\u4e00\u9636\u77e9\u7684\u76ee\u6807\u5206\u5e03\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u4e0d\u4ec5\u6539\u8fdb\u4e86\u57fa\u4e8eSDE\u7684\u91c7\u6837\u5668\u7684\u73b0\u6709\u6536\u655b\u7406\u8bba\uff0c\u8fd8\u6539\u8fdb\u4e86\u53e6\u4e00\u79cd\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u91c7\u6837\u5668\u7684\u6536\u655b\u7406\u8bba\uff0c\u540c\u65f6\u5bf9\u76ee\u6807\u6570\u636e\u5206\u5e03\u548c\u5206\u6570\u4f30\u8ba1\u65bd\u52a0\u4e86\u6700\u5c11\u7684\u5047\u8bbe\u3002\u8fd9\u4e00\u6210\u5c31\u901a\u8fc7\u4e00\u7ec4\u65b0\u9896\u7684\u5206\u6790\u5de5\u5177\u5b9e\u73b0\uff0c\u8fd9\u4e9b\u5de5\u5177\u63d0\u4f9b\u4e86\u5bf9\u53cd\u5411\u8fc7\u7a0b\u4e2d\u6bcf\u4e00\u6b65\u8bef\u5dee\u4f20\u64ad\u7684\u7ec6\u81f4\u523b\u753b\u3002 Gen Li PDF N/A $O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided $\\ell_{2}$-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process. LML\uff1a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u9884\u6d4b \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u8fd9\u7c7b\u4efb\u52a1\u901a\u5e38\u7531\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\u5904\u7406\u3002\u4e0e\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u6e05\u6d17\u548c\u7279\u5f81\u5de5\u7a0b\u7684ML\u6a21\u578b\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7LLMs\u7b80\u5316\u4e86\u6d41\u7a0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\uff08LML\uff09\u201d\u7684\u65b0\u6982\u5ff5\uff0c\u7531\u4e00\u79cd\u79f0\u4e3a\u201c\u6570\u636e\u589e\u5f3a\u9884\u6d4b\uff08DAP\uff09\u201d\u7684\u65b0\u65b9\u6cd5\u9a71\u52a8\u3002\u5206\u7c7b\u8fc7\u7a0b\u901a\u8fc7LLMs\u5b9e\u73b0\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u624b\u52a8\u63a2\u7d22\u548c\u7406\u89e3\u6570\u636e\uff0c\u5e76\u53c2\u8003\u6570\u636e\u505a\u51fa\u5206\u7c7b\u51b3\u7b56\u3002\u8bad\u7ec3\u6570\u636e\u88ab\u603b\u7ed3\u548c\u8bc4\u4f30\uff0c\u4ee5\u786e\u5b9a\u5bfc\u81f4\u6bcf\u4e2a\u6807\u7b7e\u5206\u7c7b\u7684\u6700\u663e\u8457\u7279\u5f81\u3002\u5728DAP\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u5229\u7528\u6570\u636e\u603b\u7ed3\u81ea\u52a8\u521b\u5efa\u67e5\u8be2\uff0c\u7528\u4e8e\u4ece\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u76f8\u5173\u884c\u3002LLM\u4f7f\u7528\u6570\u636e\u603b\u7ed3\u548c\u76f8\u5173\u884c\u751f\u6210\u5206\u7c7b\uff0c\u5373\u4f7f\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u65f6\u4e5f\u80fd\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u51c6\u786e\u6027\u3002DAP\u4e2d\u4f7f\u7528\u6570\u636e\u603b\u7ed3\u548c\u76f8\u4f3c\u6570\u636e\uff0c\u786e\u4fdd\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51b3\u7b56\u5236\u5b9a\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u793a\u4e2d\u4f7f\u7528\u201c\u626e\u6f14\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u201d\u7684\u8bcd\u8bed\uff0c\u901a\u8fc7\u5141\u8bb8\u7528\u6237\u5ba1\u67e5\u6bcf\u4e2a\u9884\u6d4b\u80cc\u540e\u7684\u903b\u8f91\uff0c\u589e\u5f3a\u4e86\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5728\u67d0\u4e9b\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u7cfb\u7edf\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8d85\u8d8a\u4f20\u7edfML\u6a21\u578b\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Pro-GenAI/LML-DAP\u83b7\u53d6\u3002 Praneeth Vadlapati PDF N/A LML: Language Model Learning a Dataset for Data-Augmented Prediction This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called \"Language Model Learning (LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words \"Act as an Explainable Machine Learning Model\" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP RepairBench\uff1a\u7a0b\u5e8f\u4fee\u590d\u524d\u6cbf\u6a21\u578b\u7684\u6392\u884c\u699c AI\u9a71\u52a8\u7684\u7a0b\u5e8f\u4fee\u590d\u5229\u7528AI\u6a21\u578b\u901a\u8fc7\u751f\u6210\u8865\u4e01\u6765\u4fee\u590d\u6709\u7f3a\u9677\u7684\u8f6f\u4ef6\u3002AI\u7684\u5feb\u901f\u53d1\u5c55\u65e0\u7591\u5f71\u54cd\u4e86\u7a0b\u5e8f\u4fee\u590d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u7136\u800c\uff0c\u8981\u628a\u63e1\u8fd9\u4e00\u8fdb\u5c55\uff0c\u9700\u8981\u9891\u7e41\u4e14\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u63d0\u51fa\u4e86RepairBench\uff0c\u4e00\u4e2a\u7528\u4e8eAI\u9a71\u52a8\u7a0b\u5e8f\u4fee\u590d\u7684\u65b0\u578b\u6392\u884c\u699c\u3002RepairBench\u7684\u5173\u952e\u7279\u70b9\u662f\uff1a1\uff09\u5b83\u662f\u57fa\u4e8e\u6267\u884c\u7684\uff1a\u6240\u6709\u8865\u4e01\u90fd\u4f1a\u9488\u5bf9\u6d4b\u8bd5\u5957\u4ef6\u8fdb\u884c\u7f16\u8bd1\u548c\u6267\u884c\uff1b2\uff09\u5b83\u4ee5\u9891\u7e41\u4e14\u6807\u51c6\u5316\u7684\u65b9\u5f0f\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u3002RepairBench\u5229\u7528\u4e24\u4e2a\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cDefects4J\u548cGitBug-Java\uff0c\u6765\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u5728\u5b9e\u9645\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86RepairBench\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u968f\u7740\u65b0\u524d\u6cbf\u6a21\u578b\u7684\u53d1\u5e03\uff0c\u6211\u4eec\u5c06\u66f4\u65b0\u6392\u884c\u699c\u3002 Andr\u00e9 Silva PDF N/A RepairBench: Leaderboard of Frontier Models for Program Repair AI-driven program repair uses AI models to repair buggy software by producing patches. Rapid advancements in AI surely impact state-of-the-art performance of program repair. Yet, grasping this progress requires frequent and standardized evaluations. We propose RepairBench, a novel leaderboard for AI-driven program repair. The key characteristics of RepairBench are: 1) it is execution-based: all patches are compiled and executed against a test suite, 2) it assesses frontier models in a frequent and standardized way. RepairBench leverages two high-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier models against real-world program repair tasks. We publicly release the evaluation framework of RepairBench. We will update the leaderboard as new frontier models are released. \u5149\u8c31\u5c0f\u6ce2\u4e22\u5f03\uff1a\u5c0f\u6ce2\u57df\u4e2d\u7684\u6b63\u5219\u5316 \u6b63\u5219\u5316\u6280\u672f\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fc7\u62df\u5408\u7684\u4e00\u4e2a\u539f\u56e0\u662f\u7f51\u7edc\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u7684\u590d\u6742\u5171\u9002\u5e94\u6027\uff0c\u8fd9\u4f7f\u5f97CNN\u4f9d\u8d56\u4e8e\u5b83\u4eec\u7684\u8054\u5408\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u9f13\u52b1\u6bcf\u4e2a\u90e8\u5206\u72ec\u7acb\u5b66\u4e60\u6709\u7528\u7684\u7279\u5f81\u8868\u793a\u3002\u9891\u57df\u64cd\u4f5c\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u9891\u7387\u5206\u89e3\u6765\u4fee\u6539\u5177\u6709\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u6570\u636e\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u5149\u8c31\u5c0f\u6ce2\u4e22\u5f03\uff08Spectral Wavelet Dropout, SWD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a1D-SWD\u548c2D-SWD\u3002\u8fd9\u4e9b\u53d8\u4f53\u901a\u8fc7\u968f\u673a\u4e22\u5f03\u7279\u5f81\u56fe\u79bb\u6563\u5c0f\u6ce2\u5206\u89e3\u4e2d\u7684\u7ec6\u8282\u9891\u5e26\uff0c\u6765\u63d0\u9ad8CNN\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u5149\u8c31\u201c\u5085\u91cc\u53f6\u201d\u4e22\u5f03\uff082D-SFD\uff09\u6709\u6240\u4e0d\u540c\uff0c\u540e\u8005\u662f\u5728\u5085\u91cc\u53f6\u57df\u4e2d\u6d88\u9664\u7cfb\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSWD\u53ea\u9700\u8981\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u800cSFD\u9700\u8981\u4e24\u4e2a\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5b9e\u73b0\u4e00\u7ef4\u7248\u672c\u7684\u5149\u8c31\u201c\u5085\u91cc\u53f6\u201d\u4e22\u5f03\uff081D-SFD\uff09\u6765\u6269\u5c55\u6587\u732e\uff0c\u4e3a\u5168\u9762\u6bd4\u8f83\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5728CIFAR-10/100\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c1D\u548c2D SWD\u53d8\u4f53\u76f8\u5bf9\u4e8e1D-SFD\u548c2D-SFD\u90fd\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c1D-SWD\u76f8\u6bd41D/2D-SFD\u5177\u6709\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728Pascal VOC\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWD\u53d8\u4f53\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e861D-SFD\u548c2D-SFD\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002 Rinor Cakaj PDF N/A Spectral Wavelet Dropout: Regularization in the Wavelet Domain Regularization techniques help prevent overfitting and therefore improve the ability of convolutional neural networks (CNNs) to generalize. One reason for overfitting is the complex co-adaptations among different parts of the network, which make the CNN dependent on their joint response rather than encouraging each part to learn a useful feature representation independently. Frequency domain manipulation is a powerful strategy for modifying data that has temporal and spatial coherence by utilizing frequency decomposition. This work introduces Spectral Wavelet Dropout (SWD), a novel regularization method that includes two variants: 1D-SWD and 2D-SWD. These variants improve CNN generalization by randomly dropping detailed frequency bands in the discrete wavelet decomposition of feature maps. Our approach distinguishes itself from the pre-existing Spectral \"Fourier\" Dropout (2D-SFD), which eliminates coefficients in the Fourier domain. Notably, SWD requires only a single hyperparameter, unlike the two required by SFD. We also extend the literature by implementing a one-dimensional version of Spectral \"Fourier\" Dropout (1D-SFD), setting the stage for a comprehensive comparison. Our evaluation shows that both 1D and 2D SWD variants have competitive performance on CIFAR-10/100 benchmarks relative to both 1D-SFD and 2D-SFD. Specifically, 1D-SWD has a significantly lower computational complexity compared to 1D/2D-SFD. In the Pascal VOC Object Detection benchmark, SWD variants surpass 1D-SFD and 2D-SFD in performance and demonstrate lower computational complexity during training. \u5b9e\u73b0\u9664\u6cd5\u5f52\u4e00\u5316\u7684\u9012\u5f52\u795e\u7ecf\u7535\u8def\u7684\u65e0\u6761\u4ef6\u7a33\u5b9a\u6027 \u5728\u9012\u5f52\u795e\u7ecf\u6a21\u578b\u4e2d\uff0c\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5f00\u53d1\u80fd\u591f\u65e0\u7f1d\u8bad\u7ec3\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u6a21\u578b\u65f6\u3002\u4f20\u7edf\u7684\u76ae\u5c42\u7535\u8def\u6a21\u578b\u7531\u4e8e\u52a8\u529b\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\u7279\u6027\u800c\u96be\u4ee5\u8bad\u7ec3\uff0c\u5bfc\u81f4\u4f18\u5316\u95ee\u9898\u5177\u6709\u96be\u4ee5\u65bd\u52a0\u7684\u975e\u7ebf\u6027\u7a33\u5b9a\u6027\u7ea6\u675f\u3002\u76f8\u53cd\uff0c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u5728\u6d89\u53ca\u5e8f\u5217\u6570\u636e\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u751f\u7269\u5b66\u4e0a\u7684\u5408\u7406\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u52a8\u6001\u5206\u88c2\u5f52\u4e00\u5316\uff08DN\uff09\u4e0eORGaNICs\u7684\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\u3002ORGaNICs\u662f\u4e00\u79cd\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u9012\u5f52\u76ae\u5c42\u7535\u8def\u6a21\u578b\uff0c\u80fd\u591f\u52a8\u6001\u5b9e\u73b0DN\uff0c\u5e76\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u6a21\u62df\u5e7f\u6cdb\u7684\u795e\u7ecf\u751f\u7406\u73b0\u8c61\u3002\u901a\u8fc7\u4f7f\u7528\u674e\u96c5\u666e\u8bfa\u592b\u95f4\u63a5\u65b9\u6cd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5f53\u9012\u5f52\u6743\u91cd\u77e9\u9635\u4e3a\u5355\u4f4d\u77e9\u9635\u65f6\uff0c\u4efb\u610f\u7ef4\u5ea6\u7684ORGaNICs\u7535\u8def\u5177\u6709\u65e0\u6761\u4ef6\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u663e\u8457\u7279\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06ORGaNICs\u4e0e\u8026\u5408\u963b\u5c3c\u8c10\u632f\u5b50\u7cfb\u7edf\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u80fd\u591f\u63a8\u5bfc\u51fa\u7535\u8def\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u63d0\u4f9b\u7535\u8def\u53ca\u5176\u5355\u4e2a\u795e\u7ecf\u5143\u6240\u8ffd\u6c42\u7684\u89c4\u8303\u6027\u539f\u5219\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u4e00\u822c\u7684\u9012\u5f52\u6743\u91cd\u77e9\u9635\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e8c\u7ef4\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8868\u660e\u5728\u9ad8\u7ef4\u5ea6\u4e0b\u7a33\u5b9a\u6027\u4ecd\u7136\u6210\u7acb\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u7531\u4e8eORGaNICs\u7684\u5185\u5728\u7a33\u5b9a\u6027\u7279\u6027\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5e38\u6570\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u68af\u5ea6\u88c1\u526a/\u7f29\u653e\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u68af\u5ea6\u7206\u70b8\u3001\u6d88\u5931\u548c\u632f\u8361\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5728RNN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u53d1\u73b0ORGaNICs\u5728\u9759\u6001\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u795e\u7ecf\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u4e0eLSTM\u8868\u73b0\u76f8\u5f53\u3002 Shivang Rawat PDF N/A Unconditional stability of a recurrent neural circuit implementing divisive normalization Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ORGaNICs, a biologically plausible recurrent cortical circuit model that dynamically achieves DN and has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks. Ruler\uff1a\u4e00\u79cd\u7528\u4e8e\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u957f\u5ea6\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5 \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4f7f\u5f97\u4eba\u7c7b\u80fd\u591f\u4ee5\u81ea\u7136\u7684\u65b9\u5f0f\u4e0eAI\u4ee3\u7406\u8fdb\u884c\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u5f53\u9700\u8981\u751f\u6210\u7279\u5b9a\u957f\u5ea6\u7684\u54cd\u5e94\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u7684\u9700\u6c42\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u51c6\u786e\u611f\u77e5\u6570\u503c\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u56fa\u6709\u7684\u56f0\u96be\u3002\u4e3a\u4e86\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a7\u5236\u751f\u6210\u54cd\u5e94\u957f\u5ea6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u76ee\u6807\u957f\u5ea6\u751f\u6210\u4efb\u52a1\uff08TLG\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5ea6\u91cf\u6807\u51c6\uff1a\u7cbe\u786e\u5339\u914d\uff08PM\uff09\u548c\u7075\u6d3b\u5339\u914d\uff08FM\uff09\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u9075\u5b88\u6307\u5b9a\u54cd\u5e94\u957f\u5ea6\u65b9\u9762\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aRuler\uff0c\u5b83\u5229\u7528\u5143\u957f\u5ea6\u6807\u8bb0\uff08MLT\uff09\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5ea6\u7ea6\u675f\u6307\u4ee4\u4e0b\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0cRuler\u8d4b\u4e88LLMs\u6839\u636e\u6307\u4ee4\u4e2d\u7684\u957f\u5ea6\u7ea6\u675f\u751f\u6210\u6307\u5b9a\u957f\u5ea6\u54cd\u5e94\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f53\u957f\u5ea6\u7ea6\u675f\u672a\u660e\u786e\u63d0\u4f9b\u65f6\uff0cRuler\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9002\u5f53\u7684MLT\uff0c\u5c55\u793a\u4e86\u51fa\u8272\u7684\u591a\u529f\u80fd\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cRuler\u5728\u76ee\u6807\u957f\u5ea6\u751f\u6210\u4efb\u52a1\u4e0a\u5bf9\u4e0d\u540cLLMs\u7684\u6709\u6548\u6027\uff0c\u4f8b\u5982\uff0c\u5728PM\u4e0a\u5e73\u5747\u83b7\u5f9727.97\u7684\u589e\u76ca\uff0c\u5728FM\u4e0a\u5e73\u5747\u83b7\u5f9729.57\u7684\u589e\u76ca\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u4ee5\u8fdb\u4e00\u6b65\u8bc1\u5b9eRuler\u7684\u529f\u6548\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/Geaming2002/Ruler\u83b7\u53d6\u3002 Jiaming Li PDF N/A Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler. AIPatient\uff1a\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u6a21\u62df\u60a3\u8005 \u6a21\u62df\u60a3\u8005\u7cfb\u7edf\u5728\u73b0\u4ee3\u533b\u5b66\u6559\u80b2\u548c\u7814\u7a76\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\uff0c\u63d0\u4f9b\u5b89\u5168\u3001\u7efc\u5408\u7684\u5b66\u4e60\u73af\u5883\uff0c\u5e76\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u7684\u6a21\u62df\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u901a\u8fc7\u9ad8\u4fdd\u771f\u5ea6\u548c\u4f4e\u6210\u672c\u5730\u590d\u5236\u533b\u7597\u72b6\u51b5\u548c\u533b\u60a3\u4e92\u52a8\uff0c\u4ece\u800c\u63a8\u8fdb\u6a21\u62df\u60a3\u8005\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u4e00\u4e2a\u5e9e\u5927\u3001\u591a\u6837\u4e14\u7cbe\u786e\u7684\u60a3\u8005\u77e5\u8bc6\u5e93\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u77e5\u8bc6\u4f20\u9012\u7ed9\u7528\u6237\u3002\u5728\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86AIPatient\uff0c\u4e00\u4e2a\u5148\u8fdb\u7684\u6a21\u62df\u60a3\u8005\u7cfb\u7edf\uff0c\u4ee5AIPatient\u77e5\u8bc6\u56fe\u8c31\uff08AIPatient KG\uff09\u4e3a\u8f93\u5165\uff0c\u5e76\u4ee5\u63a8\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Reasoning RAG\uff09\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u4e3a\u751f\u6210\u6838\u5fc3\u3002AIPatient KG\u4ece\u91cd\u75c7\u76d1\u62a4\u533b\u5b66\u4fe1\u606f\u96c6\u5e02\uff08MIMIC\uff09-III\u6570\u636e\u5e93\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u91c7\u6837\u6570\u636e\uff0c\u751f\u6210\u4e86\u4e00\u4e2a\u4e34\u5e8a\u591a\u6837\u4e14\u76f8\u5173\u6027\u5f3a\u76841,495\u540d\u60a3\u8005\u7684\u961f\u5217\uff0c\u77e5\u8bc6\u5e93\u7684\u6709\u6548\u6027\u9ad8\uff08F1 0.89\uff09\u3002Reasoning RAG\u5229\u7528\u516d\u4e2a\u7531LLM\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u6db5\u76d6\u68c0\u7d22\u3001KG\u67e5\u8be2\u751f\u6210\u3001\u62bd\u8c61\u3001\u68c0\u67e5\u3001\u91cd\u5199\u548c\u603b\u7ed3\u7b49\u4efb\u52a1\u3002\u8be5\u4ee3\u7406\u6846\u67b6\u5728\u57fa\u4e8eEHR\u7684\u533b\u5b66\u95ee\u7b54\uff08QA\uff09\u4e2d\u8fbe\u5230\u4e8694.15%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4e0d\u4f7f\u7528\u4ee3\u7406\u6216\u4ec5\u90e8\u5206\u96c6\u6210\u4ee3\u7406\u7684\u57fa\u51c6\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u8fd8\u8868\u73b0\u51fa\u9ad8\u53ef\u8bfb\u6027\uff08Flesch\u9605\u8bfb\u6613\u5ea6\u4e2d\u4f4d\u657077.23\uff1bFlesch Kincaid\u5e74\u7ea7\u4e2d\u4f4d\u65705.6\uff09\u3001\u9c81\u68d2\u6027\uff08ANOVA F\u503c0.6126\uff0cp&lt;0.1\uff09\u548c\u7a33\u5b9a\u6027\uff08ANOVA F\u503c0.782\uff0cp&lt;0.1\uff09\u3002AIPatient\u7cfb\u7edf\u7684\u826f\u597d\u8868\u73b0\u7a81\u663e\u4e86\u5176\u5728\u652f\u6301\u5e7f\u6cdb\u5e94\u7528\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5305\u62ec\u533b\u5b66\u6559\u80b2\u3001\u6a21\u578b\u8bc4\u4f30\u548c\u7cfb\u7edf\u96c6\u6210\u3002 Huizi Yu PDF N/A AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow Simulated patient systems play a crucial role in modern medical education and research, providing safe, integrative learning environments and enabling clinical decision-making simulations. Large Language Models (LLM) could advance simulated patient systems by replicating medical conditions and patient-doctor interactions with high fidelity and low cost. However, ensuring the effectiveness and trustworthiness of these systems remains a challenge, as they require a large, diverse, and precise patient knowledgebase, along with a robust and stable knowledge diffusion to users. Here, we developed AIPatient, an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning RAG) agentic workflow as the generation backbone. AIPatient KG samples data from Electronic Health Records (EHRs) in the Medical Information Mart for Intensive Care (MIMIC)-III database, producing a clinically diverse and relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89). Reasoning RAG leverages six LLM powered agents spanning tasks including retrieval, KG query generation, abstraction, checker, rewrite, and summarization. This agentic framework reaches an overall accuracy of 94.15% in EHR-based medical Question Answering (QA), outperforming benchmarks that use either no agent or only partial agent integration. Our system also presents high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade 5.6), robustness (ANOVA F-value 0.6126, p&lt;0.1), and stability (ANOVA F-value 0.782, p&lt;0.1). The promising performance of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration. A-FedPD\uff1a\u5bf9\u9f50\u53cc\u91cd\u6f02\u79fb\u662f\u6240\u6709\u8054\u90a6\u539f\u59cb-\u5bf9\u5076\u5b66\u4e60\u6240\u9700\u7684 \u4f5c\u4e3a\u4e00\u79cd\u6d41\u884c\u7684\u8303\u5f0f\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u84ec\u52c3\u53d1\u5c55\uff0c\u65e8\u5728\u5728\u8fb9\u7f18\u5ba2\u6237\u7aef\u4e0a\u5206\u5e03\u5f0f\u5904\u7406\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\u3002\u7531\u4e8e\u5e26\u5bbd\u9650\u5236\u548c\u5b89\u5168\u8003\u8651\uff0c\u5b83\u5de7\u5999\u5730\u5c06\u539f\u59cb\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u95ee\u9898\uff0c\u4ee5\u4fbf\u5e76\u884c\u89e3\u51b3\uff0c\u4ece\u800c\u4f7f\u539f\u59cb\u5bf9\u5076\u89e3\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5177\u6709\u5de8\u5927\u7684\u5e94\u7528\u4ef7\u503c\u3002\u672c\u6587\u56de\u987e\u4e86\u7ecf\u5178\u8054\u90a6\u539f\u59cb\u5bf9\u5076\u65b9\u6cd5\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u5e76\u6307\u51fa\u5728\u975e\u51f8\u573a\u666f\u4e0b\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u4e00\u4e2a\u4e25\u91cd\u7684\u5171\u540c\u7f3a\u9677\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u7531\u4e8e\u90e8\u5206\u53c2\u4e0e\u8bad\u7ec3\u4e0b\u957f\u671f\u4e0d\u6d3b\u8dc3\u5ba2\u6237\u7aef\u7684\u5bf9\u5076\u6ede\u540e\u6027\u5f15\u8d77\u7684\u201c\u5bf9\u5076\u6f02\u79fb\u201d\u3002\u4e3a\u8fdb\u4e00\u6b65\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u8054\u90a6\u539f\u59cb\u5bf9\u5076\uff08A-FedPD\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u865a\u62df\u5bf9\u5076\u66f4\u65b0\uff0c\u4ee5\u5bf9\u9f50\u5168\u5c40\u5171\u8bc6\u548c\u90a3\u4e9b\u957f\u671f\u672a\u53c2\u4e0e\u7684\u672c\u5730\u5ba2\u6237\u7aef\u7684\u5c40\u90e8\u5bf9\u5076\u53d8\u91cf\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5bf9A-FedPD\u65b9\u6cd5\u5728\u5149\u6ed1\u975e\u51f8\u76ee\u6807\u4e0a\u7684\u4f18\u5316\u548c\u6cdb\u5316\u6548\u7387\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8bc1\u5b9e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002\u5728\u51e0\u4e2a\u7ecf\u5178\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002 Yan Sun PDF N/A A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs As a popular paradigm for juggling data privacy and collaborative training, federated learning (FL) is flourishing to distributively process the large scale of heterogeneous datasets on edged clients. Due to bandwidth limitations and security considerations, it ingeniously splits the original problem into multiple subproblems to be solved in parallel, which empowers primal dual solutions to great application values in FL. In this paper, we review the recent development of classical federated primal dual methods and point out a serious common defect of such methods in non-convex scenarios, which we say is a \"dual drift\" caused by dual hysteresis of those longstanding inactive clients under partial participation training. To further address this problem, we propose a novel Aligned Federated Primal Dual (A-FedPD) method, which constructs virtual dual updates to align global consensus and local dual variables for those protracted unparticipated local clients. Meanwhile, we provide a comprehensive analysis of the optimization and generalization efficiency for the A-FedPD method on smooth non-convex objectives, which confirms its high efficiency and practicality. Extensive experiments are conducted on several classical FL setups to validate the effectiveness of our proposed method. \u7528\u4e8e\u63d0\u53d6\u56e0\u679c\u96c6\u4f53\u667a\u80fd\u7684\u8f6f\u6027\u63aa\u65bd \u7406\u89e3\u548c\u5efa\u6a21\u96c6\u4f53\u667a\u80fd\u5bf9\u4e8e\u89e3\u51b3\u590d\u6742\u7684\u793e\u4f1a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u6a21\u7cca\u8ba4\u77e5\u56fe\uff08FCMs\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u5411\u56fe\uff0c\u63d0\u4f9b\u4e86\u7f16\u7801\u56e0\u679c\u5fc3\u7406\u6a21\u578b\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u9ad8\u5b8c\u6574\u6027\u7684FCMs\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u81ea\u52a8\u5316\u63d0\u53d6FCM\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u4e8e\u56fe\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7Elo\u8bc4\u5206\u7cfb\u7edf\u5c06\u5176\u8f93\u51fa\u4e0e\u4eba\u7c7b\u5224\u65ad\u8fdb\u884c\u5173\u8054\u6765\u8bc4\u4f30\u8fd9\u4e9b\u5ea6\u91cf\u3002\u7ed3\u679c\u663e\u793a\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5448\u6b63\u76f8\u5173\uff0c\u4f46\u5373\u4f7f\u662f\u6700\u4f18\u7684\u5ea6\u91cf\u65b9\u6cd5\u5728\u6355\u6349FCM\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002\u5fae\u8c03LLMs\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u5ea6\u91cf\u4ecd\u663e\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9FCM\u63d0\u53d6\u5b9a\u5236\u8f6f\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63a8\u8fdb\u96c6\u4f53\u667a\u80fd\u5efa\u6a21\u3002 Maryam Berijanian PDF N/A Soft Measures for Extracting Causal Collective Intelligence Understanding and modeling collective intelligence is essential for addressing complex social systems. Directed graphs called fuzzy cognitive maps (FCMs) offer a powerful tool for encoding causal mental models, but extracting high-integrity FCMs from text is challenging. This study presents an approach using large language models (LLMs) to automate FCM extraction. We introduce novel graph-based similarity measures and evaluate them by correlating their outputs with human judgments through the Elo rating system. Results show positive correlations with human evaluations, but even the best-performing measure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs improves performance, but existing measures still fall short. This study highlights the need for soft similarity measures tailored to FCM extraction, advancing collective intelligence modeling with NLP. \u6700\u5c0f\u9057\u61be\u7684\u6700\u4f73\u81c2\u8bc6\u522b \u53d7\u73b0\u5b9e\u5e94\u7528\u9700\u6c42\u7684\u9a71\u52a8\uff0c\u8fd9\u4e9b\u5e94\u7528\u8981\u6c42\u8fdb\u884c\u8d1f\u8d23\u4efb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5e26\u6709\u6700\u5c0f\u9057\u61be\u7684\u6700\u4f73\u81c2\u8bc6\u522b\uff08BAI\uff09\u95ee\u9898\u3002\u8fd9\u79cd\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7684\u521b\u65b0\u53d8\u4f53\u4f18\u96c5\u5730\u7ed3\u5408\u4e86\u5176\u6700\u666e\u904d\u7684\u4e24\u4e2a\u76ee\u6807\uff1a\u9057\u61be\u6700\u5c0f\u5316\u548cBAI\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u667a\u80fd\u4f53\u7684\u76ee\u6807\u662f\u5728\u89c4\u5b9a\u7684\u7f6e\u4fe1\u6c34\u5e73$\\delta$\u4e0b\u8bc6\u522b\u51fa\u6700\u4f73\u81c2\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5230\u505c\u6b62\u65f6\u95f4\u4e3a\u6b62\u7684\u7d2f\u79ef\u9057\u61be\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u5355\u53c2\u6570\u6307\u6570\u65cf\u5206\u5e03\uff0c\u5229\u7528\u4fe1\u606f\u8bba\u6280\u672f\u5efa\u7acb\u4e86\u9884\u671f\u7d2f\u79ef\u9057\u61be\u7684\u5b9e\u4f8b\u4f9d\u8d56\u4e0b\u754c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u4e0d\u53ef\u80fd\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u56fa\u5b9a\u7f6e\u4fe1\u5ea6BAI\u4e2d\u7d2f\u79ef\u9057\u61be\u548c\u6837\u672c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\u3002\u4f5c\u4e3a\u8865\u5145\uff0c\u6211\u4eec\u8bbe\u8ba1\u548c\u5206\u6790\u4e86Double KL-UCB\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u7f6e\u4fe1\u6c34\u5e73\u8d8b\u8fd1\u4e8e\u96f6\u65f6\u8fbe\u5230\u6e10\u8fd1\u6700\u4f18\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u7b97\u6cd5\u91c7\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u7f6e\u4fe1\u754c\u9650\uff0c\u4ee5\u968f\u673a\u65b9\u5f0f\u6307\u5bfc\u81c2\u7684\u9009\u62e9\u3002\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u9057\u61be\u6700\u5c0f\u5316\u548cBAI\u4e4b\u95f4\u5185\u5728\u8054\u7cfb\u7684\u65b0\u89c6\u89d2\u3002 Junwen Yang PDF N/A Best Arm Identification with Minimal Regret Motivated by real-world applications that necessitate responsible experimentation, we introduce the problem of best arm identification (BAI) with minimal regret. This innovative variant of the multi-armed bandit problem elegantly amalgamates two of its most ubiquitous objectives: regret minimization and BAI. More precisely, the agent's goal is to identify the best arm with a prescribed confidence level $\\delta$, while minimizing the cumulative regret up to the stopping time. Focusing on single-parameter exponential families of distributions, we leverage information-theoretic techniques to establish an instance-dependent lower bound on the expected cumulative regret. Moreover, we present an intriguing impossibility result that underscores the tension between cumulative regret and sample complexity in fixed-confidence BAI. Complementarily, we design and analyze the Double KL-UCB algorithm, which achieves asymptotic optimality as the confidence level tends to zero. Notably, this algorithm employs two distinct confidence bounds to guide arm selection in a randomized manner. Our findings elucidate a fresh perspective on the inherent connections between regret minimization and BAI. \u533b\u7597\u6570\u636e\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u5a01\u80c1\u6df1\u5165\u5206\u6790 \u8054\u90a6\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u6b63\u5728\u533b\u7597\u9886\u57df\u4e2d\u5d2d\u9732\u5934\u89d2\uff0c\u7528\u4e8e\u5206\u6790\u533b\u7597\u56fe\u50cf\uff0c\u56e0\u4e3a\u5b83\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u654f\u611f\u7684\u60a3\u8005\u6570\u636e\u5e76\u9075\u5b88\u9690\u79c1\u6cd5\u89c4\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u8054\u90a6\u5b66\u4e60\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u5c06\u79c1\u4eba\u8bad\u7ec3\u6570\u636e\u66b4\u9732\u7ed9\u9690\u79c1\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u8fd9\u79cd\u9690\u79c1\u98ce\u9669\u7684\u7a0b\u5ea6\u4ee5\u53ca\u5728\u533b\u7597\u9886\u57df\u4e2d\u7684\u6f5c\u5728\u7f13\u89e3\u7b56\u7565\u4ecd\u7136\u4e0d\u660e\u786e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u533b\u7597\u6570\u636e\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u5206\u6790\u548c\u7f13\u89e3\u505a\u51fa\u4e86\u4e09\u4e2a\u539f\u521b\u6027\u8d21\u732e\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6MedPFL\uff0c\u7528\u4e8e\u5206\u6790\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5904\u7406\u533b\u7597\u6570\u636e\u65f6\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u7f13\u89e3\u7b56\u7565\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u6211\u4eec\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8054\u90a6\u5b66\u4e60\u5904\u7406\u533b\u7597\u56fe\u50cf\u65f6\u5b58\u5728\u7684\u4e25\u91cd\u9690\u79c1\u98ce\u9669\uff0c\u5176\u4e2d\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u6267\u884c\u9690\u79c1\u653b\u51fb\u51c6\u786e\u5730\u91cd\u5efa\u79c1\u4eba\u533b\u7597\u56fe\u50cf\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u8bf4\u660e\u4e86\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u6dfb\u52a0\u968f\u673a\u566a\u58f0\u7684\u5e38\u89c1\u9632\u5fa1\u673a\u5236\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u6709\u6548\u5730\u4fdd\u62a4\u533b\u7597\u56fe\u50cf\u514d\u53d7\u9690\u79c1\u653b\u51fb\uff0c\u8fd9\u4e3a\u4fdd\u62a4\u533b\u7597\u6570\u636e\u7684\u9690\u79c1\u5e26\u6765\u4e86\u72ec\u7279\u800c\u7d27\u8feb\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u8ba8\u8bba\u4e86\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u533b\u7597\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u76f8\u5173\u7684\u51e0\u4e2a\u72ec\u7279\u7814\u7a76\u95ee\u9898\u3002\u6211\u4eec\u5728\u51e0\u4e2a\u57fa\u51c6\u533b\u7597\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u5206\u6790\u548c\u7f13\u89e3\u8054\u90a6\u5b66\u4e60\u4e2d\u4e0e\u533b\u7597\u6570\u636e\u76f8\u5173\u7684\u9690\u79c1\u98ce\u9669\u3002 Badhan Chandra Das PDF N/A In-depth Analysis of Privacy Threats in Federated Learning for Medical Data Federated learning is emerging as a promising machine learning technique in the medical field for analyzing medical images, as it is considered an effective method to safeguard sensitive patient data and comply with privacy regulations. However, recent studies have revealed that the default settings of federated learning may inadvertently expose private training data to privacy attacks. Thus, the intensity of such privacy risks and potential mitigation strategies in the medical domain remain unclear. In this paper, we make three original contributions to privacy risk analysis and mitigation in federated learning for medical data. First, we propose a holistic framework, MedPFL, for analyzing privacy risks in processing medical data in the federated learning environment and developing effective mitigation strategies for protecting privacy. Second, through our empirical analysis, we demonstrate the severe privacy risks in federated learning to process medical images, where adversaries can accurately reconstruct private medical images by performing privacy attacks. Third, we illustrate that the prevalent defense mechanism of adding random noises may not always be effective in protecting medical images against privacy attacks in federated learning, which poses unique and pressing challenges related to protecting the privacy of medical data. Furthermore, the paper discusses several unique research questions related to the privacy protection of medical data in the federated learning environment. We conduct extensive experiments on several benchmark medical image datasets to analyze and mitigate the privacy risks associated with federated learning for medical data. \u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3001\u6b63\u4ea4\u6295\u5f71\u4e0eQR\u5206\u89e3\u7b97\u6cd5\u7684\u6982\u7387\u5206\u6790 \u672c\u6587\u6269\u5c55\u4e86Liesen\u7b49\u4eba\uff082002\uff09\u7684\u5de5\u4f5c\uff0c\u4ed6\u4eec\u5206\u6790\u4e86\u5728\u5411\u6b63\u4ea4\u77e9\u9635Q\u6dfb\u52a0\u4e00\u5217\uff08[Q, c]\uff09\u65f6\u6761\u4ef6\u6570\u7684\u53d8\u5316\uff0c\u7279\u522b\u5173\u6ce8\u4e86c\u4e0eQ\u7684\u5217\u7a7a\u95f4\u4e4b\u95f4\u7684\u6b63\u4ea4\u6027\u3002Liesen\u7b49\u4eba\uff082002\uff09\u5728\u5b9a\u74062.3\u4e2d\u63d0\u51fa\u7684\u7ed3\u679c\u5047\u8bbe\u4e86\u7cbe\u786e\u7b97\u672f\u548cQ\u7684\u6b63\u4ea4\u6027\uff0c\u8fd9\u5728\u5c06\u8fd9\u4e9b\u7ed3\u679c\u5e94\u7528\u4e8eQR\u5206\u89e3\u7b97\u6cd5\u7b49\u6570\u503c\u65b9\u6cd5\u65f6\u662f\u4e00\u4e2a\u5f3a\u5047\u8bbe\u3002\u5728\u6211\u4eec\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63a8\u5bfc\u77e9\u9635B\u7684\u6761\u4ef6\u6570\u589e\u52a0\u7684\u754c\u9650\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u800c\u4e0d\u5047\u8bbe\u5b8c\u7f8e\u7684\u6b63\u4ea4\u6027\uff0c\u5373\u4f7f\u5728\u6dfb\u52a0\u7684\u5217\u4e0d\u5b8c\u5168\u6b63\u4ea4\u4e8eB\u7684\u5217\u7a7a\u95f4\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u8fd9\u4e00\u6846\u67b6\u4f7f\u6211\u4eec\u80fd\u591f\u5206\u6790\u6b63\u4ea4\u5316\u4e0d\u5b8c\u5168\u4e14\u53d7\u9ad8\u65af\u566a\u58f0\u5f71\u54cd\u7684QR\u5206\u89e3\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5728\u9ad8\u65af\u566a\u58f0\u4e0b\u6b63\u4ea4\u6295\u5f71\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6027\u80fd\u7684\u7ed3\u679c\uff0c\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u8fd9\u4e00\u7406\u8bba\u7684\u53d1\u5c55\u3002 Ali Lotfi PDF N/A Probabilistic Analysis of Least Squares, Orthogonal Projection, and QR Factorization Algorithms Subject to Gaussian Noise In this paper, we extend the work of Liesen et al. (2002), which analyzes how the condition number of an orthonormal matrix Q changes when a column is added ([Q, c]), particularly focusing on the perpendicularity of c to the span of Q. Their result, presented in Theorem 2.3 of Liesen et al. (2002), assumes exact arithmetic and orthonormality of Q, which is a strong assumption when applying these results to numerical methods such as QR factorization algorithms. In our work, we address this gap by deriving bounds on the condition number increase for a matrix B without assuming perfect orthonormality, even when a column is not perfectly orthogonal to the span of B. This framework allows us to analyze QR factorization methods where orthogonalization is imperfect and subject to Gaussian noise. We also provide results on the performance of orthogonal projection and least squares under Gaussian noise, further supporting the development of this theory. \u591a\u6e90\u786c\u4fe1\u606f\u4e0e\u8f6f\u4fe1\u606f\u878d\u5408\u65b9\u6cd5\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u8d70\u52bf \u91d1\u878d\u548c\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u6700\u91cd\u8981\u7684\u6311\u6218\u4e4b\u4e00\u662f\u51c6\u786e\u9884\u6d4b\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u8d8b\u52bf\u3002\u5229\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6709\u52a9\u4e8e\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u4ee5\u5176\u663e\u8457\u7684\u589e\u957f\u548c\u6ce2\u52a8\u6027\u5438\u5f15\u4e86\u6295\u8d44\u8005\u548c\u5b66\u8005\uff0c\u4ed6\u4eec\u70ed\u8877\u4e8e\u89e3\u8bfb\u548c\u9884\u6d4b\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u8d70\u52bf\u3002\u8fd9\u79cd\u9884\u6d4b\u6240\u9700\u7684\u5e9e\u5927\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u52a0\u4e86\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u786c\u4fe1\u606f\u4e0e\u8f6f\u4fe1\u606f\u878d\u5408\uff08HSIF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u8d70\u52bf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u65b9\u6cd5\u4e2d\u7684\u786c\u4fe1\u606f\u90e8\u5206\u5305\u62ec\u5386\u53f2\u4ef7\u683c\u8bb0\u5f55\u548c\u6280\u672f\u6307\u6807\u3002\u4e0e\u4e4b\u4e92\u8865\u7684\u662f\uff0c\u8f6f\u6570\u636e\u90e8\u5206\u4eceX\uff08\u539fTwitter\uff09\u4e2d\u63d0\u53d6\uff0c\u6db5\u76d6\u6709\u5173\u52a0\u5bc6\u8d27\u5e01\u7684\u65b0\u95fb\u6807\u9898\u548c\u63a8\u6587\u3002\u4e3a\u4e86\u5229\u7528\u8fd9\u4e9b\u6570\u636e\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u57fa\u4e8eTransformer\u7684\u53cc\u5411\u7f16\u7801\u8868\u793a\uff08BERT\uff09\u7684\u60c5\u611f\u5206\u6790\u65b9\u6cd5\uff0c\u5373\u91d1\u878dBERT\uff08FinBERT\uff09\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u7684\u6a21\u578b\u57fa\u4e8e\u5305\u542b\u5904\u7406\u540e\u7684\u786c\u4fe1\u606f\u548c\u8f6f\u4fe1\u606f\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u91c7\u7528\u4e86\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\uff08BiLSTM\uff09\u6a21\u578b\uff0c\u56e0\u4e3a\u53cc\u5411\u5904\u7406\u4fe1\u606f\u53ef\u4ee5\u6355\u6349\u5e8f\u5217\u4fe1\u606f\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86HSIF\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u6e90\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u901a\u8fc7\u6bd4\u7279\u5e01\u76f8\u5173\u6570\u636e\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u901a\u8fc7\u878d\u5408\u6bd4\u7279\u5e01\u6570\u636e\u96c6\u4e2d\u7684\u786c\u4fe1\u606f\u548c\u8f6f\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u9884\u6d4b\u4ef7\u683c\u8d70\u52bf\u65b9\u9762\u8fbe\u5230\u4e86\u7ea696.8%\u7684\u51c6\u786e\u7387\u3002\u6574\u5408\u4fe1\u606f\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u628a\u63e1\u793e\u4f1a\u60c5\u7eea\u5bf9\u4ef7\u683c\u6ce2\u52a8\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u8865\u5145\u4e86\u57fa\u4e8e\u786c\u4fe1\u606f\u7684\u6280\u672f\u5206\u6790\u9884\u6d4b\u3002 Saeed Mohammadi Dashtaki PDF N/A Multi-Source Hard and Soft Information Fusion Approach for Accurate Cryptocurrency Price Movement Prediction One of the most important challenges in the financial and cryptocurrency field is accurately predicting cryptocurrency price trends. Leveraging artificial intelligence (AI) is beneficial in addressing this challenge. Cryptocurrency markets, marked by substantial growth and volatility, attract investors and scholars keen on deciphering and forecasting cryptocurrency price movements. The vast and diverse array of data available for such predictions increases the complexity of the task. In our study, we introduce a novel approach termed hard and soft information fusion (HSIF) to enhance the accuracy of cryptocurrency price movement forecasts. The hard information component of our approach encompasses historical price records alongside technical indicators. Complementing this, the soft data component extracts from X (formerly Twitter), encompassing news headlines and tweets about the cryptocurrency. To use this data, we use the Bidirectional Encoder Representations from Transformers (BERT)-based sentiment analysis method, financial BERT (FinBERT), which performs best. Finally, our model feeds on the information set including processed hard and soft data. We employ the bidirectional long short-term memory (BiLSTM) model because processing information in both forward and backward directions can capture long-term dependencies in sequential information. Our empirical findings emphasize the superiority of the HSIF approach over models dependent on single-source data by testing on Bitcoin-related data. By fusing hard and soft information on Bitcoin dataset, our model has about 96.8\\% accuracy in predicting price movement. Incorporating information enables our model to grasp the influence of social sentiment on price fluctuations, thereby supplementing the technical analysis-based predictions derived from hard information. HM3\uff1a\u7528\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5206\u5c42\u591a\u76ee\u6807\u6a21\u578b\u5408\u5e76 \u6a21\u578b\u878d\u5408\u662f\u4e00\u79cd\u5c06\u591a\u4e2a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5408\u5e76\u4e3a\u4e00\u4e2a\u6027\u80fd\u66f4\u5f3a\u3001\u4efb\u52a1\u9002\u5e94\u6027\u66f4\u5e7f\u7684\u5355\u4e00\u6a21\u578b\u7684\u6280\u672f\u3002\u7531\u4e8e\u5176\u80fd\u591f\u7ed5\u8fc7\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u548c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u8fc7\u7a0b\u7684\u9700\u6c42\uff0c\u6a21\u578b\u878d\u5408\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u53d1\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u591a\u6570\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u63a2\u7d22\u53c2\u6570\u7a7a\u95f4\uff0c\u5408\u5e76\u5177\u6709\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u3002\u5c3d\u7ba1\u67b6\u6784\u7a7a\u95f4\u5185\u7684\u5408\u5e76\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u641c\u7d22\u7a7a\u95f4\u5e9e\u5927\u548c\u5c42\u517c\u5bb9\u6027\u6311\u6218\uff0c\u8fd9\u4e00\u9886\u57df\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\u3002\u672c\u6587\u901a\u8fc7\u5c06\u67b6\u6784\u7a7a\u95f4\u5408\u5e76\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u6807\u5fd7\u7740\u5411\u66f4\u7075\u6d3b\u3001\u66f4\u5168\u9762\u7684\u6a21\u578b\u878d\u5408\u6280\u672f\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u6211\u4eec\u4f7f\u7528\u79bb\u7ebf\u91c7\u6837\u7684\u6743\u91cd\u5411\u91cf\u8bad\u7ec3\u7b56\u7565\u548c\u4ef7\u503c\u7f51\u7edc\uff0c\u8fd9\u4e9b\u7f51\u7edc\u968f\u540e\u7528\u4e8e\u5728\u7ebf\u4f18\u5316\u5408\u5e76\u7b56\u7565\u3002\u6b64\u5916\uff0c\u5f15\u5165\u591a\u76ee\u6807\u4f18\u5316\u8303\u5f0f\u4ee5\u9002\u5e94\u7528\u6237\u591a\u6837\u7684\u4efb\u52a1\u504f\u597d\uff0c\u5b66\u4e60\u6700\u4f18\u6a21\u578b\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4ee5\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u5408\u5e76\u5efa\u8bae\u3002\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5305\u62ec\u6587\u672c\u7ffb\u8bd1\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u5728\u6a21\u578b\u878d\u5408\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u4ee3\u7801\u5c06\u5728\u8bc4\u5ba1\u8fc7\u7a0b\u540e\u516c\u5f00\u53d1\u5e03\u3002 Yu Zhou PDF N/A HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models Model merging is a technique that combines multiple large pretrained models into a single model with enhanced performance and broader task adaptability. It has gained popularity in large pretrained model development due to its ability to bypass the need for original training data and further training processes. However, most existing model merging approaches focus solely on exploring the parameter space, merging models with identical architectures. Merging within the architecture space, despite its potential, remains in its early stages due to the vast search space and the challenges of layer compatibility. This paper marks a significant advance toward more flexible and comprehensive model merging techniques by modeling the architecture-space merging process as a reinforcement learning task. We train policy and value networks using offline sampling of weight vectors, which are then employed for the online optimization of merging strategies. Moreover, a multi-objective optimization paradigm is introduced to accommodate users' diverse task preferences, learning the Pareto front of optimal models to offer customized merging suggestions. Experimental results across multiple tasks, including text translation, mathematical reasoning, and code generation, validate the effectiveness and superiority of the proposed framework in model merging. The code will be made publicly available after the review process. IDGen\uff1a\u7528\u4e8eLLM\u8bc4\u4f30\u7684\u7269\u54c1\u533a\u5206\u5f15\u5bfc\u751f\u6210 \u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u8bc4\u4f30\u96c6\u5fc5\u987b\u8ddf\u4e0a\u8fd9\u4e9b\u8fdb\u6b65\uff0c\u4ee5\u786e\u4fdd\u5176\u4fdd\u6301\u8db3\u591f\u7684\u533a\u5206\u5ea6\u3002\u9879\u76ee\u533a\u5206\u5ea6\uff08ID\uff09\u7406\u8bba\u5728\u6559\u80b2\u8bc4\u4f30\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u4e8e\u8861\u91cf\u5355\u4e2a\u6d4b\u8bd5\u9879\u76ee\u533a\u5206\u9ad8\u4f4e\u5206\u8005\u7684\u80fd\u529b\u3002\u53d7\u6b64\u7406\u8bba\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eID\u7684\u63d0\u793a\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u96c6\u80fd\u591f\u6839\u636e\u6a21\u578b\u80fd\u529b\u6301\u7eed\u66f4\u65b0\u548c\u4f18\u5316\u3002\u6211\u4eec\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\u6ce8\u91cd\u5e7f\u5ea6\u548c\u7279\u5f02\u6027\uff0c\u80fd\u591f\u751f\u6210\u5168\u9762\u8bc4\u4f30LLMs\u80fd\u529b\u5e76\u63ed\u793a\u6a21\u578b\u95f4\u6709\u610f\u4e49\u6027\u80fd\u5dee\u5f02\u7684\u63d0\u793a\uff0c\u4ece\u800c\u5728\u5404\u79cd\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u6709\u6548\u533a\u5206\u5176\u76f8\u5bf9\u4f18\u52a3\u3002\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u6211\u4eec\u5728\u6cdb\u5316\u6846\u67b6\u4e2d\u5f15\u5165\u4e86\u81ea\u6821\u6b63\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u6a21\u578b\u6765\u9884\u6d4b\u63d0\u793a\u7684\u533a\u5206\u5ea6\u548c\u96be\u5ea6\u8bc4\u5206\uff0c\u4ee5\u4fc3\u8fdb\u6211\u4eec\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u4e3a\u8bc4\u4f30\u6570\u636e\u5408\u6210\u7814\u7a76\u8d21\u732e\u4e86\u5b9d\u8d35\u7684\u5de5\u5177\u3002\u6211\u4eec\u5c06\u751f\u6210\u7684\u6570\u636e\u5e94\u7528\u4e8e\u8bc4\u4f30\u4e94\u4e2aSOTA\u6a21\u578b\uff0c\u6570\u636e\u663e\u793a\u5e73\u5747\u5f97\u5206\u4e3a51.92\uff0c\u65b9\u5dee\u4e3a10.06\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5148\u524d\u7684\u5de5\u4f5c\uff08\u5982SELF-INSTRUCT\u548cWizardLM\uff09\u5e73\u5747\u5f97\u5206\u8d85\u8fc767\uff0c\u65b9\u5dee\u4f4e\u4e8e3.2\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u751f\u6210\u7684\u6570\u636e\u76f8\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u66f4\u5177\u6311\u6218\u6027\u548c\u533a\u5206\u5ea6\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u4e00\u4e2a\u5305\u542b\u8d85\u8fc73000\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdbLLMs\u7684\u8bc4\u4f30\u7814\u7a76\u3002 Fan Lin PDF N/A IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation As Large Language Models (LLMs) grow increasingly adept at managing complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs to ensure the evaluation set can continually update and refine according to model abilities. Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains. To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework, and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2. The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works. We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs. HR-Extreme\uff1a\u4e00\u4e2a\u7528\u4e8e\u6781\u7aef\u5929\u6c14\u9884\u62a5\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6 \u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u5e94\u7528\uff0c\u5df2\u7ecf\u663e\u8457\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5176\u4e2d\u5305\u62ec\u9ad8\u5206\u8fa8\u7387\u9884\u62a5\u548c\u5ef6\u957f\u7684\u9884\u6d4b\u5468\u671f\uff0c\u5982Pangu\u548cFuxi\u6a21\u578b\u6240\u5c55\u793a\u7684\u90a3\u6837\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u6210\u529f\uff0c\u4f46\u4ee5\u5f80\u7684\u7814\u7a76\u5927\u591a\u5ffd\u89c6\u4e86\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\uff0c\u5e76\u4e14\u4e13\u95e8\u4e3a\u8fd9\u7c7b\u4e8b\u4ef6\u5b9a\u5236\u7684\u6570\u636e\u96c6\u4ecd\u7136\u6709\u9650\u3002\u9274\u4e8e\u51c6\u786e\u9884\u62a5\u6781\u7aef\u5929\u6c14\u7684\u81f3\u5173\u91cd\u8981\u6027\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u4eceNOAA\u63d0\u4f9b\u7684\u9ad8\u5206\u8fa8\u7387\u5feb\u901f\u5237\u65b0\uff08HRRR\uff09\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u9ad8\u5206\u8fa8\u7387\u6781\u7aef\u5929\u6c14\u6848\u4f8b\uff0cHRRR\u662f\u4e00\u4e2a3\u516c\u91cc\u7684\u5b9e\u65f6\u6570\u636e\u96c6\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u7cfb\u7edf\u5728HR-Extreme\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u57fa\u51c6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u79f0\u4e3aHR-Heim\uff0c\u5b83\u5728\u603b\u4f53\u635f\u5931\u548cHR-Extreme\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6781\u7aef\u5929\u6c14\u6848\u4f8b\u7684\u8bef\u5dee\u663e\u8457\u5927\u4e8e\u6574\u4f53\u9884\u62a5\u8bef\u5dee\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u5929\u6c14\u9884\u6d4b\u4e2d\u4f5c\u4e3a\u91cd\u8981\u635f\u5931\u6765\u6e90\u7684\u5730\u4f4d\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u9700\u8981\u96c6\u4e2d\u7cbe\u529b\u63d0\u9ad8\u6781\u7aef\u5929\u6c14\u9884\u62a5\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u589e\u5f3a\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002 Nian Ran PDF N/A HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting The application of large deep learning models in weather forecasting has led to significant advancements in the field, including higher-resolution forecasting and extended prediction periods exemplified by models such as Pangu and Fuxi. Despite these successes, previous research has largely been characterized by the neglect of extreme weather events, and the availability of datasets specifically curated for such events remains limited. Given the critical importance of accurately forecasting extreme weather, this study introduces a comprehensive dataset that incorporates high-resolution extreme weather cases derived from the High-Resolution Rapid Refresh (HRRR) data, a 3-km real-time dataset provided by NOAA. We also evaluate the current state-of-the-art deep learning models and Numerical Weather Prediction (NWP) systems on HR-Extreme, and provide a improved baseline deep learning model called HR-Heim which has superior performance on both general loss and HR-Extreme compared to others. Our results reveal that the errors of extreme weather cases are significantly larger than overall forecast error, highlighting them as an crucial source of loss in weather prediction. These findings underscore the necessity for future research to focus on improving the accuracy of extreme weather forecasts to enhance their practical utility. \u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ece\u5b89\u5168\u7f51\u7cbe\u795e\u75c5\u533b\u9662\u7684\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u8fdb\u884c\u81ea\u6740\u8868\u578b\u5206\u6790\u7684\u591a\u6807\u7b7e\u5206\u7c7b \u51c6\u786e\u7684\u81ea\u6740\u4e8b\u4ef6\u8bc6\u522b\u548c\u5206\u7c7b\u53ef\u4ee5\u5e26\u6765\u66f4\u597d\u7684\u81ea\u6740\u9884\u9632\u63aa\u65bd\uff0c\u51cf\u8f7b\u64cd\u4f5c\u8d1f\u62c5\uff0c\u5e76\u63d0\u9ad8\u9ad8\u5f3a\u5ea6\u7cbe\u795e\u75c5\u5b66\u73af\u5883\u4e2d\u7684\u62a4\u7406\u8d28\u91cf\u3002\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u53d9\u8ff0\u4e2d\u8bc6\u522b\u81ea\u6740\u503e\u5411\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u4e8eBERT\u7684\u6a21\u578b\u5728\u4f7f\u7528\u4e24\u79cd\u5fae\u8c03\u7b56\u7565\uff08\u591a\u91cd\u5355\u6807\u7b7e\u548c\u5355\u4e00\u591a\u6807\u7b7e\uff09\u68c0\u6d4b500\u4efd\u6ce8\u91ca\u7684\u7cbe\u795e\u75c5\u5b66\u8bc4\u4f30\u7b14\u8bb0\u4e2d\u5e76\u5b58\u7684\u81ea\u6740\u4e8b\u4ef6\u65f6\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u7b14\u8bb0\u88ab\u6807\u8bb0\u4e3a\u81ea\u6740\u610f\u5ff5\uff08SI\uff09\u3001\u81ea\u6740\u4f01\u56fe\uff08SA\uff09\u3001\u81ea\u6740\u66b4\u9732\uff08ES\uff09\u548c\u975e\u81ea\u6740\u6027\u81ea\u4f24\uff08NSSI\uff09\u3002RoBERTa\u5728\u4f7f\u7528\u4e8c\u5143\u76f8\u5173\u6027\u65f6\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff08acc=0.86, F1=0.78\uff09\u3002MentalBERT\uff08F1=0.74\uff09\u4e5f\u8d85\u8fc7\u4e86BioClinicalBERT\uff08F1=0.72\uff09\u3002\u4f7f\u7528\u5355\u4e00\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u5fae\u8c03\u7684RoBERTa\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\uff08acc=0.88, F1=0.81\uff09\uff0c\u8fd9\u8868\u660e\u5728\u9886\u57df\u76f8\u5173\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u5355\u4e00\u591a\u6807\u7b7e\u5206\u7c7b\u7b56\u7565\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002 <p>\u5173\u952e\u8bcd\uff1a\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8868\u578b\u5206\u6790\uff1b\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff1b\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u4e8c\u6b21\u5229\u7528\uff1b\u81ea\u6740\u5206\u7c7b\uff1b\u57fa\u4e8eBERT\u7684\u6a21\u578b\uff1b\u7cbe\u795e\u75c5\u5b66\uff1b\u5fc3\u7406\u5065\u5eb7 | Zehan Li | PDF | N/A | Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models | Accurate identification and categorization of suicidal events can yield better suicide precautions, reducing operational burden, and improving care quality in high-acuity psychiatric settings. Pre-trained language models offer promise for identifying suicidality from unstructured clinical narratives. We evaluated the performance of four BERT-based models using two fine-tuning strategies (multiple single-label and single multi-label) for detecting coexisting suicidal events from 500 annotated psychiatric evaluation notes. The notes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure to suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed other models using binary relevance (acc=0.86, F1=0.78). MentalBERT (F1=0.74) also exceeded BioClinicalBERT (F1=0.72). RoBERTa fine-tuned with a single multi-label classifier further improved performance (acc=0.88, F1=0.81), highlighting that models pre-trained on domain-relevant data and the single multi-label classification strategy enhance efficiency and performance.   Keywords: EHR-based Phynotyping; Natural Language Processing; Secondary Use of EHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health | | CESNET-TimeSeries24\uff1a\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u4e0e\u9884\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6 | \u7f51\u7edc\u6d41\u91cf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5bf9\u4e8e\u7ef4\u62a4\u8ba1\u7b97\u673a\u7f51\u7edc\u7684\u5b89\u5168\u548c\u8bc6\u522b\u6076\u610f\u6d3b\u52a8\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u9884\u6d4b\u7684\u65b9\u6cd5\u662f\u5f02\u5e38\u68c0\u6d4b\u7684\u4e3b\u8981\u65b9\u6cd5\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u7684\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5bf9\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u6027\u80fd\u7684\u9ad8\u4f30\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5305\u542b\u7f51\u7edc\u5b9e\u4f53\u884c\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8be5\u6570\u636e\u96c6\u662f\u4eceCESNET3\u7f51\u7edc\u4e2d\u6536\u96c6\u7684\u3002\u8be5\u6570\u636e\u96c6\u753140\u5468\u518527.5\u4e07\u4e2a\u6d3b\u8dc3IP\u5730\u5740\u7684\u7f51\u7edc\u6d41\u91cf\u521b\u5efa\u3002\u6240\u5c55\u793a\u6570\u636e\u7684\u670d\u52a1\u63d0\u4f9b\u5546\u6765\u6e90\u786e\u4fdd\u4e86\u7f51\u7edc\u5b9e\u4f53\u4e4b\u95f4\u7684\u9ad8\u5ea6\u53d8\u5f02\u6027\uff0c\u8fd9\u4e3a\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u6784\u6210\u4e86\u72ec\u7279\u4e14\u771f\u5b9e\u7684\u6311\u6218\u3002\u5b83\u4e3a\u57fa\u4e8e\u9884\u6d4b\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002 | Josef Koumar | PDF | N/A | CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting | Anomaly detection in network traffic is crucial for maintaining the security of computer networks and identifying malicious activities. One of the primary approaches to anomaly detection are methods based on forecasting. Nevertheless, extensive real-world network datasets for forecasting and anomaly detection techniques are missing, potentially causing performance overestimation of anomaly detection algorithms. This manuscript addresses this gap by introducing a dataset comprising time series data of network entities' behavior, collected from the CESNET3 network. The dataset was created from 40 weeks of network traffic of 275 thousand active IP addresses. The ISP origin of the presented data ensures a high level of variability among network entities, which forms a unique and authentic challenge for forecasting and anomaly detection models. It provides valuable insights into the practical deployment of forecast-based anomaly detection approaches. | | \u4f7f\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6a21\u62df\u4e73\u817aMRI\u4e2d\u7684\u52a8\u6001\u80bf\u7624\u5bf9\u6bd4\u589e\u5f3a | \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e73\u817aMRI\u4e2d\u8fdb\u884c\u865a\u62df\u5bf9\u6bd4\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u975e\u4fb5\u5165\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u5bf9\u6bd4\u5242\u7684DCE-MRI\u91c7\u96c6\u3002\u5229\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u6211\u4eec\u4ece\u65e0\u5bf9\u6bd4\u589e\u5f3a\u7684MRI\u4e2d\u9884\u6d4bDCE-MRI\u56fe\u50cf\uff0c\u5305\u62ec\u8054\u5408\u751f\u6210\u7684\u591a\u4e2a\u76f8\u5e94DCE-MRI\u65f6\u95f4\u70b9\u7684\u5e8f\u5217\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e0d\u5e26\u6765\u76f8\u5173\u5065\u5eb7\u98ce\u9669\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u80bf\u7624\u5b9a\u4f4d\u548c\u7279\u5f81\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9a\u6027\u548c\u5b9a\u91cf\u5730\u8bc4\u4f30\u4e86\u5408\u6210\u7684DCE-MRI\u56fe\u50cf\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6307\u6807\u7684\u7f29\u653e\u805a\u5408\u5ea6\u91cf\uff08SAMe\uff09\uff0c\u8bc4\u4f30\u5176\u5728\u80bf\u7624\u5206\u5272\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u6700\u7ec8\u5206\u6790\u4e86\u591a\u5e8f\u5217DCE-MRI\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u903c\u771f\u4e14\u6709\u7528\u7684DCE-MRI\u5e8f\u5217\u65b9\u9762\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u865a\u62df\u5bf9\u6bd4\u589e\u5f3a\u5728\u6539\u5584\u4e73\u817a\u764c\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u7981\u5fcc\u4f7f\u7528\u5bf9\u6bd4\u5242\u7684\u60a3\u8005\u3002 | Richard Osuala | PDF | N/A | Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks | This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE-MRI acquisition. Using a conditional generative adversarial network, we predict DCE-MRI images, including jointly-generated sequences of multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs, enabling tumor localization and characterization without the associated health risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe), assessing their utility in a tumor segmentation downstream task, and conclude with an analysis of the temporal patterns in multi-sequence DCE-MRI generation. Our approach demonstrates promising results in generating realistic and useful DCE-MRI sequences, highlighting the potential of virtual contrast enhancement for improving breast cancer diagnosis and treatment, particularly for patients where contrast agent administration is contraindicated. | | \u5177\u6709\u548c\u6ca1\u6709\u89c6\u89c9\u57fa\u7840\u7684\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u4e2a\u4f53\u5316 | \u6211\u4eec\u5c55\u793a\u4e86\u5728\u7f16\u7801\u4e2a\u4f53\u5316\u4fe1\u606f\u65b9\u9762\uff0c\u8bed\u8a00\u4e0e\u89c6\u89c9\u6a21\u578bCLIP\u4e0e\u4e24\u4e2a\u7eaf\u6587\u672c\u6a21\u578b\u2014\u2014FastText\u548cSBERT\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6211\u4eec\u7814\u7a76\u4e86CLIP\u4e3a\u57fa\u8d28\u3001\u9897\u7c92\u805a\u96c6\u4f53\u4ee5\u53ca\u5404\u79cd\u6570\u91cf\u7269\u4f53\u63d0\u4f9b\u7684\u6f5c\u5728\u8868\u793a\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLIP\u5d4c\u5165\u6bd4\u4ec5\u57fa\u4e8e\u6587\u672c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u4e86\u4e2a\u4f53\u5316\u4e2d\u7684\u6570\u91cf\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4eceCLIP\u5d4c\u5165\u4e2d\u63a8\u5bfc\u51fa\u7684\u4e2a\u4f53\u5316\u5c42\u6b21\u7ed3\u6784\u4e0e\u8bed\u8a00\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u4e2d\u63d0\u51fa\u7684\u5c42\u6b21\u7ed3\u6784\u76f8\u4e00\u81f4\u3002 | Alexey Tikhonov | PDF | N/A | Individuation in Neural Models with and without Visual Grounding | We show differences between a language-and-vision model CLIP and two text-only models - FastText and SBERT - when it comes to the encoding of individuation information. We study latent representations that CLIP provides for substrates, granular aggregates, and various numbers of objects. We demonstrate that CLIP embeddings capture quantitative differences in individuation better than models trained on text-only data. Moreover, the individuation hierarchy we deduce from the CLIP embeddings agrees with the hierarchies proposed in linguistics and cognitive science. | | \u4f4d\u7f6e\u7f16\u7801\u56fe\u5206\u4f4d\u6570\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5730\u7406\u6570\u636e | \u4f4d\u7f6e\u7f16\u7801\u56fe\u795e\u7ecf\u7f51\u7edc\uff08PE-GNNs\uff09\u662f\u5efa\u6a21\u8fde\u7eed\u7a7a\u95f4\u6570\u636e\u7684\u9886\u5148\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5f80\u5f80\u65e0\u6cd5\u4ea7\u751f\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\u5206\u5e03\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4f4d\u7f6e\u7f16\u7801\u56fe\u5206\u4f4d\u6570\u795e\u7ecf\u7f51\u7edc\uff08PE-GQNN\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06PE-GNNs\u3001\u5206\u4f4d\u6570\u795e\u7ecf\u7f51\u7edc\u548c\u91cd\u65b0\u6821\u51c6\u6280\u672f\u6574\u5408\u5728\u4e00\u4e2a\u5b8c\u5168\u975e\u53c2\u6570\u5316\u7684\u6846\u67b6\u4e2d\uff0c\u5bf9\u9884\u6d4b\u5206\u5e03\u7684\u5047\u8bbe\u8981\u6c42\u6781\u4f4e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5f53\u4e0e\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u635f\u5931\u51fd\u6570\u7ed3\u5408\u65f6\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u51c6\u786e\u4e14\u53ef\u9760\u7684\u6982\u7387\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8d85\u8d8a\u7a7a\u95f4\u6570\u636e\u7684\u5e94\u7528\u573a\u666f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\uff0c\u5c06KNN\u9884\u6d4b\u5668\u878d\u5165\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u901a\u8fc7GNN\u5c42\u64cd\u4f5c\u907f\u514d\u4e86\u6570\u636e\u6cc4\u9732\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPE-GQNN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002 | William E. R. de Amorim | PDF | N/A | Positional Encoder Graph Quantile Neural Networks for Geographic Data | Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach for modeling continuous spatial data. However, they often fail to produce calibrated predictive distributions, limiting their effectiveness for uncertainty quantification. We introduce the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel method that integrates PE-GNNs, Quantile Neural Networks, and recalibration techniques in a fully nonparametric framework, requiring minimal assumptions about the predictive distributions. We propose a new network architecture that, when combined with a quantile-based loss function, yields accurate and reliable probabilistic models without increasing computational complexity. Our approach provides a flexible, robust framework for conditional density estimation, applicable beyond spatial data contexts. We further introduce a structured method for incorporating a KNN predictor into the model while avoiding data leakage through the GNN layer operation. Experiments on benchmark datasets demonstrate that PE-GQNN significantly outperforms existing state-of-the-art methods in both predictive accuracy and uncertainty quantification. | | \u751f\u6210\u7ed3\u6784\u591a\u6837\u56fe\u5f62\u7684\u6311\u6218 | \u5bf9\u4e8e\u8bb8\u591a\u4e0e\u56fe\u76f8\u5173\u7684\u95ee\u9898\uff0c\u62e5\u6709\u4e00\u7ec4\u7ed3\u6784\u591a\u6837\u5316\u7684\u56fe\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u8fd9\u4e9b\u56fe\u53ef\u4ee5\u7528\u4e8e\u6d4b\u8bd5\u56fe\u7b97\u6cd5\u6216\u5176\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u3002\u7136\u800c\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5728\u6587\u732e\u4e2d\u5c1a\u672a\u63a2\u8ba8\u751f\u6210\u7ed3\u6784\u591a\u6837\u5316\u56fe\u7684\u95ee\u9898\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u9996\u5148\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5982\u4f55\u5b9a\u4e49\u56fe\u96c6\u7684\u591a\u6837\u6027\uff0c\u4e3a\u4ec0\u4e48\u8fd9\u4e00\u4efb\u52a1\u5e76\u975e\u6613\u4e8b\uff0c\u4ee5\u53ca\u5982\u4f55\u9009\u62e9\u9002\u5f53\u7684\u591a\u6837\u6027\u5ea6\u91cf\u3002\u7136\u540e\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u4f18\u5316\u8be5\u5ea6\u91cf\u7684\u7b97\u6cd5\u5e76\u8fdb\u884c\u6bd4\u8f83\uff1a\u6211\u4eec\u8003\u8651\u4e86\u57fa\u4e8e\u6807\u51c6\u968f\u673a\u56fe\u6a21\u578b\u7684\u65b9\u6cd5\u3001\u5c40\u90e8\u56fe\u4f18\u5316\u3001\u9057\u4f20\u7b97\u6cd5\u548c\u795e\u7ecf\u751f\u6210\u6a21\u578b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u901a\u8fc7\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u56fe\u7684\u591a\u6837\u6027\uff0c\u8d85\u8d8a\u57fa\u672c\u7684\u968f\u673a\u56fe\u751f\u6210\u5668\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u751f\u6210\u56fe\u7684\u5206\u6790\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u56fe\u8ddd\u79bb\u7684\u6027\u8d28\uff1a\u6839\u636e\u7528\u4e8e\u4f18\u5316\u7684\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u6240\u83b7\u5f97\u7684\u56fe\u53ef\u80fd\u5177\u6709\u975e\u5e38\u4e0d\u540c\u7684\u7ed3\u6784\u7279\u6027\uff0c\u8fd9\u4e3a\u591a\u6837\u6027\u5ea6\u91cf\u6240\u57fa\u4e8e\u7684\u56fe\u8ddd\u79bb\u7684\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002 | Fedor Velikonivtsev | PDF | N/A | Challenges of Generating Structurally Diverse Graphs | For many graph-related problems, it can be essential to have a set of structurally diverse graphs. For instance, such graphs can be used for testing graph algorithms or their neural approximations. However, to the best of our knowledge, the problem of generating structurally diverse graphs has not been explored in the literature. In this paper, we fill this gap. First, we discuss how to define diversity for a set of graphs, why this task is non-trivial, and how one can choose a proper diversity measure. Then, for a given diversity measure, we propose and compare several algorithms optimizing it: we consider approaches based on standard random graph models, local graph optimization, genetic algorithms, and neural generative models. We show that it is possible to significantly improve diversity over basic random graph generators. Additionally, our analysis of generated graphs allows us to better understand the properties of graph distances: depending on which diversity measure is used for optimization, the obtained graphs may possess very different structural properties which gives insights about the sensitivity of the graph distance underlying the diversity measure. | | \u4e24\u4e2a\u7a00\u758f\u77e9\u9635\u80dc\u8fc7\u4e00\u4e2a\uff1a\u5229\u7528\u53cc\u91cd\u7a00\u758f\u5206\u89e3\u7a00\u758f\u5316\u795e\u7ecf\u7f51\u7edc | \u795e\u7ecf\u7f51\u7edc\u7531\u4e8e\u5176\u5e9e\u5927\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\uff0c\u5e38\u5e38\u96be\u4ee5\u5904\u7406\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5404\u79cd\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u5316\u6216\u5206\u89e3\u6743\u91cd\u77e9\u9635\u6765\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u4f8b\u5982\u5e45\u5ea6\u526a\u679d\u548c\u4f4e\u79e9\u6216\u5757\u5bf9\u89d2\u5206\u89e3\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53cc\u91cd\u7a00\u758f\u5206\u89e3\uff08Double Sparse Factorization, DSF\uff09\uff0c\u5c06\u6bcf\u4e2a\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u7a00\u758f\u77e9\u9635\u3002\u5c3d\u7ba1\u7cbe\u786e\u6c42\u89e3\u6b64\u95ee\u9898\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u6700\u5c0f\u5316\u4e0eADMM\u7684\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5b9e\u73b0\u4e86\u795e\u7ecf\u7f51\u7edc\u524d\u6240\u672a\u6709\u7684\u7a00\u758f\u5316\u3002\u4f8b\u5982\uff0c\u5728\u4e00\u6b21\u6027\u526a\u679d\u8bbe\u7f6e\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06LLaMA2-13B\u6a21\u578b\u7684\u5c3a\u5bf8\u51cf\u5c1150%\uff0c\u540c\u65f6\u4fdd\u6301\u6bd4\u5bc6\u96c6\u7684LLaMA2-7B\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u4e0e\u6700\u5148\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u9010\u5c42\u526a\u679d\u65b9\u6cd5\u2014\u2014\u6700\u4f18\u8111\u538b\u7f29\uff08Optimal Brain Compression\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u8fdb\u4e00\u6b65\u6a21\u578b\u5fae\u8c03\u540e\uff0c\u6211\u4eec\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u6539\u8fdb\u4f9d\u7136\u6301\u7eed\u3002\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1ahttps://github.com/usamec/double_sparse\u3002 | Vladim\u00edr Bo\u017ea | PDF | N/A | Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization | Neural networks are often challenging to work with due to their large size and complexity. To address this, various methods aim to reduce model size by sparsifying or decomposing weight matrices, such as magnitude pruning and low-rank or block-diagonal factorization. In this work, we present Double Sparse Factorization (DSF), where we factorize each weight matrix into two sparse matrices. Although solving this problem exactly is computationally infeasible, we propose an efficient heuristic based on alternating minimization via ADMM that achieves state-of-the-art results, enabling unprecedented sparsification of neural networks. For instance, in a one-shot pruning setting, our method can reduce the size of the LLaMA2-13B model by 50% while maintaining better performance than the dense LLaMA2-7B model. We also compare favorably with Optimal Brain Compression, the state-of-the-art layer-wise pruning approach for convolutional neural networks. Furthermore, accuracy improvements of our method persist even after further model fine-tuning.   Code available at: https://github.com/usamec/double_sparse. | | \u7ecf\u5178\u7edf\u8ba1\uff08\u6837\u672c\u5185\uff09\u76f4\u89c9\u4e0d\u9002\u7528\uff1a\u5173\u4e8e\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3001\u8fc7\u62df\u5408\u4ee5\u53ca\u4ece\u56fa\u5b9a\u8bbe\u8ba1\u5230\u968f\u673a\u8bbe\u8ba1\u7684\u8f6c\u53d8\u7684\u8bf4\u660e | \u73b0\u4ee3\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u73b0\u8c61\u7684\u7a81\u7136\u51fa\u73b0\uff0c\u5982\u53cc\u91cd\u4e0b\u964d\u548c\u826f\u6027\u8fc7\u62df\u5408\uff0c\u53ef\u80fd\u4f1a\u8ba9\u8bb8\u591a\u7ecf\u5178\u8bad\u7ec3\u7684\u7edf\u8ba1\u5b66\u5bb6\u611f\u5230\u4e0d\u5b89\u2014\u2014\u8fd9\u4e9b\u73b0\u8c61\u4f3c\u4e4e\u4e0e\u4efb\u4f55\u6570\u636e\u5b66\u4e60\u5165\u95e8\u8bfe\u7a0b\u4e2d\u4f20\u8fbe\u7684\u7edf\u8ba1\u76f4\u89c9\u7684\u6838\u5fc3\u76f8\u6096\u3002\u5386\u53f2\u4e0a\u5bf9\u8fd9\u4e9b\u73b0\u8c61\u7684\u65e9\u671f\u89c2\u5bdf\u7f3a\u4e4f\u901a\u5e38\u5f52\u56e0\u4e8e\u5f53\u4eca\u5bf9\u66f4\u590d\u6742\u7684ML\u65b9\u6cd5\u3001\u8fc7\u5ea6\u53c2\u6570\u5316\u3001\u63d2\u503c\u548c/\u6216\u66f4\u9ad8\u6570\u636e\u7ef4\u5ea6\u7684\u4f9d\u8d56\u3002\u5728\u8fd9\u7bc7\u7b14\u8bb0\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u4f46\u5f88\u5c11\u660e\u786e\u8ba8\u8bba\u7684\u539f\u56e0\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u6211\u4eec\u4eca\u5929\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u4f3c\u4e4e\u4e0e\u7ecf\u5178\u7edf\u8ba1\u5b66\u6559\u79d1\u4e66\u4e2d\u6559\u6388\u7684\u76f4\u89c9\u76f8\u6096\u3002\u7279\u522b\u662f\uff0c\u8bb8\u591a\u76f4\u89c9\u6e90\u4e8e\u56fa\u5b9a\u8bbe\u8ba1\u8bbe\u7f6e\uff0c\u5176\u4e2d\u5bf9\u6837\u672c\u5185\u9884\u6d4b\u8bef\u5dee\uff08\u5728\u566a\u58f0\u7ed3\u679c\u7684\u91cd\u91c7\u6837\u4e0b\uff09\u611f\u5174\u8da3\uff0c\u800c\u73b0\u4ee3ML\u5219\u6839\u636e\u6cdb\u5316\u8bef\u5dee\uff0c\u5373\u968f\u673a\u8bbe\u8ba1\u4e2d\u7684\u6837\u672c\u5916\u9884\u6d4b\u8bef\u5dee\u6765\u8bc4\u4f30\u5176\u9884\u6d4b\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5f3a\u8c03\u4ece\u56fa\u5b9a\u8bbe\u8ba1\u5230\u968f\u673a\u8bbe\u8ba1\u7684\u8fd9\u4e00\u7b80\u5355\u8f6c\u53d8\uff08\u4e5f\u8bb8\u4ee4\u4eba\u60ca\u8bb6\u5730\uff09\u5bf9\u4e0e\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u76f8\u5173\u7684\u6559\u79d1\u4e66\u76f4\u89c9\u4ea7\u751f\u4e86\u6df1\u8fdc\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u8bba\u4e86\u5728\u56fa\u5b9a\u8bbe\u8ba1\u4e0e\u968f\u673a\u8bbe\u8ba1\u4e2d\u89c2\u5bdf\u53cc\u91cd\u4e0b\u964d\u548c\u826f\u6027\u8fc7\u62df\u5408\u7684\uff08\u4e0d\uff09\u53ef\u80fd\u6027\u3002 | Alicia Curth | PDF | N/A | Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs | The sudden appearance of modern machine learning (ML) phenomena like double descent and benign overfitting may leave many classically trained statisticians feeling uneasy -- these phenomena appear to go against the very core of statistical intuitions conveyed in any introductory class on learning from data. The historical lack of earlier observation of such phenomena is usually attributed to today's reliance on more complex ML methods, overparameterization, interpolation and/or higher data dimensionality. In this note, we show that there is another reason why we observe behaviors today that appear at odds with intuitions taught in classical statistics textbooks, which is much simpler to understand yet rarely discussed explicitly. In particular, many intuitions originate in fixed design settings, in which in-sample prediction error (under resampling of noisy outcomes) is of interest, while modern ML evaluates its predictions in terms of generalization error, i.e. out-of-sample prediction error in random designs. Here, we highlight that this simple move from fixed to random designs has (perhaps surprisingly) far-reaching consequences on textbook intuitions relating to the bias-variance tradeoff, and comment on the resulting (im)possibility of observing double descent and benign overfitting in fixed versus random designs. | | RNC\uff1a\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u7684DNN\u9ad8\u6548RRAM\u611f\u77e5NAS\u4e0e\u7f16\u8bd1 | \u8ba1\u7b97\u5185\u5b58\uff08Computing-in-memory, CIM\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\uff0c\u5b83\u5728\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u7684\u9ad8\u5e76\u884c\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u786c\u4ef6\u67b6\u6784\u4e0e\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u672a\u5145\u5206\u8003\u8651\u8d44\u6e90\u9650\u5236\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u65e8\u5728\u4e3a\u57fa\u4e8e\u7535\u963b\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08RRAM\uff09\u7684\u52a0\u901f\u5668\u5f00\u53d1\u8fb9\u7f18\u53cb\u597d\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18\u7f16\u8bd1\u4e0e\u8d44\u6e90\u53d7\u9650\u7684RRAM\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u6846\u67b6\uff0c\u4ee5\u641c\u7d22\u6ee1\u8db3\u7279\u5b9a\u786c\u4ef6\u7ea6\u675f\u7684\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u7684\u7f16\u8bd1\u65b9\u6cd5\u6574\u5408\u4e86\u5c42\u5212\u5206\u3001\u590d\u5236\u548c\u7f51\u7edc\u6253\u5305\uff0c\u4ee5\u6700\u5927\u5316\u8ba1\u7b97\u5355\u5143\u7684\u5229\u7528\u7387\u3002\u901a\u8fc7\u91c7\u7528\u4e00\u6b21\u6027\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7ed3\u5408\u975e\u652f\u914d\u6392\u5e8f\u9057\u4f20\u7b97\u6cd5II\uff08NSGA-II\uff09\u5b9e\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\uff0c\u6240\u5f97\u7f51\u7edc\u67b6\u6784\u53ef\u9488\u5bf9\u9ad8\u7cbe\u5ea6\u6216\u4f4e\u5ef6\u8fdf\u8fdb\u884c\u4f18\u5316\u3002\u7f16\u8bd1\u79fb\u52a8\u53cb\u597d\u578b\u7f51\u7edc\uff0c\u5982Squeezenet\u548cMobilenetV3 small\uff0c\u53ef\u5b9e\u73b0\u8d85\u8fc780%\u7684\u5229\u7528\u7387\uff0c\u76f8\u6bd4ISAAC\u7c7b\u6846\u67b6\u5728\u4e0d\u540c\u4ea4\u53c9\u8d44\u6e90\u4e0b\u5b9e\u73b0\u8d85\u8fc76\u500d\u7684\u52a0\u901f\u3002\u901a\u8fc7NAS\u4f18\u5316\u901f\u5ea6\u5f97\u5230\u7684\u6a21\u578b\u5b9e\u73b0\u4e865\u81f330\u500d\u7684\u52a0\u901f\u3002\u672c\u6587\u4ee3\u7801\u53ef\u5728https://github.com/ArChiiii/rram_nas_comp_pack\u83b7\u53d6\u3002 | Kam Chi Loong | PDF | N/A | RNC: Efficient RRAM-aware NAS and Compilation for DNNs on Resource-Constrained Edge Devices | Computing-in-memory (CIM) is an emerging computing paradigm, offering noteworthy potential for accelerating neural networks with high parallelism, low latency, and energy efficiency compared to conventional von Neumann architectures. However, existing research has primarily focused on hardware architecture and network co-design for large-scale neural networks, without considering resource constraints. In this study, we aim to develop edge-friendly deep neural networks (DNNs) for accelerators based on resistive random-access memory (RRAM). To achieve this, we propose an edge compilation and resource-constrained RRAM-aware neural architecture search (NAS) framework to search for optimized neural networks meeting specific hardware constraints. Our compilation approach integrates layer partitioning, duplication, and network packing to maximize the utilization of computation units. The resulting network architecture can be optimized for either high accuracy or low latency using a one-shot neural network approach with Pareto optimality achieved through the Non-dominated Sorted Genetic Algorithm II (NSGA-II). The compilation of mobile-friendly networks, like Squeezenet and MobilenetV3 small can achieve over 80% of utilization and over 6x speedup compared to ISAAC-like framework with different crossbar resources. The resulting model from NAS optimized for speed achieved 5x-30x speedup. The code for this paper is available at https://github.com/ArChiiii/rram_nas_comp_pack. | | \u6784\u5efa\u201c\u6cdb\u5316\u8bef\u5dee\u201d\u7684\u7f6e\u4fe1\u533a\u95f4\u2014\u2014\u4e00\u9879\u7efc\u5408\u57fa\u51c6\u7814\u7a76 | \u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7684\u8d28\u91cf\u65f6\uff0c\u7528\u4e8e\u8861\u91cf\u9884\u6d4b\u6027\u80fd\u7684\u6cdb\u5316\u8bef\u5dee\u7684\u7f6e\u4fe1\u533a\u95f4\uff08CIs\uff09\u662f\u4e00\u4e2a\u5173\u952e\u5de5\u5177\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u5b58\u5728\u8bb8\u591a\u8ba1\u7b97\u6b64\u7c7bCIs\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0d\u65ad\u6709\u65b0\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u88ab\u63d0\u51fa\u3002\u901a\u5e38\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ed3\u5408\u4e86\u5404\u79cd\u91cd\u91c7\u6837\u7a0b\u5e8f\uff0c\u5176\u4e2d\u6700\u6d41\u884c\u7684\u662f\u4ea4\u53c9\u9a8c\u8bc1\u548c\u81ea\u4e3e\u6cd5\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u65b9\u5dee\u4f30\u8ba1\u6280\u672f\u3002\u7136\u800c\uff0c\u9057\u61be\u7684\u662f\uff0c\u76ee\u524d\u5c1a\u65e0\u5171\u8bc6\u4f55\u65f6\u4f7f\u7528\u8fd9\u4e9b\u7ec4\u5408\u6700\u4e3a\u53ef\u9760\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u4e00\u822c\u6bd4\u8f83\u60c5\u51b5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u9996\u6b21\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u6cdb\u5316\u8bef\u5dee\u7684CIs\u2014\u2014\u5728\u603b\u517118\u4e2a\u8868\u683c\u56de\u5f52\u548c\u5206\u7c7b\u95ee\u9898\u4e0a\uff0c\u4f7f\u7528\u56db\u79cd\u4e0d\u540c\u7684\u5f52\u7eb3\u5668\u548c\u603b\u5171\u516b\u79cd\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u4e8613\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u6982\u8ff0\u4e86\u6784\u5efa\u6cdb\u5316\u8bef\u5deeCIs\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u548c\u5185\u5728\u6311\u6218\uff0c\u5e76\u5728\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u63d0\u4f9b\u4e86\u6240\u670913\u79cd\u65b9\u6cd5\u7684\u7b80\u8981\u56de\u987e\u3002\u6700\u540e\uff0c\u6839\u636e\u76f8\u5bf9\u8986\u76d6\u9891\u7387\u3001\u5bbd\u5ea6\u3001\u548c\u8fd0\u884c\u65f6\u95f4\u8bc4\u4f30\u4e86CI\u65b9\u6cd5\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u80fd\u591f\u786e\u5b9a\u4e00\u7ec4\u63a8\u8350\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u5c06\u6570\u636e\u96c6\u4f5c\u4e3a\u57fa\u51c6\u5957\u4ef6\u53d1\u5e03\u5728OpenML\u4e0a\uff0c\u5e76\u5728GitHub\u4e0a\u53d1\u5e03\u4e86\u6211\u4eec\u7684\u4ee3\u7801\uff0c\u4ee5\u4f5c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u57fa\u7840\u3002 | Hannah Schulz-K\u00fcmpel | PDF | N/A | Constructing Confidence Intervals for 'the' Generalization Error -- a Comprehensive Benchmark Study | When assessing the quality of prediction models in machine learning, confidence intervals (CIs) for the generalization error, which measures predictive performance, are a crucial tool. Luckily, there exist many methods for computing such CIs and new promising approaches are continuously being proposed. Typically, these methods combine various resampling procedures, most popular among them cross-validation and bootstrapping, with different variance estimation techniques. Unfortunately, however, there is currently no consensus on when any of these combinations may be most reliably employed and how they generally compare. In this work, we conduct the first large-scale study comparing CIs for the generalization error - empirically evaluating 13 different methods on a total of 18 tabular regression and classification problems, using four different inducers and a total of eight loss functions. We give an overview of the methodological foundations and inherent challenges of constructing CIs for the generalization error and provide a concise review of all 13 methods in a unified framework. Finally, the CI methods are evaluated in terms of their relative coverage frequency, width, and runtime. Based on these findings, we are able to identify a subset of methods that we would recommend. We also publish the datasets as a benchmarking suite on OpenML and our code on GitHub to serve as a basis for further studies. | | \u901a\u8fc72D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u6e32\u67d3\u4e3a\u56fe\u50cf\u7684\u8f68\u8ff9\u8fdb\u884c\u5206\u7c7b\u548c\u56de\u5f52 | \u8f68\u8ff9\u53ef\u4ee5\u88ab\u89c6\u4e3a\u5750\u6807\u7684\u65f6\u5e8f\u6570\u636e\uff0c\u901a\u5e38\u6765\u6e90\u4e8e\u8fd0\u52a8\u7269\u4f53\u3002\u8f68\u8ff9\u5206\u7c7b\u65b9\u6cd5\u5bf9\u4e8e\u68c0\u6d4b\u4e0d\u540c\u7684\u8fd0\u52a8\u6a21\u5f0f\u5c24\u4e3a\u91cd\u8981\uff0c\u800c\u56de\u5f52\u65b9\u6cd5\u5219\u7528\u4e8e\u8ba1\u7b97\u8fd0\u52a8\u6307\u6807\u548c\u9884\u6d4b\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6700\u65b0\u8fdb\u5c55\u4fc3\u8fdb\u4e86\u5c06\u8f68\u8ff9\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u901a\u8fc7\u5177\u6709\u4e8c\u7ef4\u5377\u79ef\u5c42\uff08CNNs\uff09\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u4e86CNNs\u4ece\u56fe\u50cf\u4e2d\u5b66\u4e60\u7279\u5f81\u7a7a\u95f4\u5c42\u6b21\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u8bc6\u522b\u590d\u6742\u5f62\u72b6\u662f\u5fc5\u8981\u7684\u3002\u6b64\u5916\uff0c\u5b83\u514b\u670d\u4e86\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u56fa\u5b9a\u6570\u91cf\u70b9\u8f93\u5165\u8f68\u8ff9\u7684\u9650\u5236\u3002\u7136\u800c\uff0c\u5c06\u8f68\u8ff9\u6e32\u67d3\u4e3a\u56fe\u50cf\u53ef\u80fd\u4f1a\u5f15\u5165\u4e00\u4e9b\u672a\u5145\u5206\u7814\u7a76\u7684\u4eba\u4e3a\u56e0\u7d20\uff0c\u5982\u7531\u4e8e\u5728\u79bb\u6563\u7f51\u683c\u4e0a\u7ed8\u5236\u5750\u6807\u800c\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\uff0c\u4ee5\u53ca\u7531\u4e8e\u7ebf\u6761\u7c97\u7ec6\u548c\u8d70\u6837\u5f15\u8d77\u7684\u9891\u8c31\u53d8\u5316\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86CNNs\u5728\u89e3\u51b3\u4ece\u4e0d\u540c\u6a21\u5f0f\u6e32\u67d3\u4e3a\u56fe\u50cf\u7684\u5408\u6210\u8f68\u8ff9\u7684\u5206\u7c7b\u548c\u56de\u5f52\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002\u672c\u7814\u7a76\u8003\u8651\u7684\u53c2\u6570\u5305\u62ec\u7ebf\u6761\u7c97\u7ec6\u3001\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u4f7f\u7528\u8fd0\u52a8\u5386\u53f2\uff08\u65f6\u95f4\u6210\u5206\u7684\u989c\u8272\u7f16\u7801\uff09\u548c\u6297\u952f\u9f7f\u5904\u7406\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u8fd0\u52a8\u65b9\u5411\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u4e2d\uff0c\u6839\u636e\u6a21\u578b\u6df1\u5ea6\u548c\u8fd0\u52a8\u5386\u53f2\u9009\u62e9\u9002\u5f53\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u91cd\u8981\u6027\u3002 | Mariaclaudia Nicolai | PDF | N/A | Classification and regression of trajectories rendered as images via 2D Convolutional Neural Networks | Trajectories can be regarded as time-series of coordinates, typically arising from motile objects. Methods for trajectory classification are particularly important to detect different movement patterns, while methods for regression to compute motility metrics and forecasting. Recent advances in computer vision have facilitated the processing of trajectories rendered as images via artificial neural networks with 2d convolutional layers (CNNs). This approach leverages the capability of CNNs to learn spatial hierarchies of features from images, necessary to recognize complex shapes. Moreover, it overcomes the limitation of other machine learning methods that require input trajectories with a fixed number of points. However, rendering trajectories as images can introduce poorly investigated artifacts such as information loss due to the plotting of coordinates on a discrete grid, and spectral changes due to line thickness and aliasing. In this study, we investigate the effectiveness of CNNs for solving classification and regression problems from synthetic trajectories that have been rendered as images using different modalities. The parameters considered in this study include line thickness, image resolution, usage of motion history (color-coding of the temporal component) and anti-aliasing. Results highlight the importance of choosing an appropriate image resolution according to model depth and motion history in applications where movement direction is critical. | | ARLBench\uff1a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u4f18\u5316\u7684\u7075\u6d3b\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177 | \u8d85\u53c2\u6570\u662f\u53ef\u9760\u8bad\u7ec3\u51fa\u8868\u73b0\u4f18\u5f02\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u7684\u5173\u952e\u56e0\u7d20\u3002\u7136\u800c\uff0c\u5f00\u53d1\u548c\u8bc4\u4f30\u7528\u4e8e\u8c03\u6574\u8fd9\u4e9b\u8d85\u53c2\u6570\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u65e2\u8017\u8d39\u6210\u672c\u53c8\u8017\u8d39\u65f6\u95f4\u3002\u56e0\u6b64\uff0c\u8fd9\u7c7b\u65b9\u6cd5\u901a\u5e38\u4ec5\u5728\u4e00\u4e2a\u9886\u57df\u6216\u7b97\u6cd5\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4f7f\u5f97\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\uff0c\u5e76\u9650\u5236\u4e86\u5bf9\u5b83\u4eec\u901a\u7528\u6027\u7684\u6df1\u5165\u4e86\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86ARLBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8eRL\u4e2d\u8d85\u53c2\u6570\u4f18\u5316\uff08HPO\uff09\u7684\u57fa\u51c6\uff0c\u5b83\u5141\u8bb8\u5bf9\u591a\u79cdHPO\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u5728\u8bc4\u4f30\u4e2d\u4fdd\u6301\u9ad8\u6548\u3002\u4e3a\u4e86\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6761\u4ef6\u4e0b\u4e5f\u80fd\u63a8\u52a8RL\u4e2d\u7684HPO\u7814\u7a76\uff0c\u6211\u4eec\u9009\u62e9\u4e86\u4e00\u7ec4\u6db5\u76d6\u591a\u79cd\u7b97\u6cd5\u548c\u73af\u5883\u7ec4\u5408\u7684\u4ee3\u8868\u6027HPO\u4efb\u52a1\u5b50\u96c6\u3002\u8fd9\u4e00\u9009\u62e9\u4f7f\u5f97\u4ec5\u4f7f\u7528\u4e4b\u524d\u6240\u9700\u8ba1\u7b97\u8d44\u6e90\u7684\u4e00\u5c0f\u90e8\u5206\u5c31\u80fd\u751f\u6210\u81ea\u52a8\u5316RL\uff08AutoRL\uff09\u65b9\u6cd5\u7684\u6027\u80fd\u6982\u51b5\uff0c\u4ece\u800c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4ece\u4e8bRL\u4e2d\u7684HPO\u5de5\u4f5c\u3002\u57fa\u4e8e\u6211\u4eec\u9009\u62e9\u7684\u5e7f\u6cdb\u4e14\u5927\u89c4\u6a21\u7684\u8d85\u53c2\u6570\u666f\u89c2\u6570\u636e\u96c6\uff0cARLBench\u4e3aAutoRL\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u9762\u5411\u672a\u6765\u7684\u57fa\u7840\u3002\u57fa\u51c6\u548c\u6570\u636e\u96c6\u5747\u53ef\u901a\u8fc7https://github.com/automl/arlbench\u83b7\u53d6\u3002 | Jannis Becktepe | PDF | N/A | ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning | Hyperparameters are a critical factor in reliably training well-performing reinforcement learning (RL) agents. Unfortunately, developing and evaluating automated approaches for tuning such hyperparameters is both costly and time-consuming. As a result, such approaches are often only evaluated on a single domain or algorithm, making comparisons difficult and limiting insights into their generalizability. We propose ARLBench, a benchmark for hyperparameter optimization (HPO) in RL that allows comparisons of diverse HPO approaches while being highly efficient in evaluation. To enable research into HPO in RL, even in settings with low compute resources, we select a representative subset of HPO tasks spanning a variety of algorithm and environment combinations. This selection allows for generating a performance profile of an automated RL (AutoRL) method using only a fraction of the compute previously necessary, enabling a broader range of researchers to work on HPO in RL. With the extensive and large-scale dataset on hyperparameter landscapes that our selection is based on, ARLBench is an efficient, flexible, and future-oriented foundation for research on AutoRL. Both the benchmark and the dataset are available at https://github.com/automl/arlbench. | | \u745e\u58eb\u5bb6\u5ead\u62a4\u7406\u4e2d\u7684\u672c\u5730\u8f6c\u5f55\u6a21\u578b\uff1a\u4e00\u9879\u8de8\u5b66\u79d1\u6848\u4f8b\u7814\u7a76 | \u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u4e0d\u540c\u9886\u57df\uff0c\u5305\u62ec\u533b\u7597\u884c\u4e1a\uff0c\u5f00\u542f\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002\u7279\u522b\u662f\uff0c\u8f6c\u5f55\u6280\u672f\u53ef\u4ee5\u7528\u4e8e\u652f\u6301\u62a4\u7406\u6587\u6863\u8fc7\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u4ece\u800c\u4e3a\u62a4\u58eb\u817e\u51fa\u66f4\u591a\u65f6\u95f4\u4e0e\u60a3\u8005\u4e92\u52a8\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\uff08a\uff09\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\uff08b\uff09\u5730\u65b9\u8bed\u8a00\u548c\u65b9\u8a00\uff0c\u4ee5\u53ca\uff08c\uff09\u7279\u5b9a\u9886\u57df\u7684\u4e13\u4e1a\u8bcd\u6c47\u3002\u5728\u672c\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u745e\u58eb\u5bb6\u5ead\u62a4\u7406\u6587\u6863\u7684\u8f6c\u5f55\u60c5\u51b5\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u8f6c\u5f55\u5de5\u5177\u548c\u6a21\u578b\uff0c\u5e76\u4f7f\u7528OpenAI\u7684Whisper\u8fdb\u884c\u4e86\u591a\u6b21\u5b9e\u9a8c\uff0c\u6d89\u53ca\u5fb7\u8bed\u7684\u4e0d\u540c\u53d8\u4f53\uff08\u5373\u65b9\u8a00\u3001\u5916\u56fd\u53e3\u97f3\uff09\u4ee5\u53ca\u7531\u5bb6\u5ead\u62a4\u7406\u9886\u57df\u4e13\u5bb6\u624b\u5de5\u7cbe\u9009\u7684\u793a\u4f8b\u6587\u672c\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4fbf\u4f7f\u7528\u73b0\u6210\u7684\u6a21\u578b\uff0c\u5176\u8868\u73b0\u4e5f\u8db3\u4ee5\u6210\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u826f\u597d\u8d77\u70b9\u3002 | Jeremy Kramer | PDF | N/A | Local Transcription Models in Home Care Nursing in Switzerland: an Interdisciplinary Case Study | Latest advances in the field of natural language processing (NLP) enable new use cases for different domains, including the medical sector. In particular, transcription can be used to support automation in the nursing documentation process and give nurses more time to interact with the patients. However, different challenges including (a) data privacy, (b) local languages and dialects, and (c) domain-specific vocabulary need to be addressed. In this case study, we investigate the case of home care nursing documentation in Switzerland. We assessed different transcription tools and models, and conducted several experiments with OpenAI Whisper, involving different variations of German (i.e., dialects, foreign accent) and manually curated example texts by a domain expert of home care nursing. Our results indicate that even the used out-of-the-box model performs sufficiently well to be a good starting point for future research in the field. | | \u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4eceMRI\u56fe\u50cf\u4e2d\u65e9\u671f\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5 | \u4f17\u6240\u5468\u77e5\uff0c\u5168\u7403\u6700\u5e38\u89c1\u7684\u75f4\u5446\u75c7\u539f\u56e0\u662f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u3002\u8fd9\u79cd\u75c5\u75c7\u7684\u4e25\u91cd\u7a0b\u5ea6\u4ece\u8f7b\u5fae\u5230\u4e25\u91cd\u9010\u6e10\u53d1\u5c55\uff0c\u5e76\u5e72\u6270\u4eba\u4eec\u7684\u65e5\u5e38\u6d3b\u52a8\u3002\u65e9\u671f\u8bca\u65ad\u5728\u60a3\u8005\u62a4\u7406\u548c\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u88ab\u7528\u6765\u521b\u5efa\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4eceMRI\u626b\u63cf\u4e2d\u8bc6\u522b\u7279\u5b9a\u75be\u75c5\u7279\u5f81\u3002\u75f4\u5446\u75c7\u7684\u5206\u7c7b\u6d89\u53ca\u591a\u79cd\u65b9\u6cd5\uff0c\u5982\u533b\u5b66\u5386\u53f2\u56de\u987e\u3001\u795e\u7ecf\u5fc3\u7406\u5b66\u6d4b\u8bd5\u548c\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u3002\u7136\u800c\uff0c\u4eceKaggle\u83b7\u5f97\u7684\u56fe\u50cf\u6570\u636e\u96c6\u9762\u4e34\u4e00\u4e2a\u663e\u8457\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8fd9\u9700\u8981\u6bcf\u4e2a\u7c7b\u522b\u7684\u6837\u672c\u5747\u5300\u5206\u5e03\u6765\u89e3\u51b3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u4e0d\u5e73\u8861\uff0c\u91c7\u7528\u4e86\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f\uff08SMOTE\uff09\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u88ab\u5e94\u7528\u4e8eDEMNET\u75f4\u5446\u7f51\u7edc\uff0c\u4ee5\u4eceAD\u56fe\u50cf\u4e2d\u63d0\u53d6\u5173\u952e\u7279\u5f81\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8fbe\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u768498.67%\u7684\u51c6\u786e\u7387\u3002 | Sajjad Aghasi Javid | PDF | N/A | Early diagnosis of Alzheimer's disease from MRI images with deep learning model | It is acknowledged that the most common cause of dementia worldwide is Alzheimer's disease (AD). This condition progresses in severity from mild to severe and interferes with people's everyday routines. Early diagnosis plays a critical role in patient care and clinical trials. Convolutional neural networks (CNN) are used to create a framework for identifying specific disease features from MRI scans Classification of dementia involves approaches such as medical history review, neuropsychological tests, and magnetic resonance imaging (MRI). However, the image dataset obtained from Kaggle faces a significant issue of class imbalance, which requires equal distribution of samples from each class to address. In this article, to address this imbalance, the Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore, a pre-trained convolutional neural network has been applied to the DEMNET dementia network to extract key features from AD images. The proposed model achieved an impressive accuracy of 98.67%. | | LLMs4Synthesis\uff1a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u79d1\u5b66\u7efc\u5408 | \u9488\u5bf9\u79d1\u5b66\u6587\u732e\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u548c\u6570\u91cf\uff0c\u672c\u6587\u63d0\u51fa\u4e86LLMs4Synthesis\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u79d1\u5b66\u7efc\u5408\u62a5\u544a\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u6ee1\u8db3\u4e86\u5feb\u901f\u3001\u8fde\u8d2f\u4e14\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u79d1\u5b66\u89c1\u89e3\u6574\u5408\u9700\u6c42\uff0c\u540c\u65f6\u5229\u7528\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u7684LLMs\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63a2\u8ba8\u4e86LLMs\u5728\u8bc4\u4f30\u8fd9\u4e9b\u7efc\u5408\u62a5\u544a\u7684\u5b8c\u6574\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7f13\u89e3\u4e86\u5f53\u524d\u5b9a\u91cf\u6307\u6807\u7684\u4e0d\u8db3\u3002\u6211\u4eec\u7684\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u5904\u7406\u79d1\u5b66\u8bba\u6587\u7684\u65b0\u65b9\u6cd5\u3001\u5b9a\u4e49\u65b0\u7684\u7efc\u5408\u7c7b\u578b\u4ee5\u53ca\u5efa\u7acb\u4e5d\u9879\u8be6\u7ec6\u7684\u8d28\u91cf\u8bc4\u4ef7\u6807\u51c6\uff0c\u4e3a\u8be5\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002\u6211\u4eec\u63d0\u51fa\u5c06LLMs\u4e0e\u5f3a\u5316\u5b66\u4e60\u548cAI\u53cd\u9988\u76f8\u7ed3\u5408\uff0c\u4ee5\u4f18\u5316\u7efc\u5408\u62a5\u544a\u7684\u8d28\u91cf\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u65e2\u5b9a\u6807\u51c6\u3002LLMs4Synthesis\u6846\u67b6\u53ca\u5176\u7ec4\u4ef6\u7684\u5f00\u653e\u4f7f\u7528\uff0c\u6709\u671b\u63d0\u5347\u79d1\u5b66\u7814\u7a76\u7efc\u5408\u8fc7\u7a0b\u4e2d\u7684\u751f\u6210\u548c\u8bc4\u4f30\u6548\u7387\u3002 | Hamed Babaei Giglou | PDF | N/A | LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis | In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses. This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria. The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis. | | \u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u6839\u636e\u6d41\u5f62\u5047\u8bbe\u7684\u6269\u6563\u6a21\u578b\u7684\u6536\u655b\u6027 | \u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08Denoising Diffusion Probabilistic Models, DDPM\uff09\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u751f\u6210\u4ee5\u53ca\u79d1\u5b66\u548c\u5176\u4ed6\u9886\u57df\u7684\u8bb8\u591a\u5176\u4ed6\u5e94\u7528\u3002\u6d41\u5f62\u5047\u8bbe\u6307\u51fa\uff0c\u9ad8\u7ef4\u6570\u636e\u901a\u5e38\u4f4d\u4e8e\u5468\u56f4\u7a7a\u95f4\u4e2d\u7684\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u5e76\u4e14\u5728\u63d0\u4f9b\u7684\u793a\u4f8b\u4e2d\u5e7f\u6cdb\u8ba4\u4e3a\u8fd9\u4e00\u5047\u8bbe\u6210\u7acb\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u6269\u6563\u6a21\u578b\u5982\u4f55\u9002\u5e94\u6d41\u5f62\u5047\u8bbe\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u4f46\u5b83\u4eec\u5e76\u672a\u6355\u6349\u5230\u8fd9\u4e9b\u6a21\u578b\u7684\u5de8\u5927\u5b9e\u9645\u6210\u529f\uff0c\u8fd9\u4f7f\u5f97\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u975e\u5e38\u5bcc\u6709\u6210\u679c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5728\u6d41\u5f62\u5047\u8bbe\u4e0b\u7814\u7a76\u4e86DDPM\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u5b66\u4e60\u5f97\u5206\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e\u5468\u56f4\u7ef4\u5ea6\u65e0\u5173\u7684\u901f\u7387\u3002\u5728\u91c7\u6837\u65b9\u9762\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5173\u4e8eKullback-Leibler\u6563\u5ea6\u4e0e\u5468\u56f4\u7ef4\u5ea6\u65e0\u5173\u7684\u901f\u7387\uff0c\u4ee5\u53ca\u5173\u4e8eWasserstein\u8ddd\u79bb\u7684$O(\\sqrt{D})$\u901f\u7387\u3002\u6211\u4eec\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u65b0\u6846\u67b6\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u8be5\u6846\u67b6\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u7ecf\u8fc7\u5145\u5206\u7814\u7a76\u7684\u5173\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u6781\u503c\u7684\u7406\u8bba\u8054\u7cfb\u8d77\u6765\u3002 | Iskander Azangulov | PDF | N/A | Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions | Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art methods used to generate synthetic data from high-dimensional data distributions and are widely used for image, audio and video generation as well as many more applications in science and beyond. The manifold hypothesis states that high-dimensional data often lie on lower-dimensional manifolds within the ambient space, and is widely believed to hold in provided examples. While recent results has provided invaluable insight into how diffusion models adapt to the manifold hypothesis, they do not capture the great empirical success of these models, making this a very fruitful research direction.   In this work, we study DDPMs under the manifold hypothesis and prove that they achieve rates independent of the ambient dimension in terms of learning the score. In terms of sampling, we obtain rates independent of the ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\\sqrt{D})$ w.r.t. the Wasserstein distance. We do this by developing a new framework connecting diffusion models to the well-studied theory of extrema of Gaussian Processes. | | \u7535\u5b50\u7ade\u6280\u4f5c\u4e3a\u5956\u724c\u9879\u76ee\u4eae\u76f82023\u5e74\u4e9a\u8fd0\u4f1a\uff1a\u5229\u7528BERTopic\u548cGPT-4\u4e3b\u9898\u5fae\u8c03\u63a2\u7d22\u516c\u4f17\u8ba4\u77e5 | \u672c\u7814\u7a76\u91c7\u7528LLM\u589e\u5f3a\u7684BERTopic\u5efa\u6a21\u5206\u6790\uff0c\u8003\u5bdf\u4e862023\u5e74\u4e9a\u8fd0\u4f1a\u671f\u95f4\u516c\u4f17\u5bf9\u7535\u5b50\u7ade\u6280\u7684\u770b\u6cd5\u4ee5\u53ca\u8d5b\u4e8b\u4e2d\u7684\u4ef7\u503c\u5171\u521b\u60c5\u51b5\u3002\u6211\u4eec\u8bc6\u522b\u51fa\u4e94\u4e2a\u4e3b\u8981\u4e3b\u9898\uff0c\u4ee3\u8868\u4e86\u516c\u4f17\u7684\u8ba4\u77e5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e3b\u8981\u5229\u76ca\u76f8\u5173\u8005\u5982\u4f55\u5728\u7535\u5b50\u7ade\u6280\u751f\u6001\u7cfb\u7edf\u5185\u5916\u5171\u521b\u4ef7\u503c\u3002\u5173\u952e\u53d1\u73b0\u5f3a\u8c03\u4e86\u793e\u4ea4\u5a92\u4f53\u8425\u9500\u5728\u5f71\u54cd\u516c\u4f17\u610f\u89c1\u3001\u63a8\u5e7f\u7535\u7ade\u8d5b\u4e8b\u548c\u54c1\u724c\u65b9\u9762\u7684\u6218\u7565\u6027\u8fd0\u7528\uff0c\u7a81\u663e\u4e86\u8d5b\u4e8b\u7269\u6d41\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u4f20\u7edf\u7535\u7ade\u751f\u6001\u7cfb\u7edf\u4e4b\u5916\u7684\u5229\u76ca\u76f8\u5173\u8005\u6240\u8d21\u732e\u7684\u5171\u521b\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u63a8\u52a8\u56fd\u5bb6\u4ee3\u8868\u6027\u548c\u8868\u73b0\u65b9\u9762\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4e86\u5c06\u7535\u5b50\u7ade\u6280\u5408\u6cd5\u5316\u4e3a\u4e00\u9879\u8fd0\u52a8\u7684\u6301\u7eed\u52aa\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e3b\u6d41\u8ba4\u53ef\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7535\u5b50\u7ade\u6280\u4f5c\u4e3a\u5956\u724c\u9879\u76ee\u7684\u7eb3\u5165\u5c55\u793a\u4e86\u66f4\u5e7f\u6cdb\u7684\u63a5\u53d7\u5ea6\uff0c\u5e76\u6709\u52a9\u4e8e\u7f13\u89e3\u516c\u4f17\u7684\u8d1f\u9762\u770b\u6cd5\u3002\u6b64\u5916\uff0c\u975e\u4f20\u7edf\u5229\u76ca\u76f8\u5173\u8005\u7684\u8d21\u732e\u7a81\u663e\u4e86\u8de8\u4e9a\u6587\u5316\u5408\u4f5c\u5728\u7535\u7ade\u4e2d\u7684\u4ef7\u503c\u3002 | Tyreal Yizhou Qian | PDF | N/A | Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public Perceptions with BERTopic and GPT-4 Topic Fine-Tuning | This study examined the public opinions of esports at the 2023 Asian Games and value co-creation during the event using an LLM-enhanced BERTopic modeling analysis. We identified five major themes representing public perceptions, as well as how major stakeholders co-created value within and beyond the esports ecosystem. Key findings highlighted the strategic use of social media marketing to influence public opinion and promote esports events and brands, emphasizing the importance of event logistics and infrastructure. Additionally, the study revealed the co-creation value contributed by stakeholders outside the traditional esports ecosystem, particularly in promoting national representation and performance. Our findings supported the ongoing efforts to legitimize esports as a sport, noting that mainstream recognition remains a challenge. The inclusion of esports as a medal event showcased broader acceptance and helped mitigate negative public perceptions. Moreover, contributions from non-traditional stakeholders underscored the value of cross-subcultural collaborations in esports. | | \u5206\u5c42\u8054\u90a6ADMM | \u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6452\u5f03\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7b97\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7684\u65b0\u578b\u5206\u5c42FL\u6846\u67b6\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684FL\u7b97\u6cd5\uff0c\u5b83\u4eec\u90fd\u5728\u9876\u5c42\u4f7f\u7528ADMM\uff1a\u4e00\u79cd\u5728\u5e95\u5c42\u91c7\u7528ADMM\uff0c\u53e6\u4e00\u79cd\u5219\u4f7f\u7528\u4f20\u7edf\u7684\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u589e\u5f3a\u4e86\u9690\u79c1\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b66\u4e60\u6536\u655b\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5c40\u90e8\u6b65\u9aa4\u6570\u91cf\u975e\u5e38\u6709\u9650\uff0c\u5e95\u5c42\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4e5f\u80fd\u8868\u73b0\u826f\u597d\uff0c\u800c\u53cc\u5c42\u4f7f\u7528ADMM\u5219\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002 | Seyed Mohammad Azimi-Abarghouyi | PDF | N/A | Hierarchical Federated ADMM | In this paper, we depart from the widely-used gradient descent-based hierarchical federated learning (FL) algorithms to develop a novel hierarchical FL framework based on the alternating direction method of multipliers (ADMM). Within this framework, we propose two novel FL algorithms, which both use ADMM in the top layer: one that employs ADMM in the lower layer and another that uses the conventional gradient descent-based approach. The proposed framework enhances privacy, and experiments demonstrate the superiority of the proposed algorithms compared to the conventional algorithms in terms of learning convergence and accuracy. Additionally, gradient descent on the lower layer performs well even if the number of local steps is very limited, while ADMM on both layers lead to better performance otherwise. | | \u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bda\u5b9e\u6027\u7684\u8c03\u67e5 | \u8bda\u5b9e\u662f\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u57fa\u672c\u539f\u5219\uff0c\u8981\u6c42\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u5b83\u4eec\u77e5\u9053\u548c\u4e0d\u77e5\u9053\u7684\u5185\u5bb9\uff0c\u5e76\u80fd\u591f\u5fe0\u5b9e\u5730\u8868\u8fbe\u5176\u77e5\u8bc6\u3002\u5c3d\u7ba1\u524d\u666f\u770b\u597d\uff0c\u5f53\u524d\u7684LLM\u4ecd\u7136\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff0c\u4f8b\u5982\u81ea\u4fe1\u5730\u5448\u73b0\u9519\u8bef\u7b54\u6848\u6216\u672a\u80fd\u8868\u8fbe\u5176\u5df2\u77e5\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5bf9LLM\u8bda\u5b9e\u6027\u7684\u7814\u7a76\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u8bda\u5b9e\u5b9a\u4e49\u7684\u591a\u6837\u6027\u3001\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u77e5\u8bc6\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u5bf9\u76f8\u5173\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u7406\u89e3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4efd\u5173\u4e8eLLM\u8bda\u5b9e\u6027\u7684\u8c03\u67e5\uff0c\u6db5\u76d6\u4e86\u5176\u6f84\u6e05\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u6539\u8fdb\u7b56\u7565\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u65e8\u5728\u6fc0\u53d1\u5bf9\u8be5\u91cd\u8981\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002 | Siheng Li | PDF | N/A | A Survey on the Honesty of Large Language Models | Honesty is a fundamental principle for aligning large language models (LLMs) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current LLMs still exhibit significant dishonest behaviors, such as confidently presenting wrong answers or failing to express what they know. In addition, research on the honesty of LLMs also faces challenges, including varying definitions of honesty, difficulties in distinguishing between known and unknown knowledge, and a lack of comprehensive understanding of related research. To address these issues, we provide a survey on the honesty of LLMs, covering its clarification, evaluation approaches, and strategies for improvement. Moreover, we offer insights for future research, aiming to inspire further exploration in this important area. | | \u786c\u6838\u751f\u6210\uff1a\u4e3a\u6570\u636e\u589e\u5f3a\u751f\u6210\u56f0\u96be\u7684UNSAT\u95ee\u9898 | \u9ad8\u6548\u5730\u786e\u5b9a\u5e03\u5c14\u65b9\u7a0b\u7684\u53ef\u6ee1\u8db3\u6027\u2014\u2014\u7b80\u79f0\u4e3aSAT\u95ee\u9898\u2014\u2014\u5728\u5404\u79cd\u5de5\u4e1a\u95ee\u9898\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u51fa\u73b0\u4e3a\u63d0\u5347SAT\u6c42\u89e3\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u9886\u57df\u53d1\u5c55\u7684\u4e00\u4e2a\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5f53\u524d\u5927\u591a\u6570\u516c\u5f00\u6570\u636e\u96c6\u8981\u4e48\u662f\u968f\u673a\u751f\u6210\u7684\uff0c\u8981\u4e48\u6781\u5176\u6709\u9650\uff0c\u4ec5\u5305\u542b\u6765\u81ea\u4e0d\u76f8\u5173\u95ee\u9898\u5bb6\u65cf\u7684\u5c11\u6570\u793a\u4f8b\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u8bad\u7ec3\u3002\u9274\u4e8e\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u5df2\u5f00\u59cb\u63a2\u7d22\u751f\u6210\u6280\u672f\uff0c\u4ee5\u521b\u5efa\u66f4\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u60c5\u51b5\u4e0b\u9047\u5230\u7684SAT\u95ee\u9898\u7684\u6570\u636e\u3002\u8fc4\u4eca\u4e3a\u6b62\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684SAT\u95ee\u9898\uff0c\u8981\u4e48\u9762\u4e34\u65f6\u95f4\u6269\u5c55\u6027\u7684\u969c\u788d\u3002\u672c\u6587\u901a\u8fc7\u8bc6\u522b\u548c\u64cd\u63a7\u95ee\u9898\u7684\u201c\u96be\u5ea6\u201d\u5173\u952e\u56e0\u7d20\u2014\u2014\u5373\u6838\u5fc3\uff08cores\uff09\uff0c\u6765\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002\u5c3d\u7ba1\u4e4b\u524d\u7684\u4e00\u4e9b\u5de5\u4f5c\u6d89\u53ca\u6838\u5fc3\u5904\u7406\uff0c\u4f46\u7531\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u6838\u5fc3\u68c0\u6d4b\u6280\u672f\u7684\u9ad8\u6210\u672c\uff0c\u65f6\u95f4\u6210\u672c\u8fc7\u9ad8\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u6838\u5fc3\u68c0\u6d4b\u7a0b\u5e8f\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u9ad8\u6548\u751f\u6210\u96be\u4ee5\u89e3\u51b3\u4e14\u4fdd\u7559\u539f\u59cb\u793a\u4f8b\u95ee\u9898\u5173\u952e\u5c5e\u6027\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u751f\u6210\u7684\u5408\u6210SAT\u95ee\u9898\u53ef\u4ee5\u5728\u6570\u636e\u589e\u5f3a\u573a\u666f\u4e2d\u4f7f\u7528\uff0c\u4ee5\u63d0\u4f9b\u66f4\u4f18\u7684\u6c42\u89e3\u5668\u8fd0\u884c\u65f6\u95f4\u9884\u6d4b\u3002 | Joseph Cotnareanu | PDF | N/A | HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation | Efficiently determining the satisfiability of a boolean equation -- known as the SAT problem for brevity -- is crucial in various industrial problems. Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets. The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods. In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles. In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes. | | \u4e00\u79cd\u5728\u4f4e\u6bd4\u7279GEMM\u6b8b\u5dee\u8ba1\u7b97\u4e2d\u4f7f\u7528RSVD\u7684\u65b9\u6cd5 | \u8fd1\u5e74\u6765\u786c\u4ef6\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u4f4e\u7cbe\u5ea6\u5e94\u7528\u5e26\u6765\u4e86\u8bb8\u591a\u53ef\u80fd\u6027\u3002\u7136\u800c\uff0c\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u53ef\u80fd\u4f1a\u5f15\u5165\u663e\u8457\u7684\u8ba1\u7b97\u8bef\u5dee\uff0c\u8fd9\u5bf9\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u6784\u6210\u4e86\u76f8\u5f53\u5927\u7684\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u79e9\u6b8b\u5dee\u91cf\u5316\u77e9\u9635\u4e58\u6cd5\uff08LRQMM\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5bc6\u96c6\u4f4e\u7cbe\u5ea6\u91cf\u5316\u77e9\u9635\u4e58\u6cd5\u4e2d\u5f15\u5165\u4f4e\u79e9\u8fd1\u4f3c\u8fdb\u884c\u6b8b\u5dee\u8865\u507f\u3002\u5b83\u53ef\u4ee5\u5728\u4ec5\u589e\u52a0BLAS-2\u7ea7\u522b\u989d\u5916\u65f6\u95f4\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5e26\u6765\u6570\u500d\u7684\u7cbe\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0cLRQMM\u662f\u4e00\u79cd\u5b8c\u5168\u65e0\u6570\u636e\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u4e0d\u9700\u8981\u989d\u5916\u7684\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u5b83\u4ec5\u4e0e\u4f4e\u7cbe\u5ea6GEMM\u8fd0\u7b97\u7b26\u914d\u5408\u4f7f\u7528\uff0c\u6613\u4e8e\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cLRQMM\u53ef\u4ee5\u5c06\u76f4\u63a5\u91cf\u5316\u77e9\u9635\u4e58\u6cd5\u7684\u8bef\u5dee\u964d\u4f4e1~2\u4e2a\u6570\u91cf\u7ea7\uff0c\u800c\u5728\u5904\u7406\u8f83\u5927\u77e9\u9635\u5c3a\u5bf8\u65f6\uff0c\u8ba1\u7b97\u901f\u5ea6\u4ec5\u964d\u4f4e\u7ea620%\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u4e2d\uff0cLRQMM-4bit\u5728Resnet-50\u4e0a\u5b9e\u73b0\u4e8661.8%\u7684ImageNet Top-1\u51c6\u786e\u7387\uff0c\u800c\u76f4\u63a5\u91cf\u5316\u7684\u51c6\u786e\u7387\u4ec5\u4e3a8.3%\u3002 | Hongyaoxing Gu | PDF | N/A | A method of using RSVD in residual calculation of LowBit GEMM | The advancements of hardware technology in recent years has brought many possibilities for low-precision applications. However, the use of low precision can introduce significant computational errors, posing a considerable challenge to maintaining the computational accuracy.   We propose low-rank residuals quantized matrix multiplication(LRQMM) method which introduces low-rank approximation in residual compensation for dense low precision quantization matrix multiplication. It can bring several times accuracy improvement with only BLAS-2 level extra time overhead. Moreover, LRQMM is a completely data-free quantization method that does not require additional data for pre-training. And it only works with low precision GEMM operator, which is easy to couple with other methods.   Through experimentation, LRQMM can reduce the error of direct quantized matrix multiplication by 1~2 orders of magnitude, when dealing with larger matrix sizes, the computational speed is only reduced by approximately 20\\%. In deep learning networks, LRQMM-4bit achieves 61.8% ImageNet Top-1 accuracy in Resnet-50, while the Direct Quant accuracy is only 8.3%. | | \u4ece\u793a\u8303\u4e2d\u5b66\u4e60\u5e26\u6709\u9690\u5f0f\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b | \u4ece\u793a\u8303\u4e2d\u5b66\u4e60\uff08LfD\uff09\u662f\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u7528\u8303\u5f0f\uff0c\u8fd9\u4e9b\u7b56\u7565\u80fd\u591f\u89e3\u51b3\u6d89\u53ca\u590d\u6742\u52a8\u4f5c\u7684\u4efb\u52a1\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u6210\u529f\u5e94\u7528LfD\u9700\u8981\u514b\u670d\u7b56\u7565\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5373\u7531\u4e8e\u8bef\u5dee\u968f\u65f6\u95f4\u7d2f\u79ef\u800c\u5bfc\u81f4\u7684\u6f02\u79fb\u95ee\u9898\u4ee5\u53ca\u968f\u4e4b\u800c\u6765\u7684\u5206\u5e03\u5916\u884c\u4e3a\u3002\u73b0\u6709\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u6269\u5c55\u6570\u636e\u6536\u96c6\u3001\u91c7\u7528\u4eba\u5728\u56de\u8def\u4e2d\u7684\u7b56\u7565\u9519\u8bef\u7ea0\u6b63\u3001\u65f6\u95f4\u96c6\u6210\u7b56\u7565\u9884\u6d4b\u6216\u5b66\u4e60\u52a8\u529b\u7cfb\u7edf\u6a21\u578b\u53c2\u6570\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u514b\u670d\u8be5\u95ee\u9898\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u53d7\u50a8\u5907\u8ba1\u7b97\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u8be5\u5c42\u5305\u542b\u4e00\u4e2a\u5177\u6709\u53ef\u8c03\u52a8\u529b\u5b66\u7279\u6027\u7684\u56fa\u5b9a\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u3002\u6211\u4eec\u901a\u8fc7\u5728LASA\u4eba\u7c7b\u624b\u5199\u6570\u636e\u96c6\u4e0a\u91cd\u73b0\u4eba\u7c7b\u624b\u5199\u52a8\u4f5c\u7684\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5c06\u6211\u4eec\u7684\u5c42\u878d\u5165\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u89e3\u51b3LfD\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\uff0c\u5305\u62ec\u7b56\u7565\u9884\u6d4b\u7684\u65f6\u95f4\u96c6\u6210\u548c\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESNs\uff09\u5b9e\u73b0\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u624b\u5199\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7b56\u7565\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u8fd8\u80fd\u9002\u5e94\u591a\u79cd\u52a8\u529b\u5b66\u72b6\u6001\u5e76\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u5ef6\u8fdf\u8bc4\u5206\u3002 | Peter David Fagan | PDF | N/A | Learning from Demonstration with Implicit Nonlinear Dynamics Models | Learning from Demonstration (LfD) is a useful paradigm for training policies that solve tasks involving complex motions. In practice, the successful application of LfD requires overcoming error accumulation during policy execution, i.e. the problem of drift due to errors compounding over time and the consequent out-of-distribution behaviours. Existing works seek to address this problem through scaling data collection, correcting policy errors with a human-in-the-loop, temporally ensembling policy predictions or through learning the parameters of a dynamical system model. In this work, we propose and validate an alternative approach to overcoming this issue. Inspired by reservoir computing, we develop a novel neural network layer that includes a fixed nonlinear dynamical system with tunable dynamical properties. We validate the efficacy of our neural network layer on the task of reproducing human handwriting motions using the LASA Human Handwriting Dataset. Through empirical experiments we demonstrate that incorporating our layer into existing neural network architectures addresses the issue of compounding errors in LfD. Furthermore, we perform a comparative evaluation against existing approaches including a temporal ensemble of policy predictions and an Echo State Networks (ESNs) implementation. We find that our approach yields greater policy precision and robustness on the handwriting task while also generalising to multiple dynamics regimes and maintaining competitive latency scores. | | \u7ed8\u5236\u672a\u6765\uff1a\u5229\u7528\u56fe\u8868\u95ee\u7b54\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u7684LLM\u9a71\u52a8\u6570\u636e\u53ef\u89c6\u5316 | \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u6765\u81ea\u52a8\u8bc4\u4f30\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6570\u636e\u53ef\u89c6\u5316\u3002\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u4eba\u5de5\u5224\u65ad\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u6216\u8005\u4ec5\u5173\u6ce8\u6570\u636e\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u89c6\u89c9\u4f20\u8fbe\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u91c7\u7528VQA\u6a21\u578b\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u6570\u636e\u8868\u793a\u7684\u8d28\u91cf\u4ee5\u53ca\u56fe\u8868\u7684\u6574\u4f53\u4f20\u8fbe\u6e05\u6670\u5ea6\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e24\u4e2a\u9886\u5148\u7684VQA\u57fa\u51c6\u6570\u636e\u96c6\uff0cChartQA\u548cPlotQA\uff0c\u5e76\u91c7\u7528\u4e86OpenAI\u7684GPT-3.5 Turbo\u548cMeta\u7684Llama 3.1 70B-Instruct\u6a21\u578b\u751f\u6210\u7684\u53ef\u89c6\u5316\u6570\u636e\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eVQA\u6027\u80fd\u6307\u6807\uff0cLLM\u751f\u6210\u7684\u56fe\u8868\u5728\u51c6\u786e\u6027\u4e0a\u672a\u80fd\u8fbe\u5230\u539f\u59cb\u975eLLM\u751f\u6210\u56fe\u8868\u7684\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u6211\u4eec\u7684\u7ed3\u679c\u663e\u793a\uff0c\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u751f\u6210\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5728LLM\u5b8c\u5168\u5339\u914d\u4eba\u7c7b\u751f\u6210\u56fe\u8868\u7684\u7cbe\u5ea6\u4e4b\u524d\uff0c\u4ecd\u9700\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002\u8fd9\u7a81\u663e\u4e86\u6211\u4eec\u5de5\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5feb\u901f\u8fed\u4ee3\uff0c\u52a0\u901f\u4e86\u7814\u7a76\u8fdb\u7a0b\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002 | James Ford | PDF | N/A | Charting the Future: Using Chart Question-Answering for Scalable Evaluation of LLM-Driven Data Visualizations | We propose a novel framework that leverages Visual Question Answering (VQA) models to automate the evaluation of LLM-generated data visualizations. Traditional evaluation methods often rely on human judgment, which is costly and unscalable, or focus solely on data accuracy, neglecting the effectiveness of visual communication. By employing VQA models, we assess data representation quality and the general communicative clarity of charts. Experiments were conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with visualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1 70B-Instruct models. Our results indicate that LLM-generated charts do not match the accuracy of the original non-LLM-generated charts based on VQA performance measures. Moreover, while our results demonstrate that few-shot prompting significantly boosts the accuracy of chart generation, considerable progress remains to be made before LLMs can fully match the precision of human-generated graphs. This underscores the importance of our work, which expedites the research process by enabling rapid iteration without the need for human annotation, thus accelerating advancements in this field. | | \u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u5728\u661f\u7cfb-\u6697\u6655\u5173\u8054\u4e2d\u7684\u5e94\u7528\uff1a\u661f\u7cfb\u5185\u7980\u5bf9\u9f50\u7684\u6848\u4f8b\u7814\u7a76 | \u5373\u5c06\u5230\u6765\u7684\u5b87\u5b99\u5b66\u6210\u50cf\u8c03\u67e5\uff0c\u5982\u9c81\u5bbe\u5929\u6587\u53f0LSST\uff0c\u9700\u8981\u5927\u89c4\u6a21\u7684\u6a21\u62df\uff0c\u6db5\u76d6\u5404\u79cd\u79d1\u5b66\u5e94\u7528\u7684\u771f\u5b9e\u661f\u7cfb\u7fa4\u4f53\u3002\u7279\u522b\u5173\u6ce8\u7684\u662f\u56fa\u6709\u5bf9\u9f50\uff08IA\uff09\u73b0\u8c61\uff0c\u5373\u661f\u7cfb\u671d\u5411\u8fc7\u5bc6\u533a\u57df\u5b9a\u5411\uff0c\u5982\u679c\u4e0d\u9002\u5f53\u5efa\u6a21\uff0c\u53ef\u80fd\u4f1a\u5728\u5f31\u5f15\u529b\u900f\u955c\u5206\u6790\u4e2d\u5f15\u5165\u663e\u8457\u7684\u7cfb\u7edf\u504f\u5dee\u3002\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\uff0c\u6a21\u62df\u8de8\u8d8a\u5e7f\u9614\u4f53\u79ef\u7684\u4e0eIA\u76f8\u5173\u7684\u661f\u7cfb\u5f62\u6210\u548c\u6f14\u5316\u7684\u590d\u6742\u7ec6\u8282\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIllustrisTNG-100\u6a21\u62df\u8bad\u7ec3\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u91c7\u68373D\u661f\u7cfb\u5f62\u72b6\u548c\u65b9\u5411\uff0c\u4ee5\u51c6\u786e\u518d\u73b0\u56fa\u6709\u5bf9\u9f50\u53ca\u5176\u76f8\u5173\u7684\u6807\u91cf\u7279\u5f81\u3002\u6211\u4eec\u5c06\u5b87\u5b99\u7f51\u5efa\u6a21\u4e3a\u4e00\u7ec4\u56fe\uff0c\u6bcf\u4e2a\u56fe\u4ee3\u8868\u4e00\u4e2a\u5305\u542b\u5b50\u661f\u7cfb/\u661f\u7cfb\u7684\u6697\u7269\u8d28\u6655\u3002\u67b6\u6784\u5305\u62ec\u4e00\u4e2aSO(3) $\\times$ $\\mathbb{R}^n$\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u661f\u7cfb\u65b9\u5411\u548c$n$\u4e2a\u6807\u91cf\uff0c\u901a\u8fc7E(3)\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\uff0c\u660e\u786e\u5c0a\u91cd\u6211\u4eec\u5b87\u5b99\u7684\u6b27\u51e0\u91cc\u5f97\u5bf9\u79f0\u6027\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u548c\u9884\u6d4b\u4e0e\u53c2\u8003\u6a21\u62df\u7edf\u8ba1\u4e00\u81f4\u7684\u661f\u7cfb\u65b9\u5411\u7b49\u7279\u5f81\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c55\u793a\u4e86\u8054\u5408\u5efa\u6a21\u6b27\u51e0\u91cc\u5f97\u6807\u91cf\uff08\u661f\u7cfb\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u989c\u8272\uff09\u548c\u975e\u6b27\u51e0\u91cc\u5f97SO(3)\u91cf\uff08\u661f\u7cfb\u65b9\u5411\uff09\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u91cf\u5728\u975e\u7ebf\u6027\u5c3a\u5ea6\u4e0a\u53d7\u9ad8\u5ea6\u590d\u6742\u7684\u661f\u7cfb\u7269\u7406\u652f\u914d\u3002 | Yesukhei Jagvaral | PDF | N/A | Geometric deep learning for galaxy-halo connection: a case study for galaxy intrinsic alignments | Forthcoming cosmological imaging surveys, such as the Rubin Observatory LSST, require large-scale simulations encompassing realistic galaxy populations for a variety of scientific applications. Of particular concern is the phenomenon of intrinsic alignments (IA), whereby galaxies orient themselves towards overdensities, potentially introducing significant systematic biases in weak gravitational lensing analyses if they are not properly modeled. Due to computational constraints, simulating the intricate details of galaxy formation and evolution relevant to IA across vast volumes is impractical. As an alternative, we propose a Deep Generative Model trained on the IllustrisTNG-100 simulation to sample 3D galaxy shapes and orientations to accurately reproduce intrinsic alignments along with correlated scalar features. We model the cosmic web as a set of graphs, each graph representing a halo with nodes representing the subhalos/galaxies. The architecture consists of a SO(3) $\\times$ $\\mathbb{R}^n$ diffusion generative model, for galaxy orientations and $n$ scalars, implemented with E(3) equivariant Graph Neural Networks that explicitly respect the Euclidean symmetries of our Universe. The model is able to learn and predict features such as galaxy orientations that are statistically consistent with the reference simulation. Notably, our model demonstrates the ability to jointly model Euclidean-valued scalars (galaxy sizes, shapes, and colors) along with non-Euclidean valued SO(3) quantities (galaxy orientations) that are governed by highly complex galactic physics at non-linear scales. | | TensorSocket\uff1a\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u5171\u4eab\u6570\u636e\u52a0\u8f7d | \u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u662f\u4e00\u4e2a\u91cd\u590d\u4e14\u8d44\u6e90\u5bc6\u96c6\u7684\u8fc7\u7a0b\u3002\u6570\u636e\u79d1\u5b66\u5bb6\u901a\u5e38\u4f1a\u5728\u627e\u5230\u4e00\u7ec4\u53c2\u6570\uff08\u4f8b\u5982\uff0c\u8d85\u53c2\u6570\u8c03\u6574\uff09\u3001\u6a21\u578b\u67b6\u6784\uff08\u4f8b\u5982\uff0c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff09\u7b49\u80fd\u591f\u4ea7\u751f\u6700\u9ad8\u51c6\u786e\u7387\u7684\u8bbe\u7f6e\u4e4b\u524d\uff0c\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u3002\u8fd9\u4e9b\u8bad\u7ec3\u4efb\u52a1\u7684\u8ba1\u7b97\u6548\u7387\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6211\u4eec\u80fd\u591f\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u7684\u6548\u7387\u3002\u8fd9\u4e9b\u4efb\u52a1\u7684\u91cd\u590d\u6027\u5bfc\u81f4\u76f8\u540c\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u4e00\u904d\u53c8\u4e00\u904d\u5730\u8fd0\u884c\uff0c\u52a0\u5267\u4e86\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u548c\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Tensorsocket\uff0c\u901a\u8fc7\u4f7f\u591a\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u5171\u4eab\u76f8\u540c\u7684\u6570\u636e\u52a0\u8f7d\u5668\u6765\u51cf\u5c11\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7684\u8ba1\u7b97\u9700\u6c42\u3002Tensorsocket\u5728\u5171\u7f6e\u7684\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u5728GPU\u4e0a\u5177\u6709\u9ad8\u541e\u5410\u91cf\u4f46\u5728CPU\u4e0a\u6570\u636e\u52a0\u8f7d\u541e\u5410\u91cf\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u7f13\u89e3\u4e86CPU\u7aef\u7684\u74f6\u9888\u3002Tensorsocket\u901a\u8fc7\u51cf\u5c11\u5171\u7f6e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u5e76\u5229\u7528\u73b0\u4ee3GPU-GPU\u4e92\u8fde\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u5c55\u793a\u4e86Tensorsocket\u7684\u786c\u4ef6\u548c\u7ba1\u9053\u65e0\u5173\u6027\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u8bad\u7ec3\u573a\u666f\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cTensorsocket\u4f7f\u5f97\u5728\u6ca1\u6709\u6570\u636e\u5171\u4eab\u7684\u60c5\u51b5\u4e0b\u4e0d\u53ef\u884c\u7684\u573a\u666f\u53d8\u5f97\u53ef\u884c\uff0c\u5c06\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u6700\u591a100%\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u4e91\u5b9e\u4f8b\u65f6\uff0c\u901a\u8fc7\u51cf\u5c11CPU\u7aef\u7684\u786c\u4ef6\u8d44\u6e90\u9700\u6c42\uff0cTensorsocket\u5b9e\u73b0\u4e8650%\u7684\u6210\u672c\u8282\u7701\u3002\u6b64\u5916\uff0cTensorsocket\u5728\u5171\u4eab\u6570\u636e\u52a0\u8f7d\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982CoorDL\u548cJoader\u3002\u5b83\u66f4\u6613\u4e8e\u4f7f\u7528\u3001\u7ef4\u62a4\u548c\u90e8\u7f72\uff0c\u5e76\u4e14\u5728\u9700\u8981\u8f83\u5c11CPU\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u8981\u4e48\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff0c\u8981\u4e48\u4e0e\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u6301\u5e73\u3002 | Ties Robroek | PDF | N/A | TensorSocket: Shared Data Loading for Deep Learning Training | Training deep learning models is a repetitive and resource-intensive process. Data scientists often train several models before landing on set of parameters (e.g., hyper-parameter tuning), model architecture (e.g., neural architecture search), among other things that yields the highest accuracy. The computational efficiency of these training tasks depends highly on how well we can supply the training process with training data. The repetitive nature of these tasks results in the same data processing pipelines running over and over exacerbating the need for and costs of computational resources.   In this paper, we present Tensorsocket to reduce the computational needs of deep learning training by enabling simultaneous training processes to share the same data loader. Tensorsocket mitigates CPU-side bottlenecks in cases where the collocated training workloads have high throughput on GPU, but are held back by lower data-loading throughput on CPU. Tensorsocket achieves this by reducing redundant computations across collocated training processes and leveraging modern GPU-GPU interconnects. We demonstrate the hardware- and pipeline-agnostic nature of Tensorsocket and evaluate it using a variety of training scenarios.   Our evaluation shows that Tensorsocket enables scenarios that are infeasible without data sharing, increases training throughput by up to $100\\%$, and when utilizing cloud instances, Tensorsocket achieves cost savings of $50\\%$ by reducing the hardware resource needs on the CPU side. Furthermore, Tensorsocket outperforms the state-of-the-art solutions for shared data loading such as CoorDL and Joader. It is easier to use, maintain, and deploy, and either achieves higher or matches the throughput of other solutions while requiring less CPU resources. | | \u6ce8\u610f\uff1a\u5e26\u6709\u4f59\u5f26\u6ce8\u610f\u529b\u7684\u7ebf\u6027\u53d8\u538b\u5668 | \u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c24\u5176\u662fsoftmax\u6ce8\u610f\u529b\uff0c\u5728\u8bf8\u5982GPT\u7b49\u57fa\u4e8etransformer\u7684\u6a21\u578b\u7684\u6210\u529f\u4e2d\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0csoftmax\u6ce8\u610f\u529b\u76f8\u5bf9\u4e8e\u5e8f\u5217\u957f\u5ea6\u7684\u4e8c\u6b21\u5185\u5b58\u590d\u6742\u6027\uff0c\u7ed9\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\u3002\u6211\u4eec\u5f15\u5165\u4e86Cottention\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u53d6\u4ee3\u4e86softmax\u64cd\u4f5c\u3002\u901a\u8fc7\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u7279\u6027\u5e76\u91cd\u65b0\u6392\u5217\u6ce8\u610f\u529b\u65b9\u7a0b\uff0cCottention\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u5e8f\u5217\u957f\u5ea6\u7684\u672c\u5f81\u7ebf\u6027\u5185\u5b58\u590d\u6742\u6027\uff0c\u4f7f\u5176\u5728\u5185\u5b58\u6548\u7387\u4e0a\u5929\u751f\u4f18\u4e8esoftmax\u6ce8\u610f\u529b\u3002\u6211\u4eec\u8bc1\u660e\uff0cCottention\u53ef\u4ee5\u88ab\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u4e2a\u5177\u6709\u6709\u9650\u9690\u85cf\u72b6\u6001\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u4ece\u800c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6052\u5b9a\u7684\u5185\u5b58\u4f7f\u7528\u3002\u6211\u4eec\u5728\u53cc\u5411BERT\u548c\u56e0\u679cGPT\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86Cottention\uff0c\u5c55\u793a\u4e86\u5176\u4e0esoftmax\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\u3002\u4e3a\u4e86\u786e\u4fdd\u9ad8\u6548\u7684\u8ba1\u7b97\uff0c\u6211\u4eec\u4e3aCottention\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684CUDA\u5185\u6838\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCottention\u662fsoftmax\u6ce8\u610f\u529b\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5176\u672c\u5f81\u7684\u7ebf\u6027\u5185\u5b58\u590d\u6742\u6027\u548c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u6052\u5b9a\u5185\u5b58\u5360\u7528\u7684\u80fd\u529b\u3002 | Gabriel Mongaras | PDF | N/A | Cottention: Linear Transformers With Cosine Attention | Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference. | | \u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u5f15\u5bfc\u7684\u533a\u57df\u5212\u5206\u8fdb\u5316\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6709\u9650\u591a\u8f7d\u81ea\u52a8\u5bfc\u5f15\u8f66\u6761\u4ef6\u4e0b\u7684\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898 | \u5728\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u73af\u5883\u4e0b\uff0c\u4f7f\u7528\u81ea\u52a8\u5bfc\u5f15\u8f66\uff08AGV\uff09\u6765\u8fd0\u8f93\u5de5\u4ef6\u548c\u52a0\u5de5\u6750\u6599\u662f\u4fc3\u8fdb\u8f66\u95f4\u667a\u80fd\u5316\u7684\u91cd\u8981\u65b9\u5f0f\u3002\u4e0e\u5355\u8f7dAGV\u76f8\u6bd4\uff0c\u591a\u8f7dAGV\u53ef\u4ee5\u63d0\u9ad8AGV\u5229\u7528\u7387\uff0c\u51cf\u5c11\u8def\u5f84\u51b2\u7a81\u7b49\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u9488\u5bf9\u6709\u9650\u591a\u8f7dAGV\u7684\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff08FJSPMA\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5386\u53f2\u5f15\u5bfc\u7684\u533a\u57df\u5212\u5206\u7b97\u6cd5\uff08HRPEO\uff09\u3002\u9996\u5148\uff0c\u6839\u636e\u591a\u8f7dAGV\u7684\u7279\u70b9\u8bbe\u8ba1\u4e86\u7f16\u7801\u548c\u89e3\u7801\u89c4\u5219\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u5206\u652f\u5b9a\u754c\u6cd5\u7684\u521d\u59cb\u5316\u89c4\u5219\u751f\u6210\u521d\u59cb\u79cd\u7fa4\u3002\u5176\u6b21\uff0c\u4e3a\u4e86\u9632\u6b62\u7b97\u6cd5\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u7b97\u6cd5\u91c7\u7528\u4e86\u533a\u57df\u5212\u5206\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5c06\u89e3\u7a7a\u95f4\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u5e76\u6d4b\u91cf\u5404\u533a\u57df\u7684\u6f5c\u529b\u3002\u4e4b\u540e\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u5c06\u533a\u57df\u805a\u7c7b\u4e3a\u591a\u4e2a\u7c07\uff0c\u5e76\u6839\u636e\u7c07\u96c6\u9009\u62e9\u4e2a\u4f53\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u3002\u7b2c\u4e09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5c40\u90e8\u641c\u7d22\u7b56\u7565\u4ee5\u63d0\u9ad8\u7b97\u6cd5\u7684\u5f00\u53d1\u80fd\u529b\uff0c\u8be5\u7b56\u7565\u5229\u7528\u8d2a\u5fc3\u65b9\u6cd5\u6839\u636eFJSPMA\u7684\u7279\u70b9\u4f18\u5316\u673a\u5668\u9009\u62e9\u548c\u8fd0\u8f93\u987a\u5e8f\u3002\u6700\u540e\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u4ee5\u6d4b\u8bd5\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u4e0e\u591a\u4e2a\u5148\u8fdb\u7b97\u6cd5\u76f8\u6bd4\uff0c\u7ed3\u679c\u8868\u660eHRPEO\u5728\u89e3\u51b3FJSPMA\u95ee\u9898\u65b9\u9762\u5177\u6709\u66f4\u597d\u7684\u4f18\u52bf\u3002 | Feige Liu | PDF | N/A | A History-Guided Regional Partitioning Evolutionary Optimization for Solving the Flexible Job Shop Problem with Limited Multi-load Automated Guided Vehicles | In a flexible job shop environment, using Automated Guided Vehicles (AGVs) to transport jobs and process materials is an important way to promote the intelligence of the workshop. Compared with single-load AGVs, multi-load AGVs can improve AGV utilization, reduce path conflicts, etc. Therefore, this study proposes a history-guided regional partitioning algorithm (HRPEO) for the flexible job shop scheduling problem with limited multi-load AGVs (FJSPMA). First, the encoding and decoding rules are designed according to the characteristics of multi-load AGVs, and then the initialization rule based on the branch and bound method is used to generate the initial population. Second, to prevent the algorithm from falling into a local optimum, the algorithm adopts a regional partitioning strategy. This strategy divides the solution space into multiple regions and measures the potential of the regions. After that, cluster the regions into multiple clusters in each iteration, and selects individuals for evolutionary search based on the set of clusters. Third, a local search strategy is designed to improve the exploitation ability of the algorithm, which uses a greedy approach to optimize machines selection and transportation sequence according to the characteristics of FJSPMA. Finally, a large number of experiments are carried out on the benchmarks to test the performance of the algorithm. Compared with multiple advanced algorithms, the results show that the HRPEO has a better advantage in solving FJSPMA. | | \u81ea\u56de\u5f52\u7b56\u7565\u4f18\u5316\u7528\u4e8e\u7ea6\u675f\u5206\u914d\u4efb\u52a1 | \u5206\u914d\u4efb\u52a1\u4ee3\u8868\u4e00\u7c7b\u95ee\u9898\uff0c\u5176\u4e2d\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5fc5\u987b\u5c06\u6709\u9650\u6570\u91cf\u7684\u8d44\u6e90\u5206\u914d\u7ed9\u4e00\u7ec4\u5b9e\u4f53\u3002\u8fd9\u7c7b\u4efb\u52a1\u7684\u663e\u8457\u4f8b\u5b50\u5305\u62ec\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u6216\u8de8\u670d\u52a1\u5668\u5206\u914d\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u3002\u5206\u914d\u4efb\u52a1\u901a\u5e38\u53d7\u5230\u7ebf\u6027\u7ea6\u675f\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u7ea6\u675f\u63cf\u8ff0\u4e86\u5fc5\u987b\u5728\u4efb\u4f55\u65f6\u5019\u4e25\u683c\u6ee1\u8db3\u7684\u5b9e\u9645\u8981\u6c42\u3002\u4f8b\u5982\uff0c\u5728\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\uff0c\u6295\u8d44\u8005\u53ef\u80fd\u88ab\u8981\u6c42\u5728\u4efb\u4f55\u6295\u8d44\u671f\u95f4\u5c06\u8d44\u91d1\u5206\u914d\u5230\u67d0\u4e2a\u5de5\u4e1a\u90e8\u95e8\u7684\u91d1\u989d\u4e0d\u5f97\u8d85\u8fc730%\u3002\u8fd9\u4e9b\u7ea6\u675f\u4ee5\u590d\u6742\u7684\u65b9\u5f0f\u9650\u5236\u4e86\u5141\u8bb8\u7684\u5206\u914d\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u5f97\u5b66\u4e60\u907f\u514d\u8fdd\u53cd\u7ea6\u675f\u7684\u7b56\u7565\u53d8\u5f97\u56f0\u96be\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u8fc7\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ea6\u675f\u5206\u914d\u4efb\u52a1\uff0c\u4ee5\u987a\u5e8f\u5730\u4e3a\u6bcf\u4e2a\u5b9e\u4f53\u91c7\u6837\u5206\u914d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53bb\u504f\u673a\u5236\uff0c\u4ee5\u62b5\u6d88\u987a\u5e8f\u91c7\u6837\u5f15\u8d77\u7684\u521d\u59cb\u504f\u5dee\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u7ea6\u675f\u5206\u914d\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5404\u79cd\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff1a\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u3001\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u548c\u4e00\u4e2a\u7efc\u5408\u5206\u914d\u57fa\u51c6\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1ahttps://github.com/niklasdbs/paspo\u3002 | David Winkel | PDF | N/A | Autoregressive Policy Optimization for Constrained Allocation Tasks | Allocation tasks represent a class of problems where a limited amount of resources must be allocated to a set of entities at each time step. Prominent examples of this task include portfolio optimization or distributing computational workloads across servers. Allocation tasks are typically bound by linear constraints describing practical requirements that have to be strictly fulfilled at all times. In portfolio optimization, for example, investors may be obligated to allocate less than 30\\% of the funds into a certain industrial sector in any investment period. Such constraints restrict the action space of allowed allocations in intricate ways, which makes learning a policy that avoids constraint violations difficult. In this paper, we propose a new method for constrained allocation tasks based on an autoregressive process to sequentially sample allocations for each entity. In addition, we introduce a novel de-biasing mechanism to counter the initial bias caused by sequential sampling. We demonstrate the superior performance of our approach compared to a variety of Constrained Reinforcement Learning (CRL) methods on three distinct constrained allocation tasks: portfolio optimization, computational workload distribution, and a synthetic allocation benchmark. Our code is available at: https://github.com/niklasdbs/paspo | | \u8de8\u9886\u57df\u5173\u952e\u8bcd\u63d0\u53d6\u4e0e\u5173\u952e\u6027\u6a21\u5f0f | \u9886\u57df\u4f9d\u8d56\u6027\u548c\u6807\u6ce8\u4e3b\u89c2\u6027\u7ed9\u6709\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u57fa\u4e8e\u793e\u7fa4\u5c42\u9762\u5b58\u5728\u4e8c\u9636\u663e\u8457\u6027\u6a21\u5f0f\u4e14\u53ef\u4ece\u6807\u6ce8\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u76d1\u7763\u7684\u6392\u5e8f\u65b9\u6cd5\u6765\u8fdb\u884c\u5173\u952e\u8bcd\u62bd\u53d6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5305\u542b\u72ec\u7acb\u7279\u5f81\uff08\u5982\u5b50\u8bed\u8a00\u9886\u57df\u548c\u672f\u8bed\u957f\u5ea6\uff09\u548c\u4e09\u7c7b\u4f9d\u8d56\u7279\u5f81\u2014\u2014\u542f\u53d1\u5f0f\u7279\u5f81\u3001\u7279\u5f02\u6027\u7279\u5f81\u548c\u4ee3\u8868\u6027\u7279\u5f81\u7684\u663e\u8457\u6027\u6a21\u5f0f\u6765\u5bf9\u5173\u952e\u8bcd\u8fdb\u884c\u6392\u5e8f\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e24\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u4ece\u5173\u952e\u8bcd\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u663e\u8457\u6027\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3e\u91c7\u6837\u7b56\u7565\u8bad\u7ec3\u8fd9\u4e24\u79cd\u6a21\u578b\u6765\u514b\u670d\u6807\u6ce8\u4e3b\u89c2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u4e00\u822c\u6709\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u4e2d\u5728\u5341\u4e2a\u5173\u952e\u8bcd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5e73\u5747top-10-F-measure\u4e3a0.316\u7684\u5148\u8fdb\u6027\u80fd\uff0c\u800c\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672a\u5305\u542b\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747top-10-F-measure\u4e3a0.346\u7684\u8de8\u9886\u57df\u9c81\u68d2\u6027\u3002\u8fd9\u79cd\u8de8\u9886\u57df\u9c81\u68d2\u6027\u5f52\u56e0\u4e8e\u793e\u7fa4\u5c42\u9762\u7684\u663e\u8457\u6027\u6a21\u5f0f\u6570\u91cf\u6709\u9650\u4e14\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u72ec\u7acb\u4e8e\u8bed\u8a00\u9886\u57df\u3001\u72ec\u7acb\u7279\u5f81\u4e0e\u4f9d\u8d56\u7279\u5f81\u7684\u533a\u5206\uff0c\u4ee5\u53ca\u5e73\u8861\u8fc7\u62df\u5408\u98ce\u9669\u548c\u7f3a\u4e4f\u8d1f\u6837\u672c\u8bad\u7ec3\u6570\u636e\u7684\u91c7\u6837\u8bad\u7ec3\u7b56\u7565\u3002 | Dongmei Zhou | PDF | N/A | Cross-Domain Keyword Extraction with Keyness Patterns | Domain dependence and annotation subjectivity pose challenges for supervised keyword extraction. Based on the premises that second-order keyness patterns are existent at the community level and learnable from annotated keyword extraction datasets, this paper proposes a supervised ranking approach to keyword extraction that ranks keywords with keyness patterns consisting of independent features (such as sublanguage domain and term length) and three categories of dependent features -- heuristic features, specificity features, and representavity features. The approach uses two convolutional-neural-network based models to learn keyness patterns from keyword datasets and overcomes annotation subjectivity by training the two models with bootstrap sampling strategy. Experiments demonstrate that the approach not only achieves state-of-the-art performance on ten keyword datasets in general supervised keyword extraction with an average top-10-F-measure of 0.316 , but also robust cross-domain performance with an average top-10-F-measure of 0.346 on four datasets that are excluded in the training process. Such cross-domain robustness is attributed to the fact that community-level keyness patterns are limited in number and temperately independent of language domains, the distinction between independent features and dependent features, and the sampling training strategy that balances excess risk and lack of negative training data. | | \u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21\u9879\u76ee\u76ee\u5f55\u5e8f\u5217\u63a8\u8350\u4ea4\u53c9\u71b5\u635f\u5931 | \u53ef\u6269\u5c55\u6027\u95ee\u9898\u5728\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u7684\u751f\u4ea7\u5316\u8fc7\u7a0b\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5373\u4f7f\u662f\u8f7b\u91cf\u7ea7\u7684\u67b6\u6784\u4e5f\u53ef\u80fd\u7531\u4e8e\u4e2d\u95f4\u8ba1\u7b97\u800c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5e94\u7528\u5b8c\u6574\u7684\u4ea4\u53c9\u71b5\uff08CE\uff09\u635f\u5931\u901a\u5e38\u5728\u63a8\u8350\u8d28\u91cf\u65b9\u9762\u80fd\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u5927\u578b\u7269\u54c1\u76ee\u5f55\u65f6\uff0c\u5b83\u4f1a\u906d\u53d7\u8fc7\u5ea6\u7684GPU\u5185\u5b58\u5360\u7528\u3002\u672c\u6587\u5728\u5e8f\u5217\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u6269\u5c55\u4ea4\u53c9\u71b5\uff08SCE\uff09\u635f\u5931\u51fd\u6570\u3002\u5b83\u8fd1\u4f3c\u4e8e\u5177\u6709\u5927\u578b\u76ee\u5f55\u7684\u6570\u636e\u96c6\u7684CE\u635f\u5931\uff0c\u5728\u4e0d\u727a\u7272\u63a8\u8350\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\u3002\u4e0e\u4f20\u7edf\u7684\u8d1f\u91c7\u6837\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u7684GPU\u9ad8\u6548\u8ba1\u7b97\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u76ee\u5f55\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5143\u7d20\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6700\u53ef\u80fd\u6210\u4e3a\u5047\u9633\u6027\u7684\u5143\u7d20\u3002\u8fd9\u662f\u901a\u8fc7\u6700\u5927\u5185\u79ef\u641c\u7d22\u6765\u8fd1\u4f3c\u6a21\u578b\u8f93\u51fa\u5b50\u96c6\u4e0a\u7684softmax\u5206\u5e03\u5b9e\u73b0\u7684\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cSCE\u80fd\u5c06\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u9ad8\u8fbe100\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u751a\u81f3\u8d85\u8fc7\u5b83\u4eec\u7684\u6307\u6807\u503c\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fd8\u4e3a\u4e0d\u540c\u9886\u57df\u7684\u5927\u89c4\u6a21\u5f00\u53d1\u5f00\u8f9f\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u4f8b\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002 | Gleb Mezentsev | PDF | N/A | Scalable Cross-Entropy Loss for Sequential Recommendations with Large Item Catalogs | Scalability issue plays a crucial role in productionizing modern recommender systems. Even lightweight architectures may suffer from high computational overload due to intermediate calculations, limiting their practicality in real-world applications. Specifically, applying full Cross-Entropy (CE) loss often yields state-of-the-art performance in terms of recommendations quality. Still, it suffers from excessive GPU memory utilization when dealing with large item catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss function in the sequential learning setup. It approximates the CE loss for datasets with large-size catalogs, enhancing both time efficiency and memory usage without compromising recommendations quality. Unlike traditional negative sampling methods, our approach utilizes a selective GPU-efficient computation strategy, focusing on the most informative elements of the catalog, particularly those most likely to be false positives. This is achieved by approximating the softmax distribution over a subset of the model outputs through the maximum inner product search. Experimental results on multiple datasets demonstrate the effectiveness of SCE in reducing peak memory usage by a factor of up to 100 compared to the alternatives, retaining or even exceeding their metrics values. The proposed approach also opens new perspectives for large-scale developments in different domains, such as large language models. | | \u63d0\u53476G\u536b\u661f\u7f51\u7edc\u7684\u9891\u8c31\u6548\u7387\uff1a\u57fa\u4e8e\u5f02\u6b65\u8054\u90a6\u9006\u5f3a\u5316\u5b66\u4e60\u7684GAIL\u9a71\u52a8\u7684\u7b56\u7565\u5b66\u4e60 | \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff08GAIL\uff09\u9a71\u52a8\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u3001\u9891\u8c31\u5206\u914d\u548c\u8fdc\u7a0b\u7528\u6237\u8bbe\u5907\uff08RUE\uff09\u5173\u8054\u3002\u4f20\u7edf\u7684\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u53c2\u6570\u8c03\u6574\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\uff0c\u7279\u522b\u662f\u5229\u7528GAIL\u6846\u67b6\uff0c\u81ea\u52a8\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u800c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u3002\u6211\u4eec\u901a\u8fc7\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u589e\u5f3a\u8fd9\u4e00\u6846\u67b6\uff0c\u4f7f\u5206\u6563\u7684\u591a\u536b\u661f\u7cfb\u7edf\u80fd\u591f\u534f\u4f5c\u63a8\u5bfc\u51fa\u6700\u4f18\u7b56\u7565\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u6700\u5927\u5316\u9891\u8c31\u6548\u7387\uff08SE\uff09\uff0c\u540c\u65f6\u6ee1\u8db3RUE\u7684\u6700\u5c0f\u4fe1\u606f\u901f\u7387\u8981\u6c42\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u975e\u51f8\u3001NP\u96be\u6027\u8d28\uff0c\u6211\u4eec\u5c06\u591a\u5bf9\u4e00\u5339\u914d\u7406\u8bba\u4e0e\u591a\u667a\u80fd\u4f53\u5f02\u6b65\u8054\u90a6\u9006\u5f3a\u5316\u5b66\u4e60\uff08MA-AFIRL\uff09\u6846\u67b6\u76f8\u7ed3\u5408\u3002\u8fd9\u4f7f\u5f97\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u5f02\u6b65\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u5b66\u4e60\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u4e13\u5bb6\u7b56\u7565\u4f7f\u7528\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\uff08WOA\uff09\u751f\u6210\uff0c\u4e3aGAIL\u4e2d\u7684\u81ea\u52a8\u5956\u52b1\u51fd\u6570\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MA-AFIRL\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684RL\u65b9\u6cd5\uff0c\u6536\u655b\u6027\u548c\u5956\u52b1\u503c\u5206\u522b\u63d0\u9ad8\u4e8614.6%\u3002\u8fd9\u79cd\u65b0\u9896\u7684GAIL\u9a71\u52a8\u7684\u7b56\u7565\u5b66\u4e60\u4e3a6G NTN\u4f18\u5316\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002 | Sheikh Salman Hassan | PDF | N/A | Enhancing Spectrum Efficiency in 6G Satellite Networks: A GAIL-Powered Policy Learning via Asynchronous Federated Inverse Reinforcement Learning | In this paper, a novel generative adversarial imitation learning (GAIL)-powered policy learning approach is proposed for optimizing beamforming, spectrum allocation, and remote user equipment (RUE) association in NTNs. Traditional reinforcement learning (RL) methods for wireless network optimization often rely on manually designed reward functions, which can require extensive parameter tuning. To overcome these limitations, we employ inverse RL (IRL), specifically leveraging the GAIL framework, to automatically learn reward functions without manual design. We augment this framework with an asynchronous federated learning approach, enabling decentralized multi-satellite systems to collaboratively derive optimal policies. The proposed method aims to maximize spectrum efficiency (SE) while meeting minimum information rate requirements for RUEs. To address the non-convex, NP-hard nature of this problem, we combine the many-to-one matching theory with a multi-agent asynchronous federated IRL (MA-AFIRL) framework. This allows agents to learn through asynchronous environmental interactions, improving training efficiency and scalability. The expert policy is generated using the Whale optimization algorithm (WOA), providing data to train the automatic reward function within GAIL. Simulation results show that the proposed MA-AFIRL method outperforms traditional RL approaches, achieving a $14.6\\%$ improvement in convergence and reward value. The novel GAIL-driven policy learning establishes a novel benchmark for 6G NTN optimization. | | \u9605\u8bfb\u5b57\u91cc\u884c\u95f4\uff1a\u5229\u7528ASCII\u827a\u672f\u653b\u51fb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6bd2\u6027\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u63a9\u76d6\u4eb5\u6e0e\u5185\u5bb9 | \u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u653b\u51fb\u5bb6\u65cf\uff0c\u8fd9\u4e9b\u653b\u51fb\u5229\u7528\u4e86\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u89e3\u91caASCII\u827a\u672f\u7684\u7279\u70b9\u3002\u4e3a\u4e86\u8bc4\u4f30\u8fd9\u4e9b\u653b\u51fb\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ToxASCII\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u81ea\u5b9a\u4e49\u7684ASCII\u827a\u672f\u5b57\u4f53\uff1a\u4e00\u79cd\u5229\u7528\u7279\u6b8a\u6807\u8bb0\uff0c\u53e6\u4e00\u79cd\u4f7f\u7528\u586b\u5145\u6587\u672c\u7684\u5b57\u6bcd\u5f62\u72b6\u3002\u6211\u4eec\u7684\u653b\u51fb\u5728\u5305\u62ecOpenAI\u7684o1-preview\u548cLLaMA 3.1\u5728\u5185\u7684\u5341\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u76841.0\u653b\u51fb\u6210\u529f\u7387\u3002\u8b66\u544a\uff1a\u672c\u6587\u5305\u542b\u7528\u4e8e\u7814\u7a76\u76ee\u7684\u7684\u6709\u5bb3\u8bed\u8a00\u793a\u4f8b\u3002 | Sergey Berezin | PDF | N/A | Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity | We introduce a novel family of adversarial attacks that exploit the inability of language models to interpret ASCII art. To evaluate these attacks, we propose the ToxASCII benchmark and develop two custom ASCII art fonts: one leveraging special tokens and another using text-filled letter shapes. Our attacks achieve a perfect 1.0 Attack Success Rate across ten models, including OpenAI's o1-preview and LLaMA 3.1.   Warning: this paper contains examples of toxic language used for research purposes. |</p>"},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}