{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u5370\u5ea6\u516c\u52a1\u5458\u6a21\u62df\u9762\u8bd5\u4e2d\u7684\u6027\u522b\u8868\u73b0\u4e0e\u504f\u89c1 \u672c\u6587\u505a\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u8d21\u732e\u3002\u9996\u5148\uff0c\u901a\u8fc7\u4ece888\u4e2a\u5370\u5ea6\u516c\u52a1\u5458\u5019\u9009\u4eba\u6a21\u62df\u9762\u8bd5\u7684YouTube\u89c6\u9891\u4e2d\u6536\u96c6\u768451,278\u4e2a\u9762\u8bd5\u95ee\u9898\u7684\u5927\u91cf\u8bed\u6599\u5e93\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u9488\u5bf9\u7537\u6027\u548c\u5973\u6027\u5019\u9009\u4eba\u7684\u95ee\u9898\u5728\u5e7f\u4e49\u6027\u8d28\u4e0a\u5b58\u5728\u660e\u663e\u7684\u6027\u522b\u504f\u89c1\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6027\u522b\u63a8\u65ad\u4efb\u52a1\u4e2d\uff0cLLM\u63d0\u4f9b\u7684\u89e3\u91ca\u4e2d\u5b58\u5728\u5f3a\u70c8\u7684\u6027\u522b\u504f\u89c1\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b51,278\u4e2a\u9762\u8bd5\u95ee\u9898\u7684\u65b0\u9896\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u4e3a\u672a\u6765\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4fe1\u606f\u3002 Somonnoy Banerjee PDF N/A Gender Representation and Bias in Indian Civil Service Mock Interviews This paper makes three key contributions. First, via a substantial corpus of 51,278 interview questions sourced from 888 YouTube videos of mock interviews of Indian civil service candidates, we demonstrate stark gender bias in the broad nature of questions asked to male and female candidates. Second, our experiments with large language models show a strong presence of gender bias in explanations provided by the LLMs on the gender inference task. Finally, we present a novel dataset of 51,278 interview questions that can inform future social science studies. DynaMo\uff1a\u89c6\u89c9-\u8fd0\u52a8\u63a7\u5236\u9886\u57df\u5185\u52a8\u529b\u5b66\u9884\u8bad\u7ec3 \u6a21\u4eff\u5b66\u4e60\u5df2\u88ab\u8bc1\u660e\u662f\u8bad\u7ec3\u590d\u6742\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u5f3a\u5927\u5de5\u5177\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6570\u767e\u5230\u6570\u5343\u4e2a\u4e13\u5bb6\u6f14\u793a\u6765\u5904\u7406\u9ad8\u7ef4\u89c6\u89c9\u89c2\u5bdf\u3002\u8fd9\u79cd\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u4e00\u4e2a\u5173\u952e\u539f\u56e0\u662f\u89c6\u89c9\u8868\u5f81\u4e3b\u8981\u662f\u5728\u57df\u5916\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\uff0c\u6216\u8005\u76f4\u63a5\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DynaMo\uff0c\u4e00\u79cd\u65b0\u7684\u57df\u5185\u3001\u81ea\u76d1\u7763\u7684\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002\u7ed9\u5b9a\u4e00\u7ec4\u4e13\u5bb6\u6f14\u793a\uff0c\u6211\u4eec\u8054\u5408\u5b66\u4e60\u4e00\u4e2a\u6f5c\u5728\u7684\u9006\u52a8\u529b\u5b66\u6a21\u578b\u548c\u4e00\u4e2a\u524d\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5d4c\u5165\u5e8f\u5217\u4e0a\u8fd0\u884c\uff0c\u9884\u6d4b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u4e0b\u4e00\u5e27\uff0c\u65e0\u9700\u589e\u5f3a\u3001\u5bf9\u6bd4\u91c7\u6837\u6216\u8bbf\u95ee\u771f\u5b9e\u52a8\u4f5c\u3002\u91cd\u8981\u7684\u662f\uff0cDynaMo\u4e0d\u9700\u8981\u4efb\u4f55\u57df\u5916\u6570\u636e\uff0c\u5982\u4e92\u8054\u7f51\u6570\u636e\u96c6\u6216\u8de8\u5b9e\u4f53\u6570\u636e\u96c6\u3002\u5728\u516d\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4f7f\u7528DynaMo\u5b66\u4e60\u7684\u8868\u5f81\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u5148\u524d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u548c\u9884\u8bad\u7ec3\u8868\u5f81\u3002\u4f7f\u7528DynaMo\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u5728\u4e0d\u540c\u7b56\u7565\u7c7b\u522b\u4e2d\u90fd\u5f97\u5230\u4e86\u4f53\u73b0\uff0c\u5982\u884c\u4e3a\u8f6c\u6362\u5668\u3001\u6269\u6563\u7b56\u7565\u3001\u591a\u5c42\u611f\u77e5\u673a\u548c\u6700\u8fd1\u90bb\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9DynaMo\u7684\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\uff0c\u5e76\u6d4b\u91cf\u4e86\u5b83\u4eec\u5bf9\u4e0b\u6e38\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\u3002\u673a\u5668\u4eba\u89c6\u9891\u6700\u4f73\u89c2\u770b\u5730\u5740\u4e3ahttps://dynamo-ssl.github.io\u3002 Zichen Jeff Cui PDF N/A DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io Qwen2-VL\uff1a\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u610f\u5206\u8fa8\u7387\u4e0b\u5bf9\u4e16\u754c\u7684\u611f\u77e5\u80fd\u529b \u6211\u4eec\u63a8\u51fa\u4e86Qwen2-VL\u7cfb\u5217\uff0c\u8fd9\u662f\u5bf9\u4e4b\u524dQwen-VL\u6a21\u578b\u7684\u5148\u8fdb\u5347\u7ea7\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u89c9\u5904\u7406\u4e2d\u7684\u4f20\u7edf\u9884\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\u3002Qwen2-VL\u5f15\u5165\u4e86\u6734\u7d20\u52a8\u6001\u5206\u8fa8\u7387\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u52a8\u6001\u5904\u7406\u4e3a\u4e0d\u540c\u6570\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u89c6\u89c9\u8868\u793a\uff0c\u66f4\u8d34\u8fd1\u4eba\u7c7b\u7684\u611f\u77e5\u8fc7\u7a0b\u3002\u6a21\u578b\u8fd8\u96c6\u6210\u4e86\u591a\u6a21\u6001\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08M-RoPE\uff09\uff0c\u4fc3\u8fdb\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u4e4b\u95f4\u4f4d\u7f6e\u4fe1\u606f\u7684\u6709\u6548\u878d\u5408\u3002\u6211\u4eec\u91c7\u7528\u7edf\u4e00\u7684\u8303\u5f0f\u6765\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002\u4e3a\u4e86\u63a2\u7d22\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\uff0cQwen2-VL\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6269\u5c55\u89c4\u5f8b\u3002\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff08\u5305\u62ec2B\u30018B\u548c72B\u53c2\u6570\u7248\u672c\uff09\u548c\u8bad\u7ec3\u6570\u636e\u91cf\uff0cQwen2-VL\u7cfb\u5217\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cQwen2-VL-72B\u6a21\u578b\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0eGPT-4o\u548cClaude3.5-Sonnet\u7b49\u9886\u5148\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u901a\u7528\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728\\url{https://github.com/QwenLM/Qwen2-VL}\u83b7\u53d6\u3002 Peng Wang PDF N/A Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \\url{https://github.com/QwenLM/Qwen2-VL}. \u5927\u89c4\u6a21\u591a\u4eba3D\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e0e\u573a\u666f\u4e0a\u4e0b\u6587 \u9884\u6d4b\u957f\u671f\u7684\u4e09\u7ef4\u4eba\u4f53\u8fd0\u52a8\u5177\u6709\u6311\u6218\u6027\uff1a\u4eba\u7c7b\u884c\u4e3a\u7684\u968f\u673a\u6027\u4f7f\u5f97\u4ec5\u4ece\u8f93\u5165\u5e8f\u5217\u4e2d\u751f\u6210\u771f\u5b9e\u7684\u4eba\u4f53\u8fd0\u52a8\u53d8\u5f97\u56f0\u96be\u3002\u573a\u666f\u73af\u5883\u548c\u9644\u8fd1\u4eba\u7684\u8fd0\u52a8\u4fe1\u606f\u53ef\u4ee5\u6781\u5927\u5730\u8f85\u52a9\u751f\u6210\u8fc7\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u7684\u793e\u4ea4\u53d8\u538b\u5668\u6a21\u578b\uff08SAST\uff09\u6765\u9884\u6d4b\u957f\u671f\uff0810\u79d2\uff09\u7684\u4eba\u4f53\u8fd0\u52a8\u3002\u4e0e\u4ee5\u5f80\u7684\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6a21\u62df\u573a\u666f\u4e2d\u4eba\u6570\u548c\u7269\u4f53\u6570\u91cf\u5e7f\u6cdb\u53d8\u5316\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u6211\u4eec\u5c06\u65f6\u95f4\u5377\u79ef\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e0e\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u74f6\u9888\u76f8\u7ed3\u5408\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408\u8fd0\u52a8\u548c\u573a\u666f\u4fe1\u606f\u3002\u6211\u4eec\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u6765\u5efa\u6a21\u6761\u4ef6\u8fd0\u52a8\u5206\u5e03\u3002\u6211\u4eec\u5728\u201c\u53a8\u623f\u4e2d\u7684\u4eba\u7c7b\u201d\u6570\u636e\u96c6\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6570\u636e\u96c6\u540c\u65f6\u5305\u542b1\u523016\u4e2a\u4eba\u548c29\u523050\u4e2a\u53ef\u89c1\u7269\u4f53\u3002\u5728\u4e0d\u540c\u7684\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u73b0\u5b9e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728https://github.com/felixbmuller/SAST\u83b7\u53d6\u3002 Felix B Mueller PDF N/A Massively Multi-Person 3D Human Motion Forecasting with Scene Context Forecasting long-term 3D human motion is challenging: the stochasticity of human behavior makes it hard to generate realistic human motion from the input sequence alone. Information on the scene environment and the motion of nearby people can greatly aid the generation process. We propose a scene-aware social transformer model (SAST) to forecast long-term (10s) human motion motion. Unlike previous models, our approach can model interactions between both widely varying numbers of people and objects in a scene. We combine a temporal convolutional encoder-decoder architecture with a Transformer-based bottleneck that allows us to efficiently combine motion and scene information. We model the conditional motion distribution using denoising diffusion models. We benchmark our approach on the Humans in Kitchens dataset, which contains 1 to 16 persons and 29 to 50 objects that are visible simultaneously. Our model outperforms other approaches in terms of realism and diversity on different metrics and in a user study. Code is available at https://github.com/felixbmuller/SAST. Qwen2.5-Coder \u6280\u672f\u62a5\u544a \u5728\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Qwen2.5-Coder\u7cfb\u5217\uff0c\u8fd9\u662f\u4ece\u5176\u524d\u8eabCodeQwen1.5\u7684\u91cd\u5927\u5347\u7ea7\u3002\u8be5\u7cfb\u5217\u5305\u542b\u4e24\u4e2a\u6a21\u578b\uff1aQwen2.5-Coder-1.5B\u548cQwen2.5-Coder-7B\u3002\u4f5c\u4e3a\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u7684\u6a21\u578b\uff0cQwen2.5-Coder\u57fa\u4e8eQwen2.5\u67b6\u6784\u6784\u5efa\uff0c\u5e76\u5728\u8d85\u8fc75.5\u4e07\u4ebf\u4e2a\u6807\u8bb0\u7684\u5e9e\u5927\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7ee7\u7eed\u9884\u8bad\u7ec3\u3002\u901a\u8fc7\u7cbe\u7ec6\u7684\u6570\u636e\u6e05\u6d17\u3001\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u4ee5\u53ca\u5e73\u8861\u7684\u6570\u636e\u6df7\u5408\uff0cQwen2.5-Coder\u5728\u5c55\u73b0\u51fa\u8272\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4ecd\u4fdd\u6301\u4e86\u5e7f\u6cdb\u7684\u901a\u7528\u6027\u3002\u8be5\u6a21\u578b\u5df2\u5728\u591a\u79cd\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u8865\u5168\u3001\u63a8\u7406\u548c\u4fee\u590d\u5728\u5185\u7684\u8d85\u8fc710\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u76f8\u540c\u6a21\u578b\u5927\u5c0f\u7684\u5bf9\u6bd4\u4e2d\u6301\u7eed\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\u3002\u6211\u4eec\u76f8\u4fe1\uff0cQwen2.5-Coder\u7cfb\u5217\u7684\u53d1\u5e03\u4e0d\u4ec5\u5c06\u63a8\u52a8\u4ee3\u7801\u667a\u80fd\u7814\u7a76\u7684\u524d\u6cbf\uff0c\u8fd8\u5c06\u901a\u8fc7\u5176\u5bbd\u677e\u7684\u8bb8\u53ef\uff0c\u9f13\u52b1\u5f00\u53d1\u8005\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5e7f\u6cdb\u5730\u91c7\u7528\u3002 Binyuan Hui PDF N/A Qwen2.5-Coder Technical Report In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications. \u8981\u4f7f\u7528\u201c\u601d\u7ef4\u94fe\u201d\u8fd8\u662f\u4e0d\u4f7f\u7528\uff1f\u201c\u601d\u7ef4\u94fe\u201d\u4e3b\u8981\u5728\u6570\u5b66\u548c\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u53d1\u6325\u4f5c\u7528\u3002 \u601d\u7ef4\u94fe\uff08Chain-of-thought, CoT\uff09\u901a\u8fc7\u63d0\u793a\u662f\u6fc0\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u7684\u4e8b\u5b9e\u6807\u51c6\u65b9\u6cd5\u3002\u4f46\u8fd9\u79cd\u989d\u5916\u7684\u201c\u601d\u8003\u201d\u5bf9\u54ea\u4e9b\u7c7b\u578b\u7684\u4efb\u52a1\u771f\u6b63\u6709\u5e2e\u52a9\u5462\uff1f\u4e3a\u4e86\u5206\u6790\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u5b9a\u91cf\u5143\u5206\u6790\uff0c\u6db5\u76d6\u4e86\u4f7f\u7528CoT\u7684100\u591a\u7bc7\u8bba\u6587\uff0c\u5e76\u5bf914\u4e2a\u6a21\u578b\u4e0a\u768420\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6211\u4eec\u81ea\u5df1\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7ed3\u679c\u663e\u793a\uff0cCoT\u4e3b\u8981\u5728\u6d89\u53ca\u6570\u5b66\u6216\u903b\u8f91\u7684\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u800c\u5728\u5176\u4ed6\u7c7b\u578b\u7684\u4efb\u52a1\u4e0a\u6536\u76ca\u8f83\u5c0f\u3002\u5728MMLU\u4e0a\uff0c\u76f4\u63a5\u751f\u6210\u7b54\u6848\u800c\u4e0d\u4f7f\u7528CoT\u7684\u51c6\u786e\u6027\u4e0e\u4f7f\u7528CoT\u51e0\u4e4e\u76f8\u540c\uff0c\u9664\u975e\u95ee\u9898\u6216\u6a21\u578b\u7684\u56de\u7b54\u4e2d\u5305\u542b\u7b49\u53f7\uff0c\u8fd9\u8868\u660e\u6d89\u53ca\u7b26\u53f7\u64cd\u4f5c\u548c\u63a8\u7406\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u548c\u6267\u884c\uff0c\u5e76\u5c06\u5176\u4e0e\u5de5\u5177\u589e\u5f3a\u7684LLMs\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u4e86CoT\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u7684\u884c\u4e3a\u3002CoT\u7684\u5927\u90e8\u5206\u6536\u76ca\u6765\u81ea\u4e8e\u6539\u8fdb\u7b26\u53f7\u6267\u884c\uff0c\u4f46\u5728\u4f7f\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u65f6\u8868\u73b0\u76f8\u5bf9\u8f83\u5dee\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCoT\u53ef\u4ee5\u6709\u9009\u62e9\u6027\u5730\u5e94\u7528\uff0c\u65e2\u4fdd\u6301\u6027\u80fd\u53c8\u8282\u7701\u63a8\u7406\u6210\u672c\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u9700\u8981\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u7684CoT\uff0c\u8f6c\u5411\u65b0\u7684\u8303\u5f0f\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528LLM\u5e94\u7528\u4e2d\u7684\u4e2d\u95f4\u8ba1\u7b97\u3002 Zayne Sprague PDF N/A To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications. \u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u548c\u6cdb\u5316\u7684\u63a7\u5236\u7814\u7a76 \u5e7f\u6cdb\u7684\u6587\u672c\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u9700\u8981\u5229\u7528\u6574\u4e2a\u6587\u6863\u4e0a\u4e0b\u6587\u7684\u8bed\u8a00\u6a21\u578b\u3002\u7531\u4e8e\u76f4\u63a5\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u7684\u5b9e\u73b0\u6311\u6218\uff0c\u8bb8\u591a\u65b9\u6cd5\u88ab\u63d0\u51fa\u4ee5\u6269\u5c55\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6570\u636e\u548c\u6a21\u578b\u7c7b\u522b\u7684\u5dee\u5f02\uff0c\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u4e00\u76f4\u5f88\u56f0\u96be\uff0c\u5bfc\u81f4\u5982\u4f55\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u4ee5\u53ca\u5b83\u662f\u5426\u4e0e\u6807\u51c6\u8bc4\u4f30\u4e0d\u540c\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u6211\u4eec\u5b9e\u65bd\u4e86\u4e00\u4e2a\u5e26\u6709\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u6269\u5c55\u65b9\u6cd5\u63a7\u5236\u534f\u8bae\uff0c\u5229\u7528\u4e00\u81f4\u7684\u57fa\u7840\u6a21\u578b\u548c\u6269\u5c55\u6570\u636e\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f97\u51fa\u4e86\u5173\u4e8e\u957f\u4e0a\u4e0b\u6587\u884c\u4e3a\u7684\u51e0\u4e2a\u89c1\u89e3\u3002\u9996\u5148\uff0c\u6211\u4eec\u91cd\u7533\u4e86\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u901a\u7528\u6027\u80fd\u6307\u6807\u7684\u5173\u952e\u4f5c\u7528\u3002\u5176\u6b21\uff0c\u6211\u4eec\u53d1\u73b0\u5f53\u524d\u7684\u8fd1\u4f3c\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7cfb\u7edf\u6027\u5730\u8868\u73b0\u4e0d\u4f73\u3002\u6700\u540e\uff0c\u6211\u4eec\u786e\u8ba4\u57fa\u4e8e\u7cbe\u786e\u5fae\u8c03\u7684\u65b9\u6cd5\u5728\u5176\u6269\u5c55\u8303\u56f4\u5185\u901a\u5e38\u662f\u6709\u6548\u7684\uff0c\u800c\u5916\u63a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6240\u6709\u4ee3\u7801\u5e93\u3001\u6a21\u578b\u548c\u68c0\u67e5\u70b9\u5c06\u4f5c\u4e3a\u5f00\u6e90\u8d44\u6e90\u63d0\u4f9b\uff0c\u4fc3\u8fdb\u900f\u660e\u5ea6\u5e76\u63a8\u52a8\u8fd9\u4e00\u5173\u952e\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002 Yi Lu PDF N/A A Controlled Study on Long Context Extension and Generalization in LLMs Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development. \u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u8bed\u8a00\u8868\u8fbe \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u83b7\u53d6\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\uff0c\u4f46LLMs\u5f80\u5f80\u751f\u6210\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e8b\u5b9e\u76f8\u51b2\u7a81\u7684\u4fe1\u606f\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u529d\u8bf4\u6027\u98ce\u683c\u4f7f\u5f97\u8fd9\u4e9b\u4e0d\u51c6\u786e\u4e4b\u5904\u663e\u5f97\u81ea\u4fe1\u4e14\u6709\u8bf4\u670d\u529b\u3002\u56e0\u6b64\uff0c\u7ec8\u7aef\u7528\u6237\u96be\u4ee5\u6301\u7eed\u5730\u5c06LLMs\u8868\u8fbe\u7684\u81ea\u4fe1\u4e0e\u5b83\u4eec\u9884\u6d4b\u7684\u51c6\u786e\u6027\u76f8\u5339\u914d\uff0c\u8fd9\u901a\u5e38\u5bfc\u81f4\u5bf9\u6240\u6709\u8f93\u51fa\u76f2\u76ee\u4fe1\u4efb\u6216\u5b8c\u5168\u5ffd\u89c6\u5176\u53ef\u9760\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u7684\u9884\u6d4b\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u4ee5\u5f00\u53d1\u80fd\u591f\u4ea7\u751f\u4e0d\u786e\u5b9a\u6027\u8bed\u8a00\u8868\u8fbe\u7684\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6d4b\u91cf\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6821\u51c6\u60c5\u51b5\uff0c\u7136\u540e\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u6821\u51c6\u540e\u7684\u4e0d\u786e\u5b9a\u6027\u8bed\u8a00\u8868\u8fbe\u3002\u901a\u8fc7\u5728\u5404\u79cd\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86LLMs\u5728\u8bc4\u4f30\u5176\u9884\u6d4b\u65f6\u5177\u6709\u826f\u597d\u7684\u6821\u51c6\u6027\uff0c\u5e76\u4e14\u57fa\u4e8e\u6a21\u578b\u81ea\u8eab\u4fe1\u5fc3\u7684\u76d1\u7763\u5fae\u8c03\u80fd\u591f\u4ea7\u751f\u826f\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\uff0c\u7279\u522b\u662f\u5728\u5355\u9879\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u3002 Arslan Chaudhry PDF N/A Finetuning Language Models to Emit Linguistic Expressions of Uncertainty Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflicts with real-world facts, and their persuasive style can make these inaccuracies appear confident and convincing. As a result, end-users struggle to consistently align the confidence expressed by LLMs with the accuracy of their predictions, often leading to either blind trust in all outputs or a complete disregard for their reliability. In this work, we explore supervised finetuning on uncertainty-augmented predictions as a method to develop models that produce linguistic expressions of uncertainty. Specifically, we measure the calibration of pre-trained models and then fine-tune language models to generate calibrated linguistic expressions of uncertainty. Through experiments on various question-answering datasets, we demonstrate that LLMs are well-calibrated in assessing their predictions, and supervised finetuning based on the model's own confidence leads to well-calibrated expressions of uncertainty, particularly for single-claim answers. \u4f60\u53ea\u9700\u9605\u8bfb\u4e00\u6b21\uff08YORO\uff09\uff1a\u5b66\u4e60\u5c06\u6570\u636e\u5e93\u77e5\u8bc6\u5185\u5316\u4ee5\u5b9e\u73b0\u6587\u672c\u5230SQL\u7684\u8f6c\u6362 \u5c3d\u7ba1\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e0a\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6700\u8fd1\u7684\u89e3\u51b3\u65b9\u6848\u5728\u6bcf\u6b21\u63d0\u95ee\u65f6\u90fd\u4f1a\u91cd\u590d\u7f16\u7801\u76f8\u540c\u7684\u6570\u636e\u5e93\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u5e38\u5e38\u5ffd\u7565\u5173\u952e\u7684\u6570\u636e\u5e93\u77e5\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u4f60\u53ea\u8bfb\u4e00\u6b21\u201d\uff08YORO\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8303\u5f0f\uff0c\u5b83\u5728\u8bad\u7ec3\u671f\u95f4\u5c06\u6570\u636e\u5e93\u77e5\u8bc6\u76f4\u63a5\u5185\u5316\u5230\u6587\u672c\u5230SQL\u6a21\u578b\u7684\u53c2\u6570\u77e5\u8bc6\u4e2d\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d88\u9664\u4e86\u6a21\u5f0f\u7f16\u7801\u7684\u9700\u6c42\u3002YORO\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u5165\u4ee4\u724c\u957f\u5ea6\uff0c\u51cf\u5c11\u4e8666%-98%\u3002\u5c3d\u7ba1\u8f93\u5165\u8f83\u77ed\uff0c\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cYORO\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4f20\u7edf\u7cfb\u7edf\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u5927\u578b\u6570\u636e\u5e93\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002\u6b64\u5916\uff0cYORO\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u503c\u68c0\u7d22\u95ee\u9898\uff08\u5982\u7f29\u5199\uff09\u65f6\u8868\u73b0\u51fa\u8272\u3002 Hideo Kobayashi PDF N/A You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL While significant progress has been made on the text-to-SQL task, recent solutions repeatedly encode the same database schema for every question, resulting in unnecessary high inference cost and often overlooking crucial database knowledge. To address these issues, we propose You Only Read Once (YORO), a novel paradigm that directly internalizes database knowledge into the parametric knowledge of a text-to-SQL model during training and eliminates the need for schema encoding during inference. YORO significantly reduces the input token length by 66%-98%. Despite its shorter inputs, our empirical results demonstrate YORO's competitive performances with traditional systems on three benchmarks as well as its significant outperformance on large databases. Furthermore, YORO excels in handling questions with challenging value retrievals such as abbreviation. \u89e3\u7801\u98ce\u683c\uff1a\u5229\u7528\u504f\u597d\u9ad8\u6548\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u5b9e\u73b0\u56fe\u50cf\u5f15\u5bfc\u7684\u670d\u88c5\u63a8\u8350 \u4e2a\u6027\u5316\u670d\u88c5\u63a8\u8350\u4ecd\u7136\u662f\u4e00\u4e2a\u590d\u6742\u7684\u6311\u6218\uff0c\u9700\u8981\u5bf9\u65f6\u5c1a\u517c\u5bb9\u6027\u548c\u6f6e\u6d41\u610f\u8bc6\u6709\u6df1\u5165\u7684\u7406\u89e3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8868\u8fbe\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e00\u4efb\u52a1\uff0c\u901a\u8fc7\u5fae\u8c03\u548c\u76f4\u63a5\u53cd\u9988\u6574\u5408\u6765\u7f13\u89e3\u5176\u201c\u9ed1\u7bb1\u201d\u548c\u9759\u6001\u7279\u6027\u3002\u6211\u4eec\u901a\u8fc7\u91c7\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f25\u5408\u4e86\u7269\u54c1\u63cf\u8ff0\u4e2d\u7684\u89c6\u89c9-\u6587\u672c\u5dee\u8ddd\u3002\u8fd9\u4f7f\u5f97LLM\u80fd\u591f\u4ece\u4eba\u5de5\u7cbe\u9009\u7684\u65f6\u5c1a\u56fe\u50cf\u4e2d\u63d0\u53d6\u98ce\u683c\u548c\u989c\u8272\u7279\u5f81\uff0c\u4e3a\u4e2a\u6027\u5316\u63a8\u8350\u5960\u5b9a\u57fa\u7840\u3002LLM\u5728\u5f00\u6e90\u7684Polyvore\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u4f18\u5316\u4e86\u5176\u63a8\u8350\u65f6\u5c1a\u670d\u88c5\u7684\u80fd\u529b\u3002\u901a\u8fc7\u4f7f\u7528\u8d1f\u9762\u793a\u4f8b\u7684\u76f4\u63a5\u504f\u597d\u673a\u5236\uff0c\u589e\u5f3a\u4e86LLM\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u8fd9\u521b\u9020\u4e86\u4e00\u4e2a\u81ea\u6211\u589e\u5f3a\u7684AI\u53cd\u9988\u5faa\u73af\uff0c\u6301\u7eed\u6839\u636e\u5b63\u8282\u6027\u65f6\u5c1a\u8d8b\u52bf\u4f18\u5316\u63a8\u8350\u3002\u6211\u4eec\u7684\u6846\u67b6\u5728Polyvore\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e24\u4e2a\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff1a\u586b\u7a7a\u548c\u4e92\u8865\u7269\u54c1\u68c0\u7d22\u3002\u8fd9\u4e9b\u8bc4\u4f30\u5f3a\u8c03\u4e86\u6846\u67b6\u751f\u6210\u65f6\u5c1a\u3001\u7b26\u5408\u6f6e\u6d41\u7684\u670d\u88c5\u5efa\u8bae\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u53cd\u9988\u4e0d\u65ad\u6539\u8fdb\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7840LLM\uff0c\u521b\u9020\u4e86\u66f4\u52a0\u8fde\u8d2f\u7684\u670d\u88c5\u7ec4\u5408\u3002\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u63d0\u5347\u7a81\u663e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u5728\u901a\u8fc7\u51c6\u786e\u5efa\u8bae\u589e\u5f3a\u8d2d\u7269\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u57fa\u4e8e\u666e\u901aLLM\u7684\u670d\u88c5\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002 Najmeh Forouzandehmehr PDF N/A Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference Personalized outfit recommendation remains a complex challenge, demanding both fashion compatibility understanding and trend awareness. This paper presents a novel framework that harnesses the expressive power of large language models (LLMs) for this task, mitigating their \"black box\" and static nature through fine-tuning and direct feedback integration. We bridge the item visual-textual gap in items descriptions by employing image captioning with a Multimodal Large Language Model (MLLM). This enables the LLM to extract style and color characteristics from human-curated fashion images, forming the basis for personalized recommendations. The LLM is efficiently fine-tuned on the open-source Polyvore dataset of curated fashion images, optimizing its ability to recommend stylish outfits. A direct preference mechanism using negative examples is employed to enhance the LLM's decision-making process. This creates a self-enhancing AI feedback loop that continuously refines recommendations in line with seasonal fashion trends. Our framework is evaluated on the Polyvore dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank, and complementary item retrieval. These evaluations underline the framework's ability to generate stylish, trend-aligned outfit suggestions, continuously improving through direct feedback. The evaluation results demonstrated that our proposed framework significantly outperforms the base LLM, creating more cohesive outfits. The improved performance in these tasks underscores the proposed framework's potential to enhance the shopping experience with accurate suggestions, proving its effectiveness over the vanilla LLM based outfit generation. MAgICoRe\uff1a\u591a\u667a\u80fd\u4f53\u3001\u8fed\u4ee3\u3001\u7531\u7c97\u5230\u7ec6\u7684\u63a8\u7406\u4f18\u5316 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u805a\u5408\u7b56\u7565\u5f97\u5230\u63d0\u5347\uff0c\u5373\u751f\u6210\u591a\u4e2a\u6837\u672c\u5e76\u5728\u751f\u6210\u7684\u6837\u672c\u4e4b\u95f4\u8fdb\u884c\u6295\u7968\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f1a\u8fbe\u5230\u4e00\u4e2a\u9971\u548c\u70b9\u3002\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5373\u5229\u7528LLM\u751f\u6210\u7684\u53cd\u9988\u6765\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u3002\u7136\u800c\uff0c\u6539\u8fdb\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\uff081\uff09\u8fc7\u5ea6\u6539\u8fdb\uff1a\u5bf9\u6240\u6709\u5b9e\u4f8b\u8fdb\u884c\u5747\u5300\u6539\u8fdb\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u6821\u6b63\uff0c\u4ece\u800c\u964d\u4f4e\u6574\u4f53\u6027\u80fd\u3002\uff082\uff09\u65e0\u6cd5\u5b9a\u4f4d\u548c\u89e3\u51b3\u9519\u8bef\uff1aLLM\u5728\u81ea\u6211\u4fee\u6b63\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u8bc6\u522b\u548c\u7ea0\u6b63\u81ea\u8eab\u7684\u9519\u8bef\u3002\uff083\uff09\u6539\u8fdb\u4e0d\u8db3\uff1a\u51b3\u5b9a\u9700\u8981\u591a\u5c11\u6b21\u8fed\u4ee3\u6539\u8fdb\u5e76\u975e\u6613\u4e8b\uff0c\u8fc7\u65e9\u505c\u6b62\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u672a\u88ab\u89e3\u51b3\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MAgICoRe\uff0c\u5b83\u901a\u8fc7\u5c06\u95ee\u9898\u96be\u5ea6\u5206\u7c7b\u4e3a\u7b80\u5355\u6216\u56f0\u96be\uff0c\u4f7f\u7528\u7c97\u7c92\u5ea6\u805a\u5408\u89e3\u51b3\u7b80\u5355\u95ee\u9898\uff0c\u800c\u5bf9\u56f0\u96be\u95ee\u9898\u5219\u91c7\u7528\u7ec6\u7c92\u5ea6\u548c\u8fed\u4ee3\u7684\u591a\u4ee3\u7406\u6539\u8fdb\uff0c\u4ece\u800c\u907f\u514d\u4e86\u8fc7\u5ea6\u6539\u8fdb\u3002\u4e3a\u4e86\u63d0\u9ad8\u9519\u8bef\u5b9a\u4f4d\u80fd\u529b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5916\u90e8\u9010\u6b65\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u786e\u4fdd\u6709\u6548\u6539\u8fdb\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u5305\u542b\u4e09\u4e2a\u4ee3\u7406\u7684\u591a\u4ee3\u7406\u5faa\u73af\uff1a\u6c42\u89e3\u8005\u3001\u8bc4\u5ba1\u8005\uff08\u6839\u636e\u9010\u6b65RM\u8bc4\u5206\u751f\u6210\u9488\u5bf9\u6027\u53cd\u9988\uff09\u548c\u6539\u8fdb\u8005\uff08\u6574\u5408\u53cd\u9988\uff09\u3002\u4e3a\u4e86\u786e\u4fdd\u8db3\u591f\u7684\u6539\u8fdb\uff0c\u6211\u4eec\u91cd\u65b0\u8bc4\u4f30\u66f4\u65b0\u540e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8fed\u4ee3\u542f\u52a8\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\u8f6e\u6b21\u3002\u6211\u4eec\u5728Llama-3-8B\u548cGPT-3.5\u4e0a\u8bc4\u4f30\u4e86MAgICoRe\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u57285\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002\u5373\u4f7f\u53ea\u8fdb\u884c\u4e00\u6b21MAgICoRe\u8fed\u4ee3\uff0c\u5176\u6027\u80fd\u4e5f\u5206\u522b\u6bd4\u81ea\u4e00\u81f4\u6027\uff08Self-Consistency\uff09\u9ad8\u51fa3.4%\uff0c\u6bd4\u6700\u4f73k\uff08Best-of-k\uff09\u9ad8\u51fa3.2%\uff0c\u6bd4\u81ea\u6211\u6539\u8fdb\uff08Self-Refine\uff09\u9ad8\u51fa4.0%\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u6837\u672c\u6570\u91cf\u4e0d\u5230\u4e00\u534a\u3002\u4e0e\u57fa\u4e8e\u57fa\u7ebf\u7684\u8fed\u4ee3\u6539\u8fdb\u4e0d\u540c\uff0cMAgICoRe\u968f\u7740\u8fed\u4ee3\u6b21\u6570\u7684\u589e\u52a0\u7ee7\u7eed\u6539\u8fdb\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u6d88\u878d\u5b9e\u9a8c\u7a81\u663e\u4e86MAgICoRe\u7684RM\u548c\u591a\u4ee3\u7406\u901a\u4fe1\u7684\u91cd\u8981\u6027\u3002 Justin Chih-Yao Chen PDF N/A MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning Large Language Models' (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples and voting among generated samples. While these improve performance, they often reach a saturation point. Refinement offers an alternative by using LLM-generated feedback to improve solution quality. However, refinement introduces 3 key challenges: (1) Excessive refinement: Uniformly refining all instances can over-correct and reduce the overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, which avoids excessive refinement by categorizing problem difficulty as easy or hard, solving easy problems with coarse-grained aggregation and hard ones with fine-grained and iterative multi-agent refinement. To improve error localization, we incorporate external step-wise reward model (RM) scores. Moreover, to ensure effective refinement, we employ a multi-agent loop with three agents: Solver, Reviewer (which generates targeted feedback based on step-wise RM scores), and the Refiner (which incorporates feedback). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across 5 math datasets. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than half the samples. Unlike iterative refinement with baselines, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe's RMs and multi-agent communication. GRIN\uff1a\u68af\u5ea6\u5f15\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b \u6df7\u5408\u4e13\u5bb6\uff08Mixture-of-Experts, MoE\uff09\u6a21\u578b\u7531\u4e8e\u901a\u8fc7\u4e13\u5bb6\u8def\u7531\u8fdb\u884c\u7a00\u758f\u8ba1\u7b97\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6269\u5c55\u89c4\u6a21\uff0c\u53ea\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u4e00\u5c0f\u90e8\u5206\u4e13\u5bb6\u6a21\u5757\u3002\u7136\u800c\uff0c\u7a00\u758f\u8ba1\u7b97\u5bf9\u4f20\u7edf\u7684\u8bad\u7ec3\u5b9e\u8df5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u79bb\u6563\u7684\u4e13\u5bb6\u8def\u7531\u963b\u788d\u4e86\u6807\u51c6\u7684\u53cd\u5411\u4f20\u64ad\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u800c\u8fd9\u4e9b\u90fd\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u77f3\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u8ffd\u6c42MoE\u7684\u6269\u5c55\u80fd\u529b\uff0c\u6211\u4eec\u5f15\u5165\u4e86GRIN\uff08GRadient-INformed MoE\u8bad\u7ec3\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u7a00\u758f\u68af\u5ea6\u4f30\u8ba1\u7528\u4e8e\u4e13\u5bb6\u8def\u7531\uff0c\u5e76\u914d\u7f6e\u4e86\u6a21\u578b\u5e76\u884c\u6027\u4ee5\u907f\u514d\u4ee4\u724c\u4e22\u5931\u3002\u5c06GRIN\u5e94\u7528\u4e8e\u81ea\u56de\u5f52\u8bed\u8a00\u5efa\u6a21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2atop-2 16$\\times$3.8B\u7684MoE\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u4ec5\u6fc0\u6d3b\u4e866.6B\u53c2\u6570\uff0c\u8868\u73b0\u4f18\u4e8e\u4e00\u4e2a7B\u7684\u5bc6\u96c6\u6a21\u578b\uff0c\u5e76\u4e0e\u5728\u76f8\u540c\u6570\u636e\u4e0a\u8bad\u7ec3\u768414B\u5bc6\u96c6\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002\u5728\u591a\u6837\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cGRIN\u5177\u6709\u663e\u8457\u63d0\u5347MoE\u6548\u7387\u7684\u6f5c\u529b\uff0c\u5728MMLU\u4e0a\u8fbe\u523079.4\uff0c\u5728HellaSwag\u4e0a\u8fbe\u523083.7\uff0c\u5728HumanEval\u4e0a\u8fbe\u523074.4\uff0c\u5728MATH\u4e0a\u8fbe\u523058.9\u3002 Liyuan Liu PDF N/A GRIN: GRadient-INformed MoE Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH. \u7ebf\u6027\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\uff08Linear Temporal Difference Learning\uff09\u5728\u4efb\u610f\u7279\u5f81\u4e0b\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u6027 \u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684\u65f6\u95f4\u5dee\u5206\uff08TD\uff09\u5b66\u4e60\uff0c\u7b80\u79f0\u7ebf\u6027TD\uff0c\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e00\u79cd\u7ecf\u5178\u4e14\u5f3a\u5927\u7684\u9884\u6d4b\u7b97\u6cd5\u3002\u867d\u7136\u4eba\u4eec\u666e\u904d\u7406\u89e3\u7ebf\u6027TD\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u4e00\u4e2a\u552f\u4e00\u70b9\uff0c\u4f46\u8fd9\u79cd\u6536\u655b\u4f20\u7edf\u4e0a\u9700\u8981\u5047\u8bbe\u903c\u8fd1\u5668\u4f7f\u7528\u7684\u7279\u5f81\u662f\u7ebf\u6027\u72ec\u7acb\u7684\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u7ebf\u6027\u72ec\u7acb\u5047\u8bbe\u5e76\u4e0d\u6210\u7acb\u3002\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u5728\u6ca1\u6709\u8981\u6c42\u7279\u5f81\u7ebf\u6027\u72ec\u7acb\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u7acb\u4e86\u7ebf\u6027TD\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u6027\u3002\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u5bf9\u7279\u5f81\u4e0d\u505a\u4efb\u4f55\u5047\u8bbe\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8fd1\u4f3c\u503c\u51fd\u6570\u6536\u655b\u5230\u4e00\u4e2a\u552f\u4e00\u70b9\uff0c\u5e76\u4e14\u6743\u91cd\u8fed\u4ee3\u6536\u655b\u5230\u4e00\u4e2a\u96c6\u5408\u3002\u6211\u4eec\u8fd8\u5efa\u7acb\u4e86\u4e00\u79cd\u6743\u91cd\u8fed\u4ee3\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u6982\u5ff5\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u5f15\u5165\u4efb\u4f55\u5176\u4ed6\u989d\u5916\u5047\u8bbe\uff0c\u4e5f\u4e0d\u9700\u8981\u5bf9\u7ebf\u6027TD\u7b97\u6cd5\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u3002\u6211\u4eec\u7684\u5206\u6790\u5173\u952e\u5728\u4e8e\u5bf9\u7ebf\u6027TD\u5747\u503cODE\u7684\u6709\u754c\u4e0d\u53d8\u96c6\u8fdb\u884c\u4e86\u65b0\u7684\u523b\u753b\u3002 Jiuqi Wang PDF N/A Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features Temporal difference (TD) learning with linear function approximation, abbreviated as linear TD, is a classic and powerful prediction algorithm in reinforcement learning. While it is well understood that linear TD converges almost surely to a unique point, this convergence traditionally requires the assumption that the features used by the approximator are linearly independent. However, this linear independence assumption does not hold in many practical scenarios. This work is the first to establish the almost sure convergence of linear TD without requiring linearly independent features. In fact, we do not make any assumptions on the features. We prove that the approximated value function converges to a unique point and the weight iterates converge to a set. We also establish a notion of local stability of the weight iterates. Importantly, we do not need to introduce any other additional assumptions and do not need to make any modification to the linear TD algorithm. Key to our analysis is a novel characterization of bounded invariant sets of the mean ODE of linear TD. BERT-VBD\uff1a\u8d8a\u5357\u8bed\u591a\u6587\u6863\u6458\u8981\u6846\u67b6 \u5728\u5e94\u5bf9\u591a\u6587\u6863\u6458\u8981\uff08Multi-Document Summarization, MDS\uff09\u7684\u6311\u6218\u4e2d\uff0c\u5df2\u7ecf\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6458\u8981\u6280\u672f\u3002\u7136\u800c\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u5176\u5c40\u9650\u6027\uff0c\u5355\u7eaf\u4f9d\u8d56\u5176\u4e2d\u4efb\u4f55\u4e00\u79cd\u90fd\u96be\u4ee5\u8fbe\u5230\u7406\u60f3\u6548\u679c\u3002\u4e00\u79cd\u65b0\u5174\u4e14\u6709\u524d\u666f\u7684\u7b56\u7565\u662f\u878d\u5408\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6458\u8981\u65b9\u6cd5\u7684\u534f\u540c\u4f5c\u7528\u3002\u5c3d\u7ba1\u8be5\u9886\u57df\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5173\u4e8e\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u8d8a\u5357\u8bed\u5904\u7406\u9886\u57df\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d8a\u5357\u8bedMDS\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u90e8\u5206\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u6574\u5408\u4e86\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6280\u672f\u3002\u7b2c\u4e00\u90e8\u5206\u91c7\u7528\u62bd\u53d6\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u9884\u8bad\u7ec3\u7684BERT\u7f51\u7edc\uff0c\u5229\u7528\u5b6a\u751f\u548c\u4e09\u5143\u7f51\u7edc\u7ed3\u6784\u63d0\u53d6\u6bcf\u4e2a\u6587\u6863\u4e2d\u7684\u5173\u952e\u53e5\u5b50\u3002\u7b2c\u4e8c\u90e8\u5206\u5219\u5229\u7528VBD-LLaMA2-7B-50b\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u6458\u8981\uff0c\u6700\u7ec8\u751f\u6210\u6700\u7ec8\u7684\u6458\u8981\u6587\u6863\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u5728VN-MDS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8639.6%\u7684ROUGE-2\u8bc4\u5206\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002 Tuan-Cuong Vuong PDF N/A BERT-VBD: Vietnamese Multi-Document Summarization Framework In tackling the challenge of Multi-Document Summarization (MDS), numerous methods have been proposed, spanning both extractive and abstractive summarization techniques. However, each approach has its own limitations, making it less effective to rely solely on either one. An emerging and promising strategy involves a synergistic fusion of extractive and abstractive summarization methods. Despite the plethora of studies in this domain, research on the combined methodology remains scarce, particularly in the context of Vietnamese language processing. This paper presents a novel Vietnamese MDS framework leveraging a two-component pipeline architecture that integrates extractive and abstractive techniques. The first component employs an extractive approach to identify key sentences within each document. This is achieved by a modification of the pre-trained BERT network, which derives semantically meaningful phrase embeddings using siamese and triplet network structures. The second component utilizes the VBD-LLaMA2-7B-50b model for abstractive summarization, ultimately generating the final summary document. Our proposed framework demonstrates a positive performance, attaining ROUGE-2 scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art baselines. Linguini\uff1a\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u8a00\u63a8\u7406\u57fa\u51c6 \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u4f9d\u8d56\u73b0\u6709\u7279\u5b9a\u8bed\u8a00\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6d4b\u8bd5\u6db5\u76d6\u4e86894\u4e2a\u95ee\u9898\uff0c\u5206\u4e3a160\u4e2a\u95ee\u9898\uff0c\u8de8\u8d8a75\u79cd\uff08\u5927\u591a\u6570\uff09\u6781\u5ea6\u4f4e\u8d44\u6e90\u7684\u8bed\u8a00\uff0c\u8fd9\u4e9b\u95ee\u9898\u63d0\u53d6\u81ea\u56fd\u9645\u8bed\u8a00\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u8bed\u6599\u5e93\u3002\u4e3a\u4e86\u5728\u8fd9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u6a21\u578b\u4e0d\u9700\u8981\u4e8b\u5148\u4e86\u89e3\u6d4b\u8bd5\u8bed\u8a00\u7684\u77e5\u8bc6\uff0c\u56e0\u4e3a\u89e3\u51b3\u8bed\u8a00\u96be\u9898\u6240\u9700\u7684\u6240\u6709\u4fe1\u606f\u90fd\u5448\u73b0\u5728\u4e0a\u4e0b\u6587\u4e2d\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6240\u6709\u5206\u6790\u7684\u6a21\u578b\u51c6\u786e\u7387\u90fd\u4f4e\u4e8e25%\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u8868\u73b0\u6700\u4f73\u7684\u4e13\u6709\u6a21\u578b\u51c6\u786e\u7387\u4e3a24.05%\uff0c\u800c\u8868\u73b0\u6700\u4f73\u7684\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u7387\u4e3a8.84%\u3002 Eduardo S\u00e1nchez PDF N/A Linguini: A benchmark for language-agnostic linguistic reasoning We propose a new benchmark to measure a language model's linguistic reasoning skills without relying on pre-existing language-specific knowledge. The test covers 894 questions grouped in 160 problems across 75 (mostly) extremely low-resource languages, extracted from the International Linguistic Olympiad corpus. To attain high accuracy on this benchmark, models don't need previous knowledge of the tested language, as all the information needed to solve the linguistic puzzle is presented in the context. We find that, while all analyzed models rank below 25% accuracy, there is a significant gap between open and closed models, with the best-performing proprietary model at 24.05% and the best-performing open model at 8.84%. Qwen2.5-Math\u6280\u672f\u62a5\u544a\uff1a\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u8fc8\u5411\u6570\u5b66\u4e13\u5bb6\u6a21\u578b \u5728\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u7cfb\u5217\u4e13\u95e8\u9488\u5bf9\u6570\u5b66\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff1aQwen2.5-Math \u548c Qwen2.5-Math-Instruct-1.5B/7B/72B\u3002Qwen2.5 \u7cfb\u5217\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5728\u6574\u4e2a\u6d41\u7a0b\u4e2d\u8d2f\u7a7f\u4e86\u81ea\u6211\u6539\u8fdb\u7684\u7406\u5ff5\uff0c\u4ece\u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u5230\u63a8\u7406\u9636\u6bb5\uff1a\uff081\uff09\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5229\u7528 Qwen2-Math-Instruct \u751f\u6210\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u5b66\u6570\u636e\u3002\uff082\uff09\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u6211\u4eec\u901a\u8fc7\u4ece Qwen2-Math-Instruct \u4e2d\u8fdb\u884c\u5927\u91cf\u91c7\u6837\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u3002\u8be5 RM \u968f\u540e\u5e94\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e2d\u6570\u636e\u7684\u8fed\u4ee3\u8fdb\u5316\u3002\u901a\u8fc7\u66f4\u5f3a\u7684 SFT \u6a21\u578b\uff0c\u53ef\u4ee5\u8fed\u4ee3\u8bad\u7ec3\u548c\u66f4\u65b0 RM\uff0c\u8fdb\u800c\u6307\u5bfc\u4e0b\u4e00\u8f6e SFT \u6570\u636e\u8fed\u4ee3\u3002\u5728\u6700\u7ec8\u7684 SFT \u6a21\u578b\u4e0a\uff0c\u6211\u4eec\u91c7\u7528\u7ec8\u6781 RM \u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ece\u800c\u5f97\u5230 Qwen2.5-Math-Instruct\u3002\uff083\uff09\u6b64\u5916\uff0c\u5728\u63a8\u7406\u9636\u6bb5\uff0cRM \u7528\u4e8e\u6307\u5bfc\u91c7\u6837\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002Qwen2.5-Math-Instruct \u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\uff0c\u5e76\u5177\u5907\u5148\u8fdb\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\uff08TIR\uff09\u3002\u6211\u4eec\u5728 10 \u4e2a\u4e2d\u82f1\u6587\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5982 GSM8K\u3001MATH\u3001GaoKao\u3001AMC23 \u548c AIME24\uff0c\u6db5\u76d6\u4e86\u4ece\u5c0f\u5b66\u6c34\u5e73\u5230\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u7684\u5404\u79cd\u96be\u5ea6\u3002 An Yang PDF N/A Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance.   Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems. \u4f4e\u5e27\u7387\u8bed\u97f3\u7f16\u89e3\u7801\u5668\uff1a\u4e00\u79cd\u4e13\u4e3a\u5feb\u901f\u9ad8\u8d28\u91cf\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8bbe\u8ba1\u7684\u7f16\u89e3\u7801\u5668 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u7b26\u53f7\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u97f3\u9891\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ece\u800c\u4f7f\u5f97\u8bed\u8a00\u5efa\u6a21\u6280\u672f\u80fd\u591f\u5e94\u7528\u4e8e\u97f3\u9891\u6570\u636e\u3002\u7136\u800c\uff0c\u97f3\u9891\u7f16\u89e3\u7801\u5668\u901a\u5e38\u4ee5\u9ad8\u5e27\u7387\u8fd0\u884c\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u7f13\u6162\uff0c\u5c24\u5176\u662f\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5e27\u7387\u8bed\u97f3\u7f16\u89e3\u7801\u5668\uff08LFSC\uff09\uff1a\u4e00\u79cd\u5229\u7528\u6709\u9650\u6807\u91cf\u91cf\u5316\u548c\u4e0e\u5927\u578b\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u5bf9\u6297\u8bad\u7ec3\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u80fd\u591f\u57281.89 kbps\u7684\u6bd4\u7279\u7387\u548c\u6bcf\u79d221.5\u5e27\u7684\u5e27\u7387\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u97f3\u9891\u538b\u7f29\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b0\u578b\u7f16\u89e3\u7801\u5668\u53ef\u4ee5\u4f7f\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230\u8bed\u97f3\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u7ea6\u4e09\u500d\uff0c\u540c\u65f6\u63d0\u5347\u8bed\u97f3\u7684\u53ef\u7406\u89e3\u6027\uff0c\u5e76\u4ea7\u751f\u4e0e\u4e4b\u524d\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u97f3\u8d28\u3002 Edresson Casanova PDF N/A Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models. \u66f4\u5f3a\u7684\u57fa\u7ebf\u6a21\u578b\u2014\u2014\u4f7f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e0e\u4e34\u5e8a\u6548\u7528\u76f8\u4e00\u81f4\u7684\u5173\u952e\u8981\u6c42 \u8fd1\u5e74\u6765\uff0c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7814\u7a76\u663e\u8457\u589e\u52a0\uff0c\u8fd9\u4e3b\u8981\u5f52\u529f\u4e8e\u9884\u6d4b\u5efa\u6a21\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u5728\u5c1d\u8bd5\u5c06ML\u6a21\u578b\u90e8\u7f72\u4e8e\u9ad8\u98ce\u9669\u3001\u4e34\u5e8a\u73af\u5883\u4e2d\u65f6\uff0c\u5b58\u5728\u4e00\u4e9b\u4f17\u6240\u5468\u77e5\u7684\u969c\u788d\uff0c\u5305\u62ec\u6a21\u578b\u900f\u660e\u5ea6\u4e0d\u8db3\uff08\u6216\u65e0\u6cd5\u5ba1\u8ba1\u63a8\u7406\u8fc7\u7a0b\uff09\u3001\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u6570\u636e\u6765\u6e90\u5b64\u7acb\uff0c\u4ee5\u53ca\u8861\u91cf\u6a21\u578b\u6548\u7528\u7684\u590d\u6742\u6307\u6807\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5728\u533b\u7597ML\u8bc4\u4f30\u4e2d\u7eb3\u5165\u66f4\u5f3a\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u5e2e\u52a9\u4ece\u4e1a\u8005\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u5177\u6709\u91cd\u8981\u7684\u4e0b\u6e38\u6548\u5e94\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5e38\u89c1\u7684\u505a\u6cd5\u2014\u2014\u5ffd\u7565\u57fa\u7ebf\u6a21\u578b\u6216\u4ec5\u4e0e\u5f31\u57fa\u7ebf\u6a21\u578b\uff08\u5982\u672a\u7ecf\u4f18\u5316\u7684\u7ebf\u6027\u6a21\u578b\uff09\u8fdb\u884c\u6bd4\u8f83\u2014\u2014\u63a9\u76d6\u4e86\u7814\u7a76\u6587\u732e\u4e2d\u63d0\u51fa\u7684ML\u65b9\u6cd5\u7684\u5b9e\u9645\u4ef7\u503c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e9b\u6700\u4f73\u5b9e\u8df5\uff0c\u8fd9\u4e9b\u5b9e\u8df5\u5c06\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u66f4\u6709\u6548\u5730\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7814\u7a76\u548c\u90e8\u7f72ML\u6a21\u578b\u3002 Nathan Wolfrath PDF N/A Stronger Baseline Models -- A Key Requirement for Aligning Machine Learning Research with Clinical Utility Machine Learning (ML) research has increased substantially in recent years, due to the success of predictive modeling across diverse application domains. However, well-known barriers exist when attempting to deploy ML models in high-stakes, clinical settings, including lack of model transparency (or the inability to audit the inference process), large training data requirements with siloed data sources, and complicated metrics for measuring model utility. In this work, we show empirically that including stronger baseline models in healthcare ML evaluations has important downstream effects that aid practitioners in addressing these challenges. Through a series of case studies, we find that the common practice of omitting baselines or comparing against a weak baseline model (e.g. a linear model with no optimization) obscures the value of ML methods proposed in the research literature. Using these insights, we propose some best practices that will enable practitioners to more effectively study and deploy ML models in clinical settings. \u5e15\u7d2f\u6258\u6570\u636e\u6846\u67b6\uff1a\u8fc8\u5411\u8d44\u6e90\u9ad8\u6548\u51b3\u7b56\u7684\u6b65\u9aa4\u2014\u2014\u5229\u7528\u6700\u5c0f\u53ef\u884c\u6570\u636e\uff08MVD\uff09 \u672c\u6587\u4ecb\u7ecd\u4e86\u5e15\u7d2f\u6258\u6570\u636e\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u548c\u9009\u62e9\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3001\u79fb\u52a8\u8bbe\u5907\u548c\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u7b49\u53d7\u9650\u5e73\u53f0\u4e0a\u542f\u7528\u673a\u5668\u5b66\u4e60\u5e94\u7528\u6240\u9700\u7684\u6700\u5c0f\u53ef\u884c\u6570\u636e\uff08MVD\uff09\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6218\u7565\u6027\u6570\u636e\u7f29\u51cf\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u3001\u80fd\u6e90\u3001\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u6700\u5c0f\u53ef\u884c\u6570\u636e\uff08MVD\uff09\u6765\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6027\u80fd\u3002\u5b83\u89e3\u51b3\u4e86\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u4f4e\u6548\u5b9e\u8df5\uff0c\u5982\u4f20\u611f\u5668\u8fc7\u5ea6\u914d\u7f6e\u3001\u8fc7\u5ea6\u7cbe\u786e\u548c\u4fe1\u53f7\u8fc7\u5ea6\u91c7\u6837\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u4f20\u611f\u5668\u9009\u62e9\u3001\u4fe1\u53f7\u63d0\u53d6\u548c\u4f20\u8f93\u4ee5\u53ca\u6570\u636e\u8868\u793a\u3002\u5b9e\u9a8c\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u964d\u91c7\u6837\u3001\u91cf\u5316\u548c\u622a\u65ad\u540e\u6709\u6548\u58f0\u5b66\u6570\u636e\u7279\u5f81\u5316\u7684\u6548\u679c\uff0c\u4ee5\u6a21\u62df\u964d\u4f4e\u4fdd\u771f\u5ea6\u7684\u4f20\u611f\u5668\u4ee5\u53ca\u7f51\u7edc\u548c\u5b58\u50a8\u7ea6\u675f\uff1b\u7ed3\u679c\u663e\u793a\uff0c\u5728\u91c7\u6837\u7387\u964d\u4f4e75%\u3001\u4f4d\u6df1\u548c\u526a\u8f91\u957f\u5ea6\u51cf\u5c1150%\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u53ef\u4ee5\u4fdd\u6301\u572895%\u4ee5\u4e0a\uff0c\u8fd9\u8f6c\u5316\u4e3a\u663e\u8457\u7684\u6210\u672c\u548c\u8d44\u6e90\u51cf\u5c11\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u53d7\u9650\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u5f00\u53d1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u672c\u6587\u8fd8\u8ba8\u8bba\u4e86\u8be5\u6846\u67b6\u7684\u66f4\u5e7f\u6cdb\u5f71\u54cd\uff0c\u5305\u62ec\u5728\u519c\u4e1a\u3001\u4ea4\u901a\u548c\u5236\u9020\u4e1a\u7b49\u9886\u57df\u63a8\u5e7f\u5148\u8fdb\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u7684\u83b7\u53d6\u5e76\u500d\u589e\u5176\u6548\u76ca\u3002 Tashfain Ahmed PDF N/A Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD) This paper introduces the Pareto Data Framework, an approach for identifying and selecting the Minimum Viable Data (MVD) required for enabling machine learning applications on constrained platforms such as embedded systems, mobile devices, and Internet of Things (IoT) devices. We demonstrate that strategic data reduction can maintain high performance while significantly reducing bandwidth, energy, computation, and storage costs. The framework identifies Minimum Viable Data (MVD) to optimize efficiency across resource-constrained environments without sacrificing performance. It addresses common inefficient practices in an IoT application such as overprovisioning of sensors and overprecision, and oversampling of signals, proposing scalable solutions for optimal sensor selection, signal extraction and transmission, and data representation. An experimental methodology demonstrates effective acoustic data characterization after downsampling, quantization, and truncation to simulate reduced-fidelity sensors and network and storage constraints; results shows that performance can be maintained up to 95\\% with sample rates reduced by 75\\% and bit depths and clip length reduced by 50\\% which translates into substantial cost and resource reduction. These findings have implications on the design and development of constrained systems. The paper also discusses broader implications of the framework, including the potential to democratize advanced AI technologies across IoT applications and sectors such as agriculture, transportation, and manufacturing to improve access and multiply the benefits of data-driven insights. \u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5fc3\u7406\u6d4b\u91cf\u6cd5\u8bc4\u4f30\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4ef7\u503c\u89c2 \u4eba\u7c7b\u4ef7\u503c\u89c2\u53ca\u5176\u8861\u91cf\u4e00\u76f4\u662f\u8de8\u5b66\u79d1\u7684\u957f\u671f\u7814\u7a76\u8bfe\u9898\u3002\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u518d\u6b21\u6fc0\u53d1\u4e86\u5bf9\u6b64\u9886\u57df\u7684\u5174\u8da3\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u4ef7\u503c\u8861\u91cf\u7684\u5de5\u5177\u548c\u5bf9\u8c61\u5d2d\u9732\u5934\u89d2\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u57fa\u4e8eLLM\u7684\u751f\u6210\u5fc3\u7406\u8ba1\u91cf\u5b66\uff08GPV\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4ef7\u503c\u8861\u91cf\u8303\u5f0f\uff0c\u5176\u7406\u8bba\u57fa\u7840\u5728\u4e8e\u6587\u672c\u63ed\u793a\u7684\u9009\u62e9\u6027\u611f\u77e5\u3002\u6211\u4eec\u9996\u5148\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u611f\u77e5\u5c42\u9762\u4ef7\u503c\u8861\u91cf\uff0c\u5e76\u9a8c\u8bc1LLM\u5c06\u6587\u672c\u89e3\u6790\u4e3a\u611f\u77e5\u7684\u80fd\u529b\uff0c\u8fd9\u6784\u6210\u4e86GPV\u6d41\u7a0b\u7684\u6838\u5fc3\u3002\u5c06GPV\u5e94\u7528\u4e8e\u4eba\u7c7b\u64b0\u5199\u7684\u535a\u5ba2\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5176\u7a33\u5b9a\u6027\u3001\u6709\u6548\u6027\u4ee5\u53ca\u4f18\u4e8e\u5148\u524d\u5fc3\u7406\u5de5\u5177\u7684\u4f18\u52bf\u3002\u968f\u540e\uff0c\u5c06GPV\u6269\u5c55\u5230LLM\u7684\u4ef7\u503c\u8861\u91cf\uff0c\u6211\u4eec\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u63a8\u8fdb\u4e86\u5f53\u524d\u7684\u6280\u672f\u6c34\u5e73\uff1a1\uff09\u4e00\u79cd\u5fc3\u7406\u8ba1\u91cf\u5b66\u65b9\u6cd5\uff0c\u57fa\u4e8eLLM\u7684\u53ef\u6269\u5c55\u548c\u81ea\u7531\u5f62\u5f0f\u8f93\u51fa\u8861\u91cf\u5176\u4ef7\u503c\uff0c\u5b9e\u73b0\u60c5\u5883\u7279\u5b9a\u7684\u8861\u91cf\uff1b2\uff09\u5bf9\u8861\u91cf\u8303\u5f0f\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u6307\u51fa\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u54cd\u5e94\u504f\u5dee\uff1b3\uff09\u5c1d\u8bd5\u8fde\u63a5LLM\u7684\u4ef7\u503c\u4e0e\u5176\u5b89\u5168\u6027\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4ef7\u503c\u4f53\u7cfb\u7684\u9884\u6d4b\u80fd\u529b\u4ee5\u53ca\u5404\u79cd\u4ef7\u503c\u5bf9LLM\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8de8\u5b66\u79d1\u7684\u52aa\u529b\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5229\u7528AI\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5fc3\u7406\u8ba1\u91cf\u5b66\u7684\u53d1\u5c55\uff0c\u5e76\u901a\u8fc7\u5fc3\u7406\u8ba1\u91cf\u5b66\u5b9e\u73b0\u4ef7\u503c\u4e00\u81f4\u7684\u4eba\u5de5\u667a\u80fd\u3002 Haoran Ye PDF N/A Measuring Human and AI Values based on Generative Psychometrics with Large Language Models Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. We begin by fine-tuning an LLM for accurate perception-level value measurement and verifying the capability of LLMs to parse texts into perceptions, forming the core of the GPV pipeline. Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for value-aligned AI. FedLF\uff1a\u8054\u90a6\u957f\u5c3e\u5b66\u4e60\u4e2d\u7684\u81ea\u9002\u5e94Logit\u8c03\u6574\u4e0e\u7279\u5f81\u4f18\u5316 \u8054\u90a6\u5b66\u4e60\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u8303\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5206\u5e03\u5728\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u6570\u636e\u96c6\u4e0d\u53ef\u907f\u514d\u5730\u662f\u5f02\u6784\u7684\uff0c\u5e76\u4e14\u5982\u679c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u4ee5\u5168\u5c40\u805a\u5408\uff0c\u5b83\u4eec\u5f80\u5f80\u5448\u73b0\u957f\u5c3e\u5206\u5e03\uff0c\u8fd9\u6781\u5927\u5730\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f20\u7edf\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u89e3\u51b3\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u6570\u636e\u5f02\u6784\u6027\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u5168\u5c40\u957f\u5c3e\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u504f\u5dee\u73b0\u8c61\u3002\u8fd9\u5bfc\u81f4\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u504f\u5411\u4e8e\u5934\u90e8\u7c7b\u522b\uff0c\u800c\u5ffd\u89c6\u4e86\u540c\u6837\u91cd\u8981\u7684\u5c3e\u90e8\u7c7b\u522b\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u5168\u9762\u8003\u8651\u7c7b\u522b\u7684\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5FedLF\uff0c\u5728\u672c\u5730\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e86\u4e09\u4e2a\u6539\u8fdb\uff1a\u81ea\u9002\u5e94logit\u8c03\u6574\u3001\u8fde\u7eed\u7c7b\u522b\u4e2d\u5fc3\u4f18\u5316\u548c\u7279\u5f81\u53bb\u76f8\u5173\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u4e03\u79cd\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u6570\u636e\u5f02\u6784\u6027\u548c\u957f\u5c3e\u5206\u5e03\u7684\u6700\u65b0\u65b9\u6cd5\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6CIFAR-10-LT\u548cCIFAR-100-LT\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u7f13\u89e3\u4e86\u7531\u4e8e\u6570\u636e\u5f02\u6784\u6027\u548c\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/18sym/FedLF\u83b7\u53d6\u3002 Xiuhua Lu PDF N/A FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated Long-Tailed Learning Federated learning offers a paradigm to the challenge of preserving privacy in distributed machine learning. However, datasets distributed across each client in the real world are inevitably heterogeneous, and if the datasets can be globally aggregated, they tend to be long-tailed distributed, which greatly affects the performance of the model. The traditional approach to federated learning primarily addresses the heterogeneity of data among clients, yet it fails to address the phenomenon of class-wise bias in global long-tailed data. This results in the trained model focusing on the head classes while neglecting the equally important tail classes. Consequently, it is essential to develop a methodology that considers classes holistically. To address the above problems, we propose a new method FedLF, which introduces three modifications in the local training phase: adaptive logit adjustment, continuous class centred optimization, and feature decorrelation. We compare seven state-of-the-art methods with varying degrees of data heterogeneity and long-tailed distribution. Extensive experiments on benchmark datasets CIFAR-10-LT and CIFAR-100-LT demonstrate that our approach effectively mitigates the problem of model performance degradation due to data heterogeneity and long-tailed distribution. our code is available at https://github.com/18sym/FedLF. \u5bf9\u79f0\u6027\u589e\u5f3a\u5b66\u4e60\uff1a\u4e00\u79cd\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u9c81\u68d2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6846\u67b6 \u672c\u6587\u7a3f\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u9ad8\u9636\u5bf9\u79f0\u6027\u548c\u8303\u7574\u8bba\u878d\u5165\u673a\u5668\u5b66\u4e60\u4e2d\u3002\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u5305\u62ec\u8d85\u5bf9\u79f0\u8303\u7574\u548c\u51fd\u5b50\u8868\u793a\uff0c\u4ee5\u6a21\u62df\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7684\u590d\u6742\u53d8\u6362\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec\u8bbe\u8ba1\u5bf9\u79f0\u4e30\u5bcc\u7684\u5b66\u4e60\u6a21\u578b\uff0c\u5f00\u53d1\u5229\u7528\u8303\u7574\u5bf9\u79f0\u7684\u5148\u8fdb\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u53ca\u5bf9\u5176\u5728\u6a21\u578b\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6536\u655b\u6027\u65b9\u9762\u5f71\u54cd\u7684\u7406\u8bba\u5206\u6790\u3002\u901a\u8fc7\u4e25\u683c\u7684\u8bc1\u660e\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5f15\u5165\u9ad8\u7ef4\u8303\u7574\u7ed3\u6784\u4e0d\u4ec5\u589e\u5f3a\u4e86\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd8\u63d0\u5347\u4e86\u5176\u5b9e\u9645\u80fd\u529b\uff0c\u4e3a\u7814\u7a76\u548c\u521b\u65b0\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002 Ronald Katende PDF N/A Symmetry-Enriched Learning: A Category-Theoretic Framework for Robust Machine Learning Models This manuscript presents a novel framework that integrates higher-order symmetries and category theory into machine learning. We introduce new mathematical constructs, including hyper-symmetry categories and functorial representations, to model complex transformations within learning algorithms. Our contributions include the design of symmetry-enriched learning models, the development of advanced optimization techniques leveraging categorical symmetries, and the theoretical analysis of their implications for model robustness, generalization, and convergence. Through rigorous proofs and practical applications, we demonstrate that incorporating higher-dimensional categorical structures enhances both the theoretical foundations and practical capabilities of modern machine learning algorithms, opening new directions for research and innovation. \u5927\u89c4\u6a21\u6280\u80fd\u5339\u914d\uff1a\u81ea\u7531\u804c\u4e1a\u8005\u4e0e\u9879\u76ee\u7684\u9ad8\u6548\u591a\u8bed\u8a00\u5019\u9009\u4eba\u68c0\u7d22 \u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u627e\u5230\u5de5\u4f5c\u63d0\u6848\u4e0e\u81ea\u7531\u804c\u4e1a\u8005\u4e4b\u95f4\u7684\u5b8c\u7f8e\u5339\u914d\u5e76\u975e\u6613\u4e8b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u68c0\u7d22\u5668\u67b6\u6784\uff0c\u4e13\u95e8\u89e3\u51b3\u8fd9\u4e00\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u6765\u7f16\u7801\u9879\u76ee\u63cf\u8ff0\u548c\u81ea\u7531\u804c\u4e1a\u8005\u6863\u6848\u3002\u8fd9\u4e9b\u6a21\u578b\u4f5c\u4e3a\u5b9a\u5236\u53d8\u538b\u5668\u67b6\u6784\u7684\u9aa8\u5e72\uff0c\u65e8\u5728\u4fdd\u6301\u6863\u6848\u548c\u9879\u76ee\u7684\u7ed3\u6784\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5386\u53f2\u6570\u636e\u4e0a\u7684\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u591a\u9879\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6355\u6349\u6280\u80fd\u5339\u914d\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u4fc3\u8fdb\u9ad8\u6548\u7684\u5339\u914d\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002 Warren Jouanneau PDF N/A Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval Finding the perfect match between a job proposal and a set of freelancers is not an easy task to perform at scale, especially in multiple languages. In this paper, we propose a novel neural retriever architecture that tackles this problem in a multilingual setting. Our method encodes project descriptions and freelancer profiles by leveraging pre-trained multilingual language models. The latter are used as backbone for a custom transformer architecture that aims to keep the structure of the profiles and project. This model is trained with a contrastive loss on historical data. Thanks to several experiments, we show that this approach effectively captures skill matching similarity and facilitates efficient matching, outperforming traditional methods. \u5143\u7d20\u987a\u5e8f\u5bf9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6027\u80fd\u7684\u5f71\u54cd \u4eba\u4eec\u5bf9\u80fd\u591f\u5728\u7f51\u7edc\u6216\u684c\u9762\u7b49\u865a\u62df\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5174\u8da3\u6fc0\u589e\u3002\u4e3a\u4e86\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u4ee3\u7406\u4ece\u6709\u5173\u5404\u79cd\u5143\u7d20\uff08\u4f8b\u5982\u6309\u94ae\u3001\u6587\u672c\u6216\u56fe\u50cf\uff09\u7684\u4fe1\u606f\u4e2d\u53d7\u76ca\u3002\u54ea\u4e9b\u5143\u7d20\u5c5e\u6027\u5bf9\u4ee3\u7406\u6027\u80fd\u5f71\u54cd\u6700\u5927\u4ecd\u4e0d\u6e05\u695a\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u63d0\u4f9b\u56fe\u5f62\u8868\u793a\uff08\u5373\u50cf\u7d20\uff09\u7684\u73af\u5883\u4e2d\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5143\u7d20\u5448\u73b0\u7ed9\u8bed\u8a00\u6a21\u578b\u7684\u987a\u5e8f\u51fa\u4e4e\u610f\u6599\u5730\u5177\u6709\u5f71\u54cd\u529b\u2014\u2014\u5728\u7f51\u9875\u4e2d\u968f\u673a\u5316\u5143\u7d20\u987a\u5e8f\u4f1a\u964d\u4f4e\u4ee3\u7406\u6027\u80fd\uff0c\u76f8\u5f53\u4e8e\u4ece\u4ee3\u7406\u7684\u72b6\u6001\u8868\u793a\u4e2d\u79fb\u9664\u6240\u6709\u53ef\u89c1\u6587\u672c\u3002\u867d\u7136\u7f51\u9875\u63d0\u4f9b\u4e86\u5143\u7d20\u7684\u5206\u5c42\u6392\u5e8f\uff0c\u4f46\u4ece\u50cf\u7d20\u76f4\u63a5\u89e3\u6790\u5143\u7d20\u65f6\u6ca1\u6709\u8fd9\u6837\u7684\u6392\u5e8f\u3002\u6b64\u5916\uff0c\u968f\u7740\u4efb\u52a1\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u6a21\u578b\u66f4\u52a0\u5148\u8fdb\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\u6392\u5e8f\u7684\u5f71\u54cd\u589e\u52a0\u3002\u627e\u5230\u6709\u6548\u7684\u6392\u5e8f\u5e76\u975e\u6613\u4e8b\u3002\u6211\u4eec\u7814\u7a76\u4e86\u5728\u7f51\u7edc\u548c\u684c\u9762\u73af\u5883\u4e2d\u5404\u79cd\u5143\u7d20\u6392\u5e8f\u65b9\u6cd5\u7684\u5f71\u54cd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u964d\u7ef4\u4e3a\u4ec5\u50cf\u7d20\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u6392\u5e8f\u3002\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2aUI\u5143\u7d20\u68c0\u6d4b\u6a21\u578b\uff0c\u4ece\u50cf\u7d20\u4e2d\u63d0\u53d6\u5143\u7d20\uff0c\u5e76\u5c06\u6211\u4eec\u7684\u53d1\u73b0\u5e94\u7528\u4e8e\u4e00\u4e2a\u4ee3\u7406\u57fa\u51c6\u2014\u2014OmniACT\u2014\u2014\u6211\u4eec\u53ea\u80fd\u8bbf\u95ee\u50cf\u7d20\u3002\u4e0e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5e73\u5747\u5b8c\u6210\u4efb\u52a1\u7684\u6570\u91cf\u662f\u5176\u4e24\u500d\u4ee5\u4e0a\u3002 Wayne Chi PDF N/A The Impact of Element Ordering on LM Agent Performance There has been a surge of interest in language model agents that can navigate virtual environments such as the web or desktop. To navigate such environments, agents benefit from information on the various elements (e.g., buttons, text, or images) present. It remains unclear which element attributes have the greatest impact on agent performance, especially in environments that only provide a graphical representation (i.e., pixels). Here we find that the ordering in which elements are presented to the language model is surprisingly impactful--randomizing element ordering in a webpage degrades agent performance comparably to removing all visible text from an agent's state representation. While a webpage provides a hierarchical ordering of elements, there is no such ordering when parsing elements directly from pixels. Moreover, as tasks become more challenging and models more sophisticated, our experiments suggest that the impact of ordering increases. Finding an effective ordering is non-trivial. We investigate the impact of various element ordering methods in web and desktop environments. We find that dimensionality reduction provides a viable ordering for pixel-only environments. We train a UI element detection model to derive elements from pixels and apply our findings to an agent benchmark--OmniACT--where we only have access to pixels. Our method completes more than two times as many tasks on average relative to the previous state-of-the-art. \u9762\u5411\u53ef\u89e3\u91ca\u7684\u7ec8\u672b\u671f\u80be\u75c5\uff08ESRD\uff09\u9884\u6d4b\uff1a\u5229\u7528\u884c\u653f\u7d22\u8d54\u6570\u636e\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f \u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u884c\u653f\u7d22\u8d54\u6570\u636e\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u9884\u6d4b\u6162\u6027\u80be\u75c5\uff08CKD\uff09\u5411\u7ec8\u672b\u671f\u80be\u75c5\uff08ESRD\uff09\u8fdb\u5c55\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5206\u6790\u4e86\u4e00\u5bb6\u4e3b\u8981\u5065\u5eb7\u4fdd\u9669\u673a\u6784\u63d0\u4f9b\u7684\u5168\u976210\u5e74\u6570\u636e\u96c6\uff0c\u91c7\u7528\u968f\u673a\u68ee\u6797\u3001XGBoost\u7b49\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ee5\u53ca\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u9488\u5bf9\u591a\u4e2a\u89c2\u5bdf\u7a97\u53e3\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLSTM\u6a21\u578b\u572824\u4e2a\u6708\u89c2\u5bdf\u7a97\u53e3\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u5728\u9884\u6d4bESRD\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5e94\u7528SHapley\u52a0\u6027\u89e3\u91ca\uff08SHAP\uff09\u5206\u6790\u6765\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6df1\u5165\u4e86\u89e3\u4e2a\u4f53\u7279\u5f81\u5bf9\u4e2a\u4f53\u60a3\u8005\u9884\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5229\u7528\u884c\u653f\u7d22\u8d54\u6570\u636e\u8fdb\u884cCKD\u7ba1\u7406\u53ca\u9884\u6d4bESRD\u8fdb\u5c55\u7684\u4ef7\u503c\u3002 Yubo Li PDF N/A Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques This study explores the potential of utilizing administrative claims data, combined with advanced machine learning and deep learning techniques, to predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal Disease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major health insurance organization to develop prediction models for multiple observation windows using traditional machine learning methods such as Random Forest and XGBoost as well as deep learning approaches such as Long Short-Term Memory (LSTM) networks. Our findings demonstrate that the LSTM model, particularly with a 24-month observation window, exhibits superior performance in predicting ESRD progression, outperforming existing models in the literature. We further apply SHapley Additive exPlanations (SHAP) analysis to enhance interpretability, providing insights into the impact of individual features on predictions at the individual patient level. This study underscores the value of leveraging administrative claims data for CKD management and predicting ESRD progression. \u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u663e\u5fae\u56fe\u50cf\u6062\u590d\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b \u663e\u5fae\u6210\u50cf\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u89c2\u5bdf\u7ed3\u6784\uff0c\u4ece\u800c\u63ed\u793a\u751f\u7269\u7ec4\u7ec7\u7684\u590d\u6742\u7ec6\u8282\u3002\u7136\u800c\uff0c\u56fe\u50cf\u566a\u58f0\u3001\u8367\u5149\u67d3\u6599\u7684\u5149\u6f02\u767d\u4ee5\u53ca\u751f\u7269\u6837\u672c\u5bf9\u9ad8\u5149\u5242\u91cf\u7684\u4f4e\u8010\u53d7\u6027\u7b49\u95ee\u9898\u4f9d\u7136\u5b58\u5728\uff0c\u9650\u5236\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u5b9e\u9a8c\u6301\u7eed\u65f6\u95f4\u3002\u964d\u4f4e\u6fc0\u5149\u5242\u91cf\u867d\u7136\u53ef\u4ee5\u5ef6\u957f\u6d4b\u91cf\u65f6\u95f4\uff0c\u4f46\u4ee3\u4ef7\u662f\u5206\u8fa8\u7387\u964d\u4f4e\u548c\u566a\u58f0\u589e\u52a0\uff0c\u8fd9\u963b\u788d\u4e86\u4e0b\u6e38\u5206\u6790\u7684\u51c6\u786e\u6027\u3002\u5728\u6b64\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f4e\u5206\u8fa8\u7387\u4fe1\u606f\u5bf9\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u6765\u9884\u6d4b\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002\u6b64\u5916\uff0cDDPM\u7684\u6982\u7387\u7279\u6027\u5141\u8bb8\u91cd\u590d\u751f\u6210\u56fe\u50cf\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4fe1\u566a\u6bd4\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u56db\u4e2a\u9ad8\u5ea6\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u4e4b\u524d\u8868\u73b0\u6700\u4f73\u7684\u65b9\u6cd5\u76f8\u5f53\u3002\u91cd\u8981\u7684\u662f\uff0c\u5c3d\u7ba1\u4e4b\u524d\u7684\u65b9\u6cd5\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6570\u636e\u96c6\u90fd\u80fd\u8fbe\u5230\u540c\u7b49\u6c34\u5e73\uff0c\u800c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u56db\u4e2a\u6570\u636e\u96c6\u4e2d\u5747\u80fd\u6301\u7eed\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5176\u5177\u6709\u9ad8\u5ea6\u7684\u666e\u9002\u6027\u3002 Pamela Osuna-Vargas PDF N/A Denoising diffusion models for high-resolution microscopy image restoration Advances in microscopy imaging enable researchers to visualize structures at the nanoscale level thereby unraveling intricate details of biological organization. However, challenges such as image noise, photobleaching of fluorophores, and low tolerability of biological samples to high light doses remain, restricting temporal resolutions and experiment durations. Reduced laser doses enable longer measurements at the cost of lower resolution and increased noise, which hinders accurate downstream analyses. Here we train a denoising diffusion probabilistic model (DDPM) to predict high-resolution images by conditioning the model on low-resolution information. Additionally, the probabilistic aspect of the DDPM allows for repeated generation of images that tend to further increase the signal-to-noise ratio. We show that our model achieves a performance that is better or similar to the previously best-performing methods, across four highly diverse datasets. Importantly, while any of the previous methods show competitive performance for some, but not all datasets, our method consistently achieves high performance across all four data sets, suggesting high generalizability. \u901a\u8fc7\u6570\u636e\u4fee\u526a\u5b9e\u73b0\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94 \u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u79fb\u9664\u7cbe\u5fc3\u6311\u9009\u7684\u6837\u672c\uff0c\u6700\u8fd1\u88ab\u8bc1\u660e\u662f\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5982\u4f55\u6700\u4f73\u5730\u9009\u62e9\u8fd9\u4e9b\u6837\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u7684\u89d2\u5ea6\u6765\u8003\u8651\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86AdaPrune\uff0c\u4e00\u79cdUDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u8bad\u7ec3\u6837\u672c\uff0c\u8bd5\u56fe\u5c06\u8bad\u7ec3\u5206\u5e03\u4e0e\u76ee\u6807\u6570\u636e\u7684\u5206\u5e03\u5bf9\u9f50\u3002\u901a\u8fc7\u91c7\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u4f5c\u4e3a\u5bf9\u9f50\u6807\u51c6\uff0c\u95ee\u9898\u53ef\u4ee5\u88ab\u7b80\u6d01\u5730\u8868\u8ff0\u5e76\u4f5c\u4e3a\u4e00\u4e2a\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u6765\u89e3\u51b3\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u771f\u5b9e\u7684\u751f\u7269\u58f0\u5b66\u4e8b\u4ef6\u68c0\u6d4b\u9886\u57df\u8f6c\u79fb\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u4f5c\u4e3a\u4e00\u79cdUDA\u65b9\u6cd5\uff0c\u6211\u4eec\u5c55\u793a\u4e86AdaPrune\u4f18\u4e8e\u76f8\u5173\u6280\u672f\uff0c\u5e76\u4e14\u4e0e\u5176\u4ed6UDA\u7b97\u6cd5\uff08\u5982CORAL\uff09\u662f\u4e92\u8865\u7684\u3002\u6211\u4eec\u5bf9MMD\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u5173\u7cfb\u7684\u5206\u6790\uff0c\u4ee5\u53cat-SNE\u56fe\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u539f\u5219\u6027\u5f3a\u4e14\u6709\u6839\u636e\u7684\u6570\u636e\u4fee\u526a\u65b9\u5f0f\u3002 Andrea Napoli PDF N/A Unsupervised Domain Adaptation Via Data Pruning The removal of carefully-selected examples from training data has recently emerged as an effective way of improving the robustness of machine learning models. However, the best way to select these examples remains an open question. In this paper, we consider the problem from the perspective of unsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDA whereby training examples are removed to attempt to align the training distribution to that of the target data. By adopting the maximum mean discrepancy (MMD) as the criterion for alignment, the problem can be neatly formulated and solved as an integer quadratic program. We evaluate our approach on a real-world domain shift task of bioacoustic event detection. As a method for UDA, we show that AdaPrune outperforms related techniques, and is complementary to other UDA algorithms such as CORAL. Our analysis of the relationship between the MMD and model accuracy, along with t-SNE plots, validate the proposed method as a principled and well-founded way of performing data pruning. \u62df\u5408\u591a\u5c42\u6b21\u56e0\u5b50\u6a21\u578b \u6211\u4eec\u7814\u7a76\u4e86\u591a\u5c42\u6b21\u56e0\u5b50\u6a21\u578b\u7684\u4e00\u4e2a\u7279\u6b8a\u60c5\u51b5\uff0c\u5176\u534f\u65b9\u5dee\u7531\u591a\u5c42\u6b21\u4f4e\u79e9\uff08MLR\uff09\u77e9\u9635\u7ed9\u51fa~\\cite{parshakova2023factor}\u3002\u6211\u4eec\u4e3a\u591a\u5c42\u6b21\u56e0\u5b50\u6a21\u578b\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5feb\u901f\u7684\u671f\u671b\u6700\u5927\u5316\uff08EM\uff09\u7b97\u6cd5\u5b9e\u73b0\uff0c\u65e8\u5728\u6700\u5927\u5316\u89c2\u6d4b\u6570\u636e\u7684\u53ef\u80fd\u6027\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u548c\u5b58\u50a8\u590d\u6742\u5ea6\u3002\u8fd9\u4e00\u6210\u679c\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u6280\u672f\u5b9e\u73b0\uff0c\u8be5\u6280\u672f\u7528\u4e8e\u8ba1\u7b97\u6b63\u5b9aMLR\u77e9\u9635\u7684\u9006\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u53ef\u9006PSD MLR\u77e9\u9635\u7684\u9006\u77e9\u9635\u4e5f\u662f\u4e00\u4e2a\u5177\u6709\u76f8\u540c\u56e0\u5b50\u7a00\u758f\u6027\u7684MLR\u77e9\u9635\uff0c\u5e76\u5229\u7528\u9012\u5f52\u7684Sherman-Morrison-Woodbury\u77e9\u9635\u6052\u7b49\u5f0f\u6765\u83b7\u5f97\u9006\u77e9\u9635\u7684\u56e0\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ee5\u7ebf\u6027\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u8ba1\u7b97\u6269\u5c55\u77e9\u9635\u7684Cholesky\u5206\u89e3\uff0c\u4ece\u800c\u5f97\u5230\u534f\u65b9\u5dee\u77e9\u9635\u4f5c\u4e3a\u5176Schur\u8865\u7801\u3002\u672c\u6587\u9644\u5e26\u4e86\u4e00\u4e2a\u5f00\u6e90\u5305\uff0c\u5b9e\u73b0\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002 Tetiana Parshakova PDF N/A Fitting Multilevel Factor Models We examine a special case of the multilevel factor model, with covariance given by multilevel low rank (MLR) matrix~\\cite{parshakova2023factor}. We develop a novel, fast implementation of the expectation-maximization (EM) algorithm, tailored for multilevel factor models, to maximize the likelihood of the observed data. This method accommodates any hierarchical structure and maintains linear time and storage complexities per iteration. This is achieved through a new efficient technique for computing the inverse of the positive definite MLR matrix. We show that the inverse of an invertible PSD MLR matrix is also an MLR matrix with the same sparsity in factors, and we use the recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of the inverse. Additionally, we present an algorithm that computes the Cholesky factorization of an expanded matrix with linear time and space complexities, yielding the covariance matrix as its Schur complement. This paper is accompanied by an open-source package that implements the proposed methods. PARAPHRASUS\uff1a\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u91ca\u4e49\u68c0\u6d4b\u6a21\u578b\u7684\u57fa\u51c6 \u5224\u65ad\u4e24\u6bb5\u6587\u672c\u662f\u5426\u4e3a\u91ca\u4e49\u7684\u4efb\u52a1\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u4e00\u76f4\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7136\u800c\uff0c\u4e3b\u6d41\u7684\u91ca\u4e49\u6982\u5ff5\u5f80\u5f80\u8fc7\u4e8e\u7b80\u5355\uff0c\u4ec5\u63d0\u4f9b\u5bf9\u5e7f\u6cdb\u91ca\u4e49\u73b0\u8c61\u7684\u6709\u9650\u89c6\u89d2\u3002\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u91ca\u4e49\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u53ef\u80fd\u4f1a\u5bf9\u5176\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u4ea7\u751f\u4e0d\u786e\u5b9a\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u53d1\u5e03\u4e86paraphrasus\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u7ef4\u5ea6\u8bc4\u4f30\u91ca\u4e49\u68c0\u6d4b\u6a21\u578b\u548c\u8fdb\u884c\u66f4\u7cbe\u7ec6\u6a21\u578b\u9009\u62e9\u7684\u57fa\u51c6\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u89c6\u89d2\u4e0b\uff0c\u91ca\u4e49\u68c0\u6d4b\u6a21\u578b\u5c55\u73b0\u51fa\u7684\u6743\u8861\u5173\u7cfb\u65e0\u6cd5\u901a\u8fc7\u5355\u4e00\u7684\u5206\u7c7b\u6570\u636e\u96c6\u6765\u6355\u6349\u3002 Andrianos Michail PDF N/A PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models The task of determining whether two texts are paraphrases has long been a challenge in NLP. However, the prevailing notion of paraphrase is often quite simplistic, offering only a limited view of the vast spectrum of paraphrase phenomena. Indeed, we find that evaluating models in a paraphrase dataset can leave uncertainty about their true semantic understanding. To alleviate this, we release paraphrasus, a benchmark designed for multi-dimensional assessment of paraphrase detection models and finer model selection. We find that paraphrase detection models under a fine-grained evaluation lens exhibit trade-offs that cannot be captured through a single classification dataset. \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u5c42\u8bad\u7ec3\u4e0e\u89e3\u7801\uff1a\u540c\u65f6\u601d\u8003\u4e0e\u8868\u8fbe \u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5408\u7406\u5730\u7406\u89e3\u548c\u751f\u6210\u4eba\u7c7b\u8868\u8fbe\uff0c\u4f46\u53ef\u80fd\u7f3a\u4e4f\u6df1\u5165\u7684\u601d\u8003\u548c\u63a8\u7406\u673a\u5236\u3002\u6700\u8fd1\u6709\u51e0\u9879\u7814\u7a76\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u601d\u8003\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6570\u90fd\u4e0d\u662f\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u6216\u8bad\u7ec3\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u53d7\u5230\u81ea\u7136\u754c\u8ba4\u77e5\u673a\u5236\u7684\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aTaS\u7684\u65b0\u578b\u6a21\u578b\u67b6\u6784\uff0c\u4f7f\u5176\u80fd\u591f\u9996\u5148\u8003\u8651\u60f3\u6cd5\uff0c\u7136\u540e\u6839\u636e\u67e5\u8be2\u8868\u8fbe\u54cd\u5e94\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u51e0\u4e2a\u6d41\u7a0b\u6765\u4ece\u63d0\u793a-\u54cd\u5e94\u6837\u672c\u4e2d\u6ce8\u91ca\u6216\u751f\u6210\u601d\u7ef4\u5185\u5bb9\uff0c\u7136\u540e\u5728\u4e2d\u95f4\u5c42\u6dfb\u52a0\u8bed\u8a00\u5934\uff0c\u4f5c\u4e3a\u601d\u8003\u5c42\u3002\u6211\u4eec\u901a\u8fc7\u601d\u7ef4\u589e\u5f3a\u7684\u6570\u636e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u6210\u529f\u5730\u4f7f\u601d\u8003\u5c42\u81ea\u52a8\u751f\u6210\u5408\u7406\u7684\u601d\u7ef4\uff0c\u5e76\u6700\u7ec8\u8f93\u51fa\u66f4\u5408\u7406\u7684\u54cd\u5e94\u3002\u5b9a\u6027\u793a\u4f8b\u548c\u5b9a\u91cf\u7ed3\u679c\u90fd\u9a8c\u8bc1\u4e86TaS\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://anonymous.4open.science/r/TadE\u83b7\u53d6\u3002 Ningyuan Xi PDF N/A Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE. Cartan\u79fb\u52a8\u6807\u67b6\u4e0e\u6570\u636e\u6d41\u5f62 \u672c\u6587\u65e8\u5728\u8fd0\u7528Cartan\u6d3b\u52a8\u6807\u67b6\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u6570\u636e\u4fe1\u606f\u5ea6\u91cf\u53ca\u5176\u5728\u6570\u636e\u70b9\u5904\u7684\u66f2\u7387\uff0c\u7814\u7a76\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u53ca\u5176\u9ece\u66fc\u7ed3\u6784\u3002\u5229\u7528\u8fd9\u4e00\u6846\u67b6\u5e76\u901a\u8fc7\u5b9e\u9a8c\uff0c\u901a\u8fc7\u6307\u51fa\u4ece\u7ed9\u5b9a\u8f93\u5165\u5bb9\u6613\u5230\u8fbe\u7684\u8f93\u51fa\u7c7b\u522b\uff0c\u7ed9\u51fa\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u54cd\u5e94\u7684\u89e3\u91ca\u3002\u8fd9\u5f3a\u8c03\u4e86\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u8f93\u51fa\u4e0e\u8f93\u5165\u51e0\u4f55\u4e4b\u95f4\u7684\u6570\u5b66\u5173\u7cfb\u5982\u4f55\u88ab\u5229\u7528\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u3002 Eliot Tron PDF N/A Cartan moving frames and the data manifolds The purpose of this paper is to employ the language of Cartan moving frames to study the geometry of the data manifolds and its Riemannian structure, via the data information metric and its curvature at data points. Using this framework and through experiments, explanations on the response of a neural network are given by pointing out the output classes that are easily reachable from a given input. This emphasizes how the proposed mathematical relationship between the output of the network and the geometry of its inputs can be exploited as an explainable artificial intelligence tool. \u6269\u5c55\u7684\u6df1\u5ea6\u5b50\u6a21\u5757\u51fd\u6570 \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u96c6\u5408\u51fd\u6570\u7c7b\u522b\uff0c\u79f0\u4e3a\u6269\u5c55\u6df1\u5ea6\u5b50\u6a21\u51fd\u6570\uff08Extended Deep Submodular Functions, EDSFs\uff09\uff0c\u8fd9\u4e9b\u51fd\u6570\u53ef\u4ee5\u7528\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u3002EDSFs \u4f5c\u4e3a\u6df1\u5ea6\u5b50\u6a21\u51fd\u6570\uff08Deep Submodular Functions, DSFs\uff09\u7684\u6269\u5c55\uff0c\u7ee7\u627f\u4e86 DSFs \u7684\u5173\u952e\u7279\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5176\u56fa\u6709\u7684\u5c40\u9650\u6027\u3002\u5df2\u77e5 DSFs \u53ea\u80fd\u8868\u793a\u5b50\u6a21\u51fd\u6570\u7684\u4e00\u4e2a\u6709\u9650\u5b50\u96c6\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u901a\u8fc7\u5bf9\u591a\u9762\u4f53\u6027\u8d28\u7684\u5206\u6790\uff0c\u6211\u4eec\u786e\u7acb\u4e86 EDSFs \u5177\u6709\u8868\u793a\u6240\u6709\u5355\u8c03\u5b50\u6a21\u51fd\u6570\u7684\u80fd\u529b\uff0c\u76f8\u8f83\u4e8e DSFs \u8fd9\u662f\u4e00\u4e2a\u663e\u8457\u7684\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cEDSFs \u53ef\u4ee5\u8868\u793a\u4efb\u4f55\u5355\u8c03\u96c6\u5408\u51fd\u6570\uff0c\u8fd9\u610f\u5473\u7740 EDSFs \u65cf\u4e0e\u6240\u6709\u5355\u8c03\u96c6\u5408\u51fd\u6570\u65cf\u662f\u7b49\u4ef7\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5f53\u8f93\u5165\u5411\u91cf\u7684\u5206\u91cf\u4e3a\u975e\u8d1f\u5b9e\u6570\u65f6\uff0cEDSFs \u4fdd\u6301\u4e86 DSFs \u56fa\u6709\u7684\u51f9\u6027\u2014\u2014\u8fd9\u662f\u67d0\u4e9b\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u8986\u76d6\u51fd\u6570\u7684\u5b66\u4e60\u4e2d\uff0cEDSFs \u7684\u5b9e\u8bc1\u6cdb\u5316\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e DSFs\u3002\u8fd9\u8868\u660e EDSFs \u5728\u8868\u793a\u548c\u5b66\u4e60\u5177\u6709\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u7684\u96c6\u5408\u51fd\u6570\u65b9\u9762\u5c55\u73b0\u51fa\u4e86\u6709\u524d\u666f\u7684\u8fdb\u5c55\u3002 Seyed Mohammad Hosseini PDF N/A Extended Deep Submodular Functions We introduce a novel category of set functions called Extended Deep Submodular functions (EDSFs), which are neural network-representable. EDSFs serve as an extension of Deep Submodular Functions (DSFs), inheriting crucial properties from DSFs while addressing innate limitations. It is known that DSFs can represent a limiting subset of submodular functions. In contrast, through an analysis of polymatroid properties, we establish that EDSFs possess the capability to represent all monotone submodular functions, a notable enhancement compared to DSFs. Furthermore, our findings demonstrate that EDSFs can represent any monotone set function, indicating the family of EDSFs is equivalent to the family of all monotone set functions. Additionally, we prove that EDSFs maintain the concavity inherent in DSFs when the components of the input vector are non-negative real numbers-an essential feature in certain combinatorial optimization problems. Through extensive experiments, we illustrate that EDSFs exhibit significantly lower empirical generalization error than DSFs in the learning of coverage functions. This suggests that EDSFs present a promising advancement in the representation and learning of set functions with improved generalization capabilities. \u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e34\u5e8a\u8bd5\u9a8c\u8868\u683c\u548c\u56fe\u8868 \u8868\u683c\u3001\u56fe\u5f62\u548c\u5217\u8868\uff08TFLs\uff09\u662f\u603b\u7ed3\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u7684\u91cd\u8981\u5de5\u5177\u3002\u5728\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u521b\u5efaTFLs\u7528\u4e8e\u62a5\u544a\u6d3b\u52a8\u901a\u5e38\u662f\u4e00\u9879\u8017\u65f6\u7684\u4efb\u52a1\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u6765\u81ea\u52a8\u751f\u6210TFLs\u7684\u65b9\u6cd5\u3002\u5229\u7528ADaM\u683c\u5f0f\u7684\u516c\u5f00\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLLMs\u80fd\u591f\u901a\u8fc7\u63d0\u793a\u6307\u4ee4\u9ad8\u6548\u751f\u6210TFLs\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u8fd9\u4e00\u9886\u57df\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u4e34\u5e8a\u8bd5\u9a8cTFL\u751f\u6210\u4ee3\u7406\u201d\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u8be5\u5e94\u7528\u7a0b\u5e8f\u80fd\u591f\u5c06\u7528\u6237\u67e5\u8be2\u4e0e\u9884\u5b9a\u4e49\u7684\u63d0\u793a\u5339\u914d\uff0c\u4ece\u800c\u751f\u6210\u5b9a\u5236\u5316\u7684\u7a0b\u5e8f\u6765\u751f\u6210\u7279\u5b9a\u7684\u9884\u5b9a\u4e49TFLs\u3002 Yumeng Yang PDF N/A Using Large Language Models to Generate Clinical Trial Tables and Figures Tables, figures, and listings (TFLs) are essential tools for summarizing clinical trial data. Creation of TFLs for reporting activities is often a time-consuming task encountered routinely during the execution of clinical trials. This study explored the use of large language models (LLMs) to automate the generation of TFLs through prompt engineering and few-shot transfer learning. Using public clinical trial data in ADaM format, our results demonstrated that LLMs can efficiently generate TFLs with prompt instructions, showcasing their potential in this domain. Furthermore, we developed a conservational agent named Clinical Trial TFL Generation Agent: An app that matches user queries to predefined prompts that produce customized programs to generate specific predefined TFLs. \u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u5904\u7406\u957f\u671f\u5b89\u5168\u6027\u548c\u4e0d\u786e\u5b9a\u6027 \u5b89\u5168\u6027\u662f\u963b\u788d\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e2d\u5e94\u7528\u7684\u5173\u952e\u95ee\u9898\u4e4b\u4e00\u3002\u5c3d\u7ba1\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u4e2d\u7684\u5927\u591a\u6570\u65b9\u6cd5\u4e0d\u9700\u8981\u4e8b\u5148\u4e86\u89e3\u7ea6\u675f\u6761\u4ef6\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\uff0c\u4ec5\u4f9d\u8d56\u6570\u636e\uff0c\u4f46\u5728\u590d\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u5b83\u4eec\u5f80\u5f80\u5341\u5206\u56f0\u96be\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5c06\u7ea6\u675f\u6761\u4ef6\u548c\u52a8\u529b\u5b66\u7684\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5b66\u4e60\u6846\u67b6\uff0c\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u76f4\u63a5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u5b66\u4e60\u7b97\u6cd5\u3002\u9057\u61be\u7684\u662f\uff0c\u5c3d\u7ba1\u901a\u5e38\u53ef\u4ee5\u83b7\u5f97\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c\u4f46\u5b89\u5168\u7ea6\u675f\u6761\u4ef6\u5177\u6709\u4efb\u52a1\u7279\u5b9a\u6027\u4e14\u96be\u4ee5\u83b7\u53d6\uff1a\u5b83\u4eec\u53ef\u80fd\u8fc7\u4e8e\u590d\u6742\u800c\u65e0\u6cd5\u8fdb\u884c\u89e3\u6790\u7f16\u7801\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u6216\u8005\u96be\u4ee5\u4e8b\u5148\u9884\u89c1\u957f\u671f\u7684\u5b89\u5168\u9700\u6c42\u3002\u672c\u6587\u901a\u8fc7\u6269\u5c55\u5b89\u5168\u63a2\u7d22\u65b9\u6cd5ATACOM\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7279\u522b\u5173\u6ce8\u4e8e\u786e\u4fdd\u957f\u671f\u5b89\u5168\u6027\u548c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6700\u7ec8\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e86\u66f4\u5b89\u5168\u7684\u884c\u4e3a\u3002 Jonas G\u00fcnster PDF N/A Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning Safety is one of the key issues preventing the deployment of reinforcement learning techniques in real-world robots. While most approaches in the Safe Reinforcement Learning area do not require prior knowledge of constraints and robot kinematics and rely solely on data, it is often difficult to deploy them in complex real-world settings. Instead, model-based approaches that incorporate prior knowledge of the constraints and dynamics into the learning framework have proven capable of deploying the learning algorithm directly on the real robot. Unfortunately, while an approximated model of the robot dynamics is often available, the safety constraints are task-specific and hard to obtain: they may be too complicated to encode analytically, too expensive to compute, or it may be difficult to envision a priori the long-term safety requirements. In this paper, we bridge this gap by extending the safe exploration method, ATACOM, with learnable constraints, with a particular focus on ensuring long-term safety and handling of uncertainty. Our approach is competitive or superior to state-of-the-art methods in final performance while maintaining safer behavior during training. \u7406\u89e3\u767e\u5ea6-ULTR\u65e5\u5fd7\u7b56\u7565\u5bf9\u53cc\u5854\u6a21\u578b\u7684\u5f71\u54cd \u5c3d\u7ba1\u53cc\u5854\u6a21\u578b\u5728\u65e0\u504f\u6392\u5e8f\uff08ULTR\uff09\u4efb\u52a1\u4e2d\u5e7f\u53d7\u6b22\u8fce\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u91cd\u5927\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u5176\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5d29\u6e83\uff1a\u65e5\u5fd7\u7b56\u7565\u6df7\u6dc6\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86\u51e0\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8bc4\u4f30\u5927\u591a\u57fa\u4e8e\u534a\u5408\u6210\u6a21\u62df\u5b9e\u9a8c\u3002\u672c\u6587\u901a\u8fc7\u5728\u6700\u5927\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u2014\u2014\u767e\u5ea6-ULTR\u4e0a\u7814\u7a76\u6df7\u6dc6\u95ee\u9898\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u7a7a\u767d\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u6709\u4e09\u70b9\uff1a1\uff09\u6211\u4eec\u5c55\u793a\u4e86\u5728\u767e\u5ea6-ULTR\u4e0a\u5b58\u5728\u6df7\u6dc6\u95ee\u9898\u7684\u6761\u4ef6\uff1b2\uff09\u6df7\u6dc6\u95ee\u9898\u5bf9\u53cc\u5854\u6a21\u578b\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff1b3\uff09\u6211\u4eec\u6307\u51fa\u4e13\u5bb6\u6807\u6ce8\u4e0e\u7528\u6237\u70b9\u51fb\u884c\u4e3a\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u6f5c\u5728\u7684\u4e0d\u5339\u914d\uff0c\u800c\u4e13\u5bb6\u6807\u6ce8\u5728ULTR\u4e2d\u88ab\u89c6\u4e3a\u9ec4\u91d1\u6807\u51c6\u3002 Morris de Haan PDF N/A Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower Models Despite the popularity of the two-tower model for unbiased learning to rank (ULTR) tasks, recent work suggests that it suffers from a major limitation that could lead to its collapse in industry applications: the problem of logging policy confounding. Several potential solutions have even been proposed; however, the evaluation of these methods was mostly conducted using semi-synthetic simulation experiments. This paper bridges the gap between theory and practice by investigating the confounding problem on the largest real-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) we show that the conditions for the confounding problem are given on Baidu-ULTR, 2) the confounding problem bears no significant effect on the two-tower model, and 3) we point to a potential mismatch between expert annotations, the golden standard in ULTR, and user click behavior. ASR\u57fa\u51c6\u6d4b\u8bd5\uff1a\u9700\u8981\u66f4\u5177\u4ee3\u8868\u6027\u7684\u5bf9\u8bdd\u6570\u636e\u96c6 \u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5728LibriSpeech\u548cFleurs\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u8868\u73b0\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u5e76\u4e0d\u80fd\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\uff0c\u8bed\u97f3\u5f80\u5f80\u662f\u65e0\u7ed3\u6784\u7684\uff0c\u5e76\u4e14\u5305\u542b\u8bf8\u5982\u505c\u987f\u3001\u6253\u65ad\u548c\u591a\u6837\u53e3\u97f3\u7b49\u4e0d\u6d41\u7545\u73b0\u8c61\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6e90\u81eaTalkBank\uff0c\u5305\u542b\u6210\u4eba\u4e4b\u95f4\u7684\u975e\u7ed3\u6784\u5316\u7535\u8bdd\u5bf9\u8bdd\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u6d4b\u8bd5\u65f6\uff0c\u5404\u79cd\u6700\u5148\u8fdb\u7684ASR\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5355\u8bcd\u9519\u8bef\u7387\u4e0e\u8bed\u97f3\u4e0d\u6d41\u7545\u73b0\u8c61\u7684\u5b58\u5728\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff0c\u8fd9\u7a81\u663e\u4e86\u5f00\u53d1\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u5bf9\u8bdd\u6027\u7684ASR\u57fa\u51c6\u6d4b\u8bd5\u7684\u8feb\u5207\u9700\u6c42\u3002 Gaurav Maheshwari PDF N/A ASR Benchmarking: Need for a More Representative Conversational Dataset Automatic Speech Recognition (ASR) systems have achieved remarkable performance on widely used benchmarks such as LibriSpeech and Fleurs. However, these benchmarks do not adequately reflect the complexities of real-world conversational environments, where speech is often unstructured and contains disfluencies such as pauses, interruptions, and diverse accents. In this study, we introduce a multilingual conversational dataset, derived from TalkBank, consisting of unstructured phone conversation between adults. Our results show a significant performance drop across various state-of-the-art ASR models when tested in conversational settings. Furthermore, we observe a correlation between Word Error Rate and the presence of speech disfluencies, highlighting the critical need for more realistic, conversational ASR benchmarks. \u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u95f4\u795e\u7ecf\u8ba1\u7b97\u548c\u5b66\u4e60\u6846\u67b6 \u672c\u6587\u63d0\u51fa\u4e86\u54c8\u5bc6\u987f\u5b66\u4e60\uff08Hamiltonian Learning\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u201c\u968f\u65f6\u95f4\u201d\u8fdb\u884c\u5b66\u4e60\uff0c\u5373\u4ee5\u5728\u7ebf\u65b9\u5f0f\u4ece\u53ef\u80fd\u65e0\u9650\u7684\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u672a\u6765\u7684\u4fe1\u606f\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7b80\u5316\u7684\u8bbe\u7f6e\u4e0a\uff0c\u5176\u4e2d\u6570\u636e\u6d41\u5177\u6709\u5df2\u77e5\u7684\u6709\u9650\u957f\u5ea6\u6216\u88ab\u5206\u5272\u6210\u66f4\u5c0f\u7684\u5e8f\u5217\uff0c\u5229\u7528\u4e86\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u4e2d\u6210\u719f\u7684\u5b66\u4e60\u7b56\u7565\u3002\u672c\u6587\u4ece\u96f6\u5f00\u59cb\u91cd\u65b0\u601d\u8003\u4e86\u968f\u65f6\u95f4\u5b66\u4e60\u7684\u96be\u9898\uff0c\u5229\u7528\u4e86\u6700\u4f18\u63a7\u5236\u7406\u8bba\u4e2d\u7684\u5de5\u5177\uff0c\u4ece\u800c\u5bf9\u795e\u7ecf\u8ba1\u7b97\u548c\u5b66\u4e60\u7684\u65f6\u5e8f\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\u3002\u54c8\u5bc6\u987f\u5b66\u4e60\u57fa\u4e8e\u5fae\u5206\u65b9\u7a0b\uff0c\u8fd9\u4e9b\u65b9\u7a0b\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a(i) \u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u5916\u90e8\u8f6f\u4ef6\u6c42\u89e3\u5668\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u79ef\u5206\uff1b(ii) \u63a8\u5e7f\u4e86\u524d\u9988\u548c\u9012\u5f52\u7f51\u7edc\u4e2d\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u8fd9\u4e00\u6210\u719f\u6982\u5ff5\uff1b(iii) \u5f00\u542f\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5b83\u5982\u4f55\u6062\u590d\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6210\u7684\u4f18\u5316\u5668\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd8\u63cf\u8ff0\u4e86\u5b83\u5982\u4f55\u7075\u6d3b\u5730\u4ece\u5b8c\u5168\u5c40\u90e8\u5207\u6362\u5230\u90e8\u5206/\u975e\u5c40\u90e8\u8ba1\u7b97\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u53ef\u80fd\u5206\u5e03\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\uff0c\u5e76\u4e14\u65e0\u9700\u5b58\u50a8\u6fc0\u6d3b\u5373\u53ef\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\u54c8\u5bc6\u987f\u5b66\u4e60\u6613\u4e8e\u5b9e\u73b0\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u4ee5\u6709\u539f\u5219\u4e14\u521b\u65b0\u7684\u65b9\u5f0f\u89e3\u51b3\u968f\u65f6\u95f4\u5b66\u4e60\u7684\u96be\u9898\u3002 Stefano Melacci PDF N/A A Unified Framework for Neural Computation and Learning Over Time This paper proposes Hamiltonian Learning, a novel unified framework for learning with neural networks \"over time\", i.e., from a possibly infinite stream of data, in an online manner, without having access to future information. Existing works focus on the simplified setting in which the stream has a known finite length or is segmented into smaller sequences, leveraging well-established learning strategies from statistical machine learning. In this paper, the problem of learning over time is rethought from scratch, leveraging tools from optimal control theory, which yield a unifying view of the temporal dynamics of neural computations and learning. Hamiltonian Learning is based on differential equations that: (i) can be integrated without the need of external software solvers; (ii) generalize the well-established notion of gradient-based learning in feed-forward and recurrent networks; (iii) open to novel perspectives. The proposed framework is showcased by experimentally proving how it can recover gradient-based learning, comparing it to out-of-the box optimizers, and describing how it is flexible enough to switch from fully-local to partially/non-local computational schemes, possibly distributed over multiple devices, and BackPropagation without storing activations. Hamiltonian Learning is easy to implement and can help researches approach in a principled and innovative manner the problem of learning over time. \u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff1a\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u5355\u7eaf\u590d\u5f62\u65b9\u6cd5 \u57fa\u4e8e\u6d88\u606f\u4f20\u9012\uff08MP\uff09\u673a\u5236\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08Graph Neural Networks\uff09\u662f\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5b83\u4eec\u672c\u8d28\u4e0a\u4ec5\u9650\u4e8e\u5efa\u6a21\u6210\u5bf9\u4ea4\u4e92\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u660e\u786e\u6355\u6349\u5177\u6709$n$\u4f53\u5173\u7cfb\u7684\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u4f5c\u4e3a\u4e00\u4e2a\u6709\u524d\u666f\u7684\u9886\u57df\u5d2d\u9732\u5934\u89d2\uff0c\u5b83\u5229\u7528\u5404\u79cd\u62d3\u6251\u57df\uff08\u5982\u5355\u7eaf\u590d\u5f62\u548c\u7ec6\u80de\u590d\u5f62\uff09\u6765\u7814\u7a76\u548c\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u65b0\u9886\u57df\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u6311\u6218\uff0c\u4f8b\u5982\u901a\u8fc7\u9ad8\u9636MP\u6709\u6548\u5efa\u6a21\u9ad8\u9636\u7ed3\u6784\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u5e76\u4e14\u6700\u8fd1\u901a\u8fc7\u5c06\u8282\u70b9\u7684\u90bb\u57df\u7f16\u7801\u4e3a\u5e8f\u5217\uff0c\u4ece\u800c\u907f\u514d\u4e86MP\u673a\u5236\uff0c\u88ab\u9002\u914d\u7528\u4e8e\u56fe\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u65e8\u5728\u4e0e\u5355\u7eaf\u590d\u5f62\u4e00\u8d77\u64cd\u4f5c\uff0c\u5229\u7528Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4f5c\u4e3a\u5176\u9aa8\u5e72\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u76f8\u90bb\u5355\u5143\u751f\u6210\u8282\u70b9\u7684\u5e8f\u5217\uff0c\u4f7f\u5f97\u6240\u6709\u9ad8\u9636\u7ed3\u6784\u4e4b\u95f4\u80fd\u591f\u76f4\u63a5\u901a\u4fe1\uff0c\u65e0\u8bba\u5176\u9636\u6570\u5982\u4f55\u3002\u6211\u4eec\u5e7f\u6cdb\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u5176\u5728\u4e0e\u4e3a\u5355\u7eaf\u590d\u5f62\u5f00\u53d1\u7684\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\u65f6\uff0c\u8868\u73b0\u51fa\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002 Marco Montagna PDF N/A Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes Graph Neural Networks based on the message-passing (MP) mechanism are a dominant approach for handling graph-structured data. However, they are inherently limited to modeling only pairwise interactions, making it difficult to explicitly capture the complexity of systems with $n$-body relations. To address this, topological deep learning has emerged as a promising field for studying and modeling higher-order interactions using various topological domains, such as simplicial and cellular complexes. While these new domains provide powerful representations, they introduce new challenges, such as effectively modeling the interactions among higher-order structures through higher-order MP. Meanwhile, structured state-space sequence models have proven to be effective for sequence modeling and have recently been adapted for graph data by encoding the neighborhood of a node as a sequence, thereby avoiding the MP mechanism. In this work, we propose a novel architecture designed to operate with simplicial complexes, utilizing the Mamba state-space model as its backbone. Our approach generates sequences for the nodes based on the neighboring cells, enabling direct communication between all higher-order structures, regardless of their rank. We extensively validate our model, demonstrating that it achieves competitive performance compared to state-of-the-art models developed for simplicial complexes. \u4fa7\u626b\u58f0\u5450\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u53d8\u6362\u5668 \u4fa7\u626b\u58f0\u5450\uff08SSS\uff09\u56fe\u50cf\u5728\u6d77\u5e95\u4eba\u9020\u7269\u4f53\u7684\u5206\u7c7b\u4e2d\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u590d\u6742\u591a\u6837\u7684\u6c34\u4e0b\u73af\u5883\u6240\u81f4\u3002\u4f20\u7edf\u4e0a\uff0c\u4e13\u5bb6\u4eec\u901a\u8fc7\u624b\u5de5\u7279\u5f81\u548c\u5e38\u89c4\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u624b\u52a8\u89e3\u8bfbSSS\u56fe\u50cf\u3002\u5c3d\u7ba1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u5728\u8fd9\u4e00\u9886\u57df\u663e\u8457\u63a8\u8fdb\u4e86\u81ea\u52a8\u5316\u5206\u7c7b\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u6d77\u5e95\u7eb9\u7406\uff08\u5982\u5ca9\u77f3\u6216\u6ce2\u7eb9\u6c99\u5e95\uff09\u65f6\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u62a5\u7387\u4e0a\u5347\u3002\u6700\u8fd1\uff0c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u901a\u8fc7\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u56fe\u50cf\u5757\u4e2d\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5c55\u73b0\u51fa\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u5728\u5904\u7406\u7a7a\u95f4\u5c42\u6b21\u7ed3\u6784\u65f6\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002\u672c\u6587\u4e25\u683c\u6bd4\u8f83\u4e86ViT\u6a21\u578b\u4e0e\u5e38\u7528\u7684CNN\u67b6\u6784\uff08\u5982ResNet\u548cConvNext\uff09\u5728SSS\u56fe\u50cf\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u79cd\u5730\u7406\u6d77\u5e95\u7c7b\u578b\uff0c\u5e76\u4e14\u5728\u4eba\u9020\u7269\u4f53\u7684\u5b58\u5728\u4e0e\u5426\u4e4b\u95f4\u4fdd\u6301\u5e73\u8861\u3002\u57fa\u4e8eViT\u7684\u6a21\u578b\u5728f1\u5206\u6570\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548c\u51c6\u786e\u6027\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5c3d\u7ba1\u4ee3\u4ef7\u662f\u66f4\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u3002CNNs\u51ed\u501f\u5176\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c55\u793a\u4e86\u66f4\u597d\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5176\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u5982\u6c34\u4e0b\u8f66\u8f86\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u63a2\u7d22ViTs\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4ee5\u53ca\u591a\u6a21\u6001\u878d\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002 BW Sheffield PDF N/A On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery Side-scan sonar (SSS) imagery presents unique challenges in the classification of man-made objects on the seafloor due to the complex and varied underwater environments. Historically, experts have manually interpreted SSS images, relying on conventional machine learning techniques with hand-crafted features. While Convolutional Neural Networks (CNNs) significantly advanced automated classification in this domain, they often fall short when dealing with diverse seafloor textures, such as rocky or ripple sand bottoms, where false positive rates may increase. Recently, Vision Transformers (ViTs) have shown potential in addressing these limitations by utilizing a self-attention mechanism to capture global information in image patches, offering more flexibility in processing spatial hierarchies. This paper rigorously compares the performance of ViT models alongside commonly used CNN architectures, such as ResNet and ConvNext, for binary classification tasks in SSS imagery. The dataset encompasses diverse geographical seafloor types and is balanced between the presence and absence of man-made objects. ViT-based models exhibit superior classification performance across f1-score, precision, recall, and accuracy metrics, although at the cost of greater computational resources. CNNs, with their inductive biases, demonstrate better computational efficiency, making them suitable for deployment in resource-constrained environments like underwater vehicles. Future research directions include exploring self-supervised learning for ViTs and multi-modal fusion to further enhance performance in challenging underwater environments. \u534f\u4f5c\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u627f\u8bfa\u4e0e\u98ce\u9669\uff1a\u5e73\u8861\u6709\u6548\u6027\u4e0e\u8bb0\u5fc6 \u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u4f7f\u7528\u6765\u81ea\u4e0d\u540c\u5730\u70b9\u548c\u7ec4\u7ec7\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u9762\u4e34\u7740\u91cd\u5927\u7684\u9690\u79c1\u548c\u6cd5\u5f8b\u6311\u6218\u3002\u63a2\u7d22\u80fd\u591f\u6709\u6548\u5229\u7528\u5206\u5e03\u4e14\u5b64\u7acb\u7684\u6570\u636e\u96c6\u4e2d\u5b9d\u8d35\u77e5\u8bc6\u7684\u534f\u4f5c\u8bad\u7ec3\u8bbe\u7f6e\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5f71\u54cd\u4ee3\u7801\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u4e2d\u534f\u4f5c\u8bad\u7ec3\u65b9\u6cd5\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u53ca\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5c55\u793a\u4e86\u6b64\u7c7b\u65b9\u6cd5\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5728\u4e0d\u540c\u534f\u4f5c\u8bad\u7ec3\u8bbe\u7f6e\uff08\u5305\u62ec\u96c6\u4e2d\u5f0f\u3001\u8054\u90a6\u5f0f\u548c\u589e\u91cf\u5f0f\u8bad\u7ec3\uff09\u4e2d\uff0c\u4e0d\u540c\u53c2\u4e0e\u8005\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u60c5\u51b5\uff0c\u7a81\u663e\u4e86\u6570\u636e\u6cc4\u9732\u7684\u6f5c\u5728\u98ce\u9669\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7801\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u662f\u5f71\u54cd\u534f\u4f5c\u8bad\u7ec3\u4ee3\u7801\u6a21\u578b\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u76f8\u6bd4\uff0c\u8054\u90a6\u5b66\u4e60\u5728\u63d0\u4f9b\u66f4\u597d\u6570\u636e\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8fd9\u5728\u751f\u6210\u4ee3\u7801\u4e2d\u8f83\u4f4e\u7684\u8bb0\u5fc6\u7387\u4e2d\u5f97\u5230\u4e86\u4f53\u73b0\u3002\u7136\u800c\uff0c\u8054\u90a6\u5b66\u4e60\u4ecd\u53ef\u80fd\u4ece\u9690\u85cf\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u4ea7\u751f\u9010\u5b57\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u8fd9\u53ef\u80fd\u8fdd\u53cd\u9690\u79c1\u6216\u7248\u6743\u3002\u6211\u4eec\u7684\u7814\u7a76\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4e86\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6548\u679c\u548c\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4e86\u5f15\u5165\u4e2a\u4f53\u53c2\u4e0e\u8005\u6570\u636e\u96c6\u7684\u987a\u5e8f\u3002\u6211\u4eec\u8fd8\u8bc6\u522b\u4e86\u8de8\u7ec4\u7ec7\u514b\u9686\u4f5c\u4e3a\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u4e00\u4e2a\u666e\u904d\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u672a\u88ab\u770b\u5230\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002\u6700\u540e\uff0c\u6211\u4eec\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4f18\u5316\u591a\u6e90\u6570\u636e\u96c6\u7684\u5efa\u8bae\uff0c\u63a8\u52a8\u8de8\u7ec4\u7ec7\u534f\u4f5c\u5411\u524d\u53d1\u5c55\u3002 Zhi Chen PDF N/A Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization In the rapidly evolving field of machine learning, training models with datasets from various locations and organizations presents significant challenges due to privacy and legal concerns. The exploration of effective collaborative training settings capable of leveraging valuable knowledge from distributed and isolated datasets is increasingly crucial. This study investigates key factors that impact the effectiveness of collaborative training methods in code next-token prediction, as well as the correctness and utility of the generated code, demonstrating the promise of such methods. Additionally, we evaluate the memorization of different participant training data across various collaborative training settings, including centralized, federated, and incremental training, highlighting their potential risks in leaking data. Our findings indicate that the size and diversity of code datasets are pivotal factors influencing the success of collaboratively trained code models. We show that federated learning achieves competitive performance compared to centralized training while offering better data protection, as evidenced by lower memorization ratios in the generated code. However, federated learning can still produce verbatim code snippets from hidden training data, potentially violating privacy or copyright. Our study further explores effectiveness and memorization patterns in incremental learning, emphasizing the sequence in which individual participant datasets are introduced. We also identify cross-organizational clones as a prevalent challenge in both centralized and federated learning scenarios. Our findings highlight the persistent risk of data leakage during inference, even when training data remains unseen. We conclude with recommendations for practitioners and researchers to optimize multisource datasets, propelling cross-organizational collaboration forward. \u8de8\u91cf\u5b50\u5316\u5b66\u5c42\u6b21\u7684\u4e00\u4f53\u5316\u57fa\u7840\u6a21\u578b\u5b66\u4e60 \u673a\u5668\u5b66\u4e60\uff08ML\uff09\u52bf\u80fd\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u7684\u91cf\u5b50\u5316\u5b66\uff08QC\uff09\u6c34\u5e73\uff0c\u800c\u9488\u5bf9\u591a\u4fdd\u771f\u5ea6\u5b66\u4e60\u7684ML\u6a21\u578b\u5c1a\u672a\u663e\u793a\u51fa\u80fd\u4e3a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5728\u6b64\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5168\u5408\u4e00\uff08AIO\uff09ANI\u6a21\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u5b66\u4e60\u4efb\u610f\u6570\u91cf\u7684QC\u6c34\u5e73\u3002\u6211\u4eec\u7684\u5168\u5408\u4e00\u5b66\u4e60\u65b9\u6cd5\u4e3a\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u4e14\u66f4\u6613\u4f7f\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u5229\u7528\u5b83\u8bad\u7ec3\u4e86AIO-ANI-UIP\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u4e0e\u534a\u7ecf\u9a8cGFN2-xTB\u548c\u6709\u673a\u5206\u5b50\u53cc\u03b6\u57fa\u7ec4\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u76f8\u5ab2\u7f8e\u3002\u6211\u4eec\u5c55\u793a\u4e86AIO-ANI\u6a21\u578b\u80fd\u591f\u8de8\u8d8a\u4ece\u534a\u7ecf\u9a8c\u5230\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u518d\u5230\u8026\u5408\u7c07\u7684\u4e0d\u540cQC\u6c34\u5e73\u8fdb\u884c\u5b66\u4e60\u3002\u6211\u4eec\u8fd8\u5229\u7528AIO\u6a21\u578b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u0394\u5b66\u4e60\u7684\u57fa\u7840\u6a21\u578b\u0394-AIO-ANI\uff0c\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u76f8\u8f83\u4e8eAIO-ANI-UIP\u6709\u6240\u63d0\u5347\u3002\u4ee3\u7801\u548c\u57fa\u7840\u6a21\u578b\u53ef\u5728https://github.com/dralgroup/aio-ani\u83b7\u53d6\uff1b\u5b83\u4eec\u5c06\u88ab\u6574\u5408\u5230\u901a\u7528\u4e14\u53ef\u66f4\u65b0\u7684AI\u589e\u5f3a\u91cf\u5b50\u529b\u5b66\uff08UAIQM\uff09\u5e93\u4e2d\uff0c\u5e76\u53ef\u901a\u8fc7MLatom\u5305\u5728\u7ebf\u4f7f\u7528\u4e8eXACS\u4e91\u8ba1\u7b97\u5e73\u53f0\uff08\u53c2\u89c1https://github.com/dralgroup/mlatom\u83b7\u53d6\u66f4\u65b0\uff09\u3002 Yuxinxin Chen PDF N/A All-in-one foundational models learning across quantum chemical levels Machine learning (ML) potentials typically target a single quantum chemical (QC) level while the ML models developed for multi-fidelity learning have not been shown to provide scalable solutions for foundational models. Here we introduce the all-in-one (AIO) ANI model architecture based on multimodal learning which can learn an arbitrary number of QC levels. Our all-in-one learning approach offers a more general and easier-to-use alternative to transfer learning. We use it to train the AIO-ANI-UIP foundational model with the generalization capability comparable to semi-empirical GFN2-xTB and DFT with a double-zeta basis set for organic molecules. We show that the AIO-ANI model can learn across different QC levels ranging from semi-empirical to density functional theory to coupled cluster. We also use AIO models to design the foundational model {\\Delta}-AIO-ANI based on {\\Delta}-learning with increased accuracy and robustness compared to AIO-ANI-UIP. The code and the foundational models are available at https://github.com/dralgroup/aio-ani; they will be integrated into the universal and updatable AI-enhanced QM (UAIQM) library and made available in the MLatom package so that they can be used online at the XACS cloud computing platform (see https://github.com/dralgroup/mlatom for updates). \u5c06\u6570\u636e\u7f6e\u4e8e\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u4e2d\u5fc3 \u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u662f\u4e00\u4e2a\u4ee4\u4eba\u5174\u594b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5b83\u5229\u7528\u9759\u6001\u6570\u636e\u96c6\u6765\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u627e\u5230\u6700\u4f18\u63a7\u5236\u7b56\u7565\u3002\u5c3d\u7ba1\u8be5\u9886\u57df\u672c\u8d28\u4e0a\u662f\u4ee5\u6570\u636e\u4e3a\u9a71\u52a8\u7684\uff0c\u4f46\u8fc4\u4eca\u4e3a\u6b62\u7684\u52aa\u529b\u5728\u8ffd\u6c42\u6700\u5148\u8fdb\u6210\u679c\u7684\u8fc7\u7a0b\u4e2d\u5ffd\u89c6\u4e86\u6570\u636e\u7684\u4f5c\u7528\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7\u6587\u732e\u8c03\u67e5\u6765\u8bc1\u5b9e\u8fd9\u4e00\u89c2\u70b9\uff0c\u5c55\u793a\u4e86\u5927\u591a\u6570\u7814\u7a76\u5982\u4f55\u5728\u6ca1\u6709\u4e00\u81f4\u65b9\u6cd5\u8bba\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u5bf9\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u7279\u5f81\u63d0\u4f9b\u7684\u4fe1\u606f\u975e\u5e38\u6709\u9650\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u663e\u8457\u7684\u4f8b\u5b50\u6765\u8bf4\u660e\u5ffd\u89c6\u6570\u636e\u672c\u8d28\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u4f8b\u5b50\u5c55\u793a\u4e86\u7b97\u6cd5\u6027\u80fd\u4e0e\u6240\u7528\u6570\u636e\u96c6\u7684\u7d27\u5bc6\u8026\u5408\u5173\u7cfb\uff0c\u8fd9\u8981\u6c42\u8be5\u9886\u57df\u5185\u7684\u5b9e\u9a8c\u9700\u8981\u4e00\u4e2a\u5171\u540c\u7684\u57fa\u7840\u3002\u4f5c\u4e3a\u56de\u5e94\uff0c\u6211\u4eec\u5728\u6539\u8fdb\u79bb\u7ebfMARL\u4e2d\u7684\u6570\u636e\u4f7f\u7528\u548c\u6570\u636e\u610f\u8bc6\u65b9\u9762\u8fc8\u51fa\u4e86\u4e00\u5927\u6b65\uff0c\u505a\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a\uff081\uff09\u751f\u6210\u65b0\u6570\u636e\u96c6\u7684\u660e\u786e\u6307\u5357\uff1b\uff082\uff09\u5bf980\u591a\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u7684\u6807\u51c6\u5316\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6258\u7ba1\u5728\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u5b58\u50a8\u5e93\u4e2d\uff0c\u91c7\u7528\u4e00\u81f4\u7684\u5b58\u50a8\u683c\u5f0f\u548c\u6613\u4e8e\u4f7f\u7528\u7684API\uff1b\uff083\uff09\u4e00\u5957\u5206\u6790\u5de5\u5177\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u6570\u636e\u96c6\uff0c\u4ece\u800c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u53d1\u5c55\u3002 Claude Formanek PDF N/A Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development. \u201c\u5b83\u5728\u6280\u672f\u4e0a\u53ef\u80fd\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u5bf9\u6211\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u6beb\u65e0\u5e2e\u52a9\u201d\uff1a\u65b0\u95fb\u884c\u4e1a\u4e2d\u56f4\u7ed5\u4eba\u5de5\u667a\u80fd\u7684\u8de8\u804c\u80fd\u534f\u4f5c\u7684\u5b9e\u8df5\u3001\u6311\u6218\u4e0e\u673a\u9047 \u8fd1\u671f\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u65b0\u95fb\u673a\u6784\u5c06\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u878d\u5165\u5176\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bfc\u81f4\u66f4\u591a\u7684AI\u6280\u672f\u4eba\u5458\u548c\u6570\u636e\u5de5\u4f5c\u8005\u6d8c\u5165\u65b0\u95fb\u884c\u4e1a\u3002\u8fd9\u4fc3\u6210\u4e86\u8fd9\u4e9b\u4e13\u4e1a\u4eba\u5458\u4e0e\u8bb0\u8005\u4e4b\u95f4\u7684\u8de8\u804c\u80fd\u5408\u4f5c\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0eAI\u76f8\u5173\u7684\u89d2\u8272\u8fdb\u5165\u65b0\u95fb\u884c\u4e1a\u7684\u5f71\u54cd\uff0c\u4f46\u5173\u4e8eAI\u4e13\u4e1a\u4eba\u5458\u4e0e\u8bb0\u8005\u4e4b\u95f4\u8de8\u804c\u80fd\u5408\u4f5c\u5982\u4f55\u5c55\u5f00\u7684\u7814\u7a76\u5c1a\u663e\u4e0d\u8db3\u3002\u901a\u8fc7\u5bf917\u540d\u8bb0\u8005\u30016\u540dAI\u6280\u672f\u4eba\u5458\u4ee5\u53ca3\u540d\u6765\u81ea\u9886\u5148\u65b0\u95fb\u673a\u6784\u7684\u5177\u6709\u8de8\u804c\u80fd\u7ecf\u9a8c\u7684AI\u5de5\u4f5c\u4eba\u5458\u8fdb\u884c\u8bbf\u8c08\uff0c\u6211\u4eec\u8c03\u67e5\u4e86\u5f53\u4eca\u65b0\u95fb\u884c\u4e1a\u4e2d\u56f4\u7ed5AI\u7684\u8de8\u804c\u80fd\u5408\u4f5c\u7684\u73b0\u72b6\u3001\u6311\u6218\u548c\u673a\u9047\u3002\u9996\u5148\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8bb0\u8005\u548cAI\u4e13\u4e1a\u4eba\u5458\u5982\u4f55\u770b\u5f85\u73b0\u6709\u7684\u8de8\u5408\u4f5c\u7b56\u7565\u3002\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4e86\u8de8\u804c\u80fd\u5408\u4f5c\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u63d0\u5347\u65b0\u95fb\u884c\u4e1a\u4e2d\u672a\u6765\u56f4\u7ed5AI\u7684\u8de8\u804c\u80fd\u5408\u4f5c\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002 Qing Xiao PDF N/A \"It Might be Technically Impressive, But It's Practically Useless to Us\": Practices, Challenges, and Opportunities for Cross-Functional Collaboration around AI within the News Industry Recently, an increasing number of news organizations have integrated artificial intelligence (AI) into their workflows, leading to a further influx of AI technologists and data workers into the news industry. This has initiated cross-functional collaborations between these professionals and journalists. While prior research has explored the impact of AI-related roles entering the news industry, there is a lack of studies on how cross-functional collaboration unfolds between AI professionals and journalists. Through interviews with 17 journalists, 6 AI technologists, and 3 AI workers with cross-functional experience from leading news organizations, we investigate the current practices, challenges, and opportunities for cross-functional collaboration around AI in today's news industry. We first study how journalists and AI professionals perceive existing cross-collaboration strategies. We further explore the challenges of cross-functional collaboration and provide recommendations for enhancing future cross-functional collaboration around AI in the news industry. \u89e3\u5f00Hessian\u4e4b\u8c1c\uff1a\u4f18\u5316\u635f\u5931\u51fd\u6570\u666f\u89c2\u4e2d\u7684\u5e73\u6ed1\u6536\u655b\u7684\u5173\u952e \u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\u662f\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u65b9\u9762\uff0c\u7406\u89e3\u5176\u7279\u6027\u5bf9\u4e8e\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u6837\u672c\u91cf\u589e\u52a0\u65f6\u635f\u5931\u8868\u9762\u7684\u53d8\u5316\u60c5\u51b5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u95ee\u9898\u3002\u6211\u4eec\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u4e2d\u635f\u5931\u666f\u89c2\u7684\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u5728\u6837\u672c\u4e2d\u6dfb\u52a0\u65b0\u5bf9\u8c61\u65f6\u635f\u5931\u51fd\u6570\u503c\u5dee\u5f02\u7684\u4e0a\u754c\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u635f\u5931\u51fd\u6570\u8868\u9762\u7684\u6536\u655b\u6027\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u795e\u7ecf\u635f\u5931\u666f\u89c2\u7684\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u5bf9\u6837\u672c\u91cf\u786e\u5b9a\u6280\u672f\u7684\u53d1\u5c55\u5177\u6709\u542f\u793a\u610f\u4e49\u3002 Nikita Kiselev PDF N/A Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes The loss landscape of neural networks is a critical aspect of their training, and understanding its properties is essential for improving their performance. In this paper, we investigate how the loss surface changes when the sample size increases, a previously unexplored issue. We theoretically analyze the convergence of the loss landscape in a fully connected neural network and derive upper bounds for the difference in loss function values when adding a new object to the sample. Our empirical study confirms these results on various datasets, demonstrating the convergence of the loss function surface for image classification tasks. Our findings provide insights into the local geometry of neural loss landscapes and have implications for the development of sample size determination techniques. \u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u53d7\u9650\u571f\u58e4\u6d4b\u91cf\u5e94\u7528\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6a21\u578b\u65e0\u5173\u65b9\u6cd5 \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u5f3a\u571f\u58e4\u5c5e\u6027\u9884\u6d4b\u5efa\u6a21\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u8fd9\u662f\u63a8\u8fdb\u5730\u7edf\u8ba1\u5b66\u548c\u6570\u5b57\u571f\u58e4\u5236\u56fe\u5b9e\u8df5\u7684\u5173\u952e\u56e0\u7d20\u3002\u4e3a\u4e86\u89e3\u51b3\u571f\u58e4\u7814\u7a76\u4e2d\u5e38\u89c1\u7684\u6570\u636e\u7a00\u7f3a\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5c06\u56de\u5f52\u4efb\u52a1\u8f6c\u5316\u4e3a\u5206\u7c7b\u95ee\u9898\uff0c\u8fd9\u4e0d\u4ec5\u80fd\u591f\u751f\u6210\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u8fd8\u4f7f\u5f97\u80fd\u591f\u5e94\u7528\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u6210\u719f\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u800c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u76ee\u524d\u7684\u5730\u7edf\u8ba1\u5b66\u4e2d\u5c1a\u672a\u5f97\u5230\u5229\u7528\u3002\u4ece\u4e24\u4e2a\u5fb7\u56fd\u519c\u4e1a\u7530\u5730\u6536\u96c6\u7684\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u7ed3\u679c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6211\u4eec\u7684\u7ed3\u679c\u548c\u53d1\u73b0\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u53ef\u80fd\u63d0\u4f9b\u6bd4\u5730\u7edf\u8ba1\u5b66\u4e2d\u5e38\u7528\u6a21\u578b\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002 Viacheslav Barkov PDF N/A An Efficient Model-Agnostic Approach for Uncertainty Estimation in Data-Restricted Pedometric Applications This paper introduces a model-agnostic approach designed to enhance uncertainty estimation in the predictive modeling of soil properties, a crucial factor for advancing pedometrics and the practice of digital soil mapping. For addressing the typical challenge of data scarcity in soil studies, we present an improved technique for uncertainty estimation. This method is based on the transformation of regression tasks into classification problems, which not only allows for the production of reliable uncertainty estimates but also enables the application of established machine learning algorithms with competitive performance that have not yet been utilized in pedometrics. Empirical results from datasets collected from two German agricultural fields showcase the practical application of the proposed methodology. Our results and findings suggest that the proposed approach has the potential to provide better uncertainty estimation than the models commonly used in pedometrics. \u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5ea6\u91cf-\u8bed\u4e49\u56e0\u5b50\u56fe\u751f\u6210 \u7406\u89e3\u51e0\u4f55\u7ed3\u6784\u4e0e\u8bed\u4e49\u6982\u5ff5\u4e4b\u95f4\u7684\u5173\u7cfb\u5bf9\u4e8e\u6784\u5efa\u590d\u6742\u73af\u5883\u7684\u7cbe\u786e\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u5728\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u5c3d\u7ba1\u5e03\u5c40\u6709\u6240\u53d8\u5316\uff0c\u67d0\u4e9b\u7a7a\u95f4\u7ea6\u675f\uff08\u5982\u5e73\u9762\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff09\u4ecd\u7136\u4fdd\u6301\u4e00\u81f4\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u56feSLAM\u6846\u67b6\u4e2d\u6355\u6349\u8fd9\u4e9b\u4e0d\u53d8\u5173\u7cfb\uff0c\u901a\u8fc7\u8868\u793a\u623f\u95f4\u548c\u5899\u58c1\u7b49\u9ad8\u7ea7\u6982\u5ff5\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u5e73\u9762\u7b49\u51e0\u4f55\u5143\u7d20\u901a\u8fc7\u53ef\u4f18\u5316\u7684\u56e0\u5b50\u56fe\u76f8\u8fde\u63a5\u3002\u5df2\u6709\u591a\u79cd\u52aa\u529b\u5c1d\u8bd5\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u6982\u5ff5\u751f\u6210\u548c\u624b\u52a8\u5b9a\u4e49\u56e0\u5b50\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf-\u8bed\u4e49\u56e0\u5b50\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u5b9a\u4e49\u8bed\u4e49\u573a\u666f\u56fe\u3001\u6574\u5408\u51e0\u4f55\u4fe1\u606f\u4ee5\u53ca\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5b66\u4e60\u4e92\u8054\u56e0\u5b50\u3002\u4e00\u4e2a\u8fb9\u5206\u7c7b\u7f51\u7edc\uff08G-GNN\uff09\u5c06\u5e73\u9762\u4e4b\u95f4\u7684\u8fb9\u5206\u7c7b\u4e3a\u540c\u4e00\u623f\u95f4\u3001\u540c\u4e00\u5899\u58c1\u6216\u65e0\u7c7b\u578b\u3002\u751f\u6210\u7684\u5173\u7cfb\u88ab\u805a\u7c7b\uff0c\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u751f\u6210\u4e00\u4e2a\u623f\u95f4\u6216\u5899\u58c1\u3002\u7b2c\u4e8c\u7c7b\u7f51\u7edc\uff08F-GNN\uff09\u63a8\u65ad\u65b0\u8282\u70b9\u7684\u51e0\u4f55\u8d77\u6e90\u3002\u56e0\u5b50\u5b9a\u4e49\u91c7\u7528\u4e86\u4e0e\u751f\u6210\u8282\u70b9\u5ea6\u91cf\u5c5e\u6027\u76f8\u540c\u7684F-GNN\u3002\u6b64\u5916\uff0c\u4e0eS-Graphs+\u7b97\u6cd5\u5171\u4eab\u65b0\u7684\u56e0\u5b50\u56fe\uff0c\u6269\u5c55\u5176\u56fe\u8868\u8fbe\u80fd\u529b\u548c\u573a\u666f\u8868\u793a\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u63d0\u9ad8SLAM\u6027\u80fd\u3002\u901a\u8fc7\u5728L\u5f62\u623f\u95f4\u4e0a\u8bad\u7ec3\u7f51\u7edc\uff0c\u5c06\u73af\u5883\u7684\u590d\u6742\u6027\u589e\u52a0\u5230N\u5e73\u9762\u623f\u95f4\u3002\u8be5\u6846\u67b6\u5728\u6ca1\u6709\u6240\u9700\u590d\u6742\u5e03\u5c40\u7684\u771f\u5b9e\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5408\u6210\u548c\u6a21\u62df\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002 Jose Andres Millan-Romera PDF N/A Metric-Semantic Factor Graph Generation based on Graph Neural Networks Understanding the relationships between geometric structures and semantic concepts is crucial for building accurate models of complex environments. In indoors, certain spatial constraints, such as the relative positioning of planes, remain consistent despite variations in layout. This paper explores how these invariant relationships can be captured in a graph SLAM framework by representing high-level concepts like rooms and walls, linking them to geometric elements like planes through an optimizable factor graph. Several efforts have tackled this issue with add-hoc solutions for each concept generation and with manually-defined factors.   This paper proposes a novel method for metric-semantic factor graph generation which includes defining a semantic scene graph, integrating geometric information, and learning the interconnecting factors, all based on Graph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the edges between planes into same room, same wall or none types. The resulting relations are clustered, generating a room or wall for each cluster. A second family of networks (F-GNN) infers the geometrical origin of the new nodes. The definition of the factors employs the same F-GNN used for the metric attribute of the generated nodes. Furthermore, share the new factor graph with the S-Graphs+ algorithm, extending its graph expressiveness and scene representation with the ultimate goal of improving the SLAM performance. The complexity of the environments is increased to N-plane rooms by training the networks on L-shaped rooms. The framework is evaluated in synthetic and simulated scenarios as no real datasets of the required complex layouts are available. \u4eceLLM\u884d\u751f\u7684\u5d4c\u5165\u8868\u793a\u4e2d\u91c7\u6837\u6f5c\u5728\u6750\u6599\u5c5e\u6027\u4fe1\u606f \u6e90\u81ea\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5411\u91cf\u5d4c\u5165\u5728\u6355\u6349\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002\u6709\u8da3\u7684\u662f\uff0c\u8fd9\u4e9b\u5d4c\u5165\u53ef\u4ee5\u6574\u5408\u5230\u6750\u6599\u5d4c\u5165\u4e2d\uff0c\u53ef\u80fd\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u6750\u6599\u6027\u8d28\u9884\u6d4b\u6709\u7528\u3002\u6211\u4eec\u7814\u7a76\u4e86LLM\u884d\u751f\u7684\u5411\u91cf\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u6355\u6349\u5230\u6240\u9700\u4fe1\u606f\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6750\u6599\u6027\u8d28\u89c1\u89e3\u7684\u6f5c\u529b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1LLMs\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u53cd\u6620\u67d0\u4e9b\u6027\u8d28\u4fe1\u606f\u7684\u8868\u793a\uff0c\u4f46\u63d0\u53d6\u8fd9\u4e9b\u5d4c\u5165\u9700\u8981\u8bc6\u522b\u6700\u4f73\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u9002\u5f53\u7684\u6bd4\u8f83\u5bf9\u8c61\u3002\u5c3d\u7ba1\u5b58\u5728\u8fd9\u4e00\u9650\u5236\uff0cLLMs\u4f3c\u4e4e\u4ecd\u7136\u6709\u53ef\u80fd\u5728\u751f\u6210\u6709\u610f\u4e49\u7684\u6750\u6599\u79d1\u5b66\u8868\u793a\u65b9\u9762\u53d1\u6325\u4f5c\u7528\u3002 Luke P. J. Gilligan PDF N/A Sampling Latent Material-Property Information From LLM-Derived Embedding Representations Vector embeddings derived from large language models (LLMs) show promise in capturing latent information from the literature. Interestingly, these can be integrated into material embeddings, potentially useful for data-driven predictions of materials properties. We investigate the extent to which LLM-derived vectors capture the desired information and their potential to provide insights into material properties without additional training. Our findings indicate that, although LLMs can be used to generate representations reflecting certain property information, extracting the embeddings requires identifying the optimal contextual clues and appropriate comparators. Despite this restriction, it appears that LLMs still have the potential to be useful in generating meaningful materials-science representations. \u5408\u6210\u6570\u636e\u4f5c\u4e3a\u57fa\u51c6\u7684\u6709\u6548\u6027 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u73af\u5883\u4e2d\u63a8\u52a8\u4e86\u4e00\u7cfb\u5217\u5e94\u7528\uff0c\u5305\u62ec\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u5408\u6210\u6570\u636e\u96c6\u7684\u751f\u6210\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u53ef\u9760\u5730\u4f7f\u7528\u8fd9\u4e9b\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e86\u89e3\u5b83\u4eec\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4ee3\u8868\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\u6211\u4eec\u901a\u8fc7\u8bc4\u4f30\u4f7f\u7528LLM\u751f\u6210\u5408\u6210\u6570\u636e\u5e76\u5c06\u5176\u4f5c\u4e3a\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u57fa\u51c6\u7684\u6709\u6548\u6027\u6765\u7814\u7a76\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u5404\u79cd\u65b9\u6cd5\u5728\u7b80\u5355\u4efb\u52a1\uff08\u5982\u610f\u56fe\u5206\u7c7b\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff08\u5982\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff09\u5219\u8868\u73b0\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\uff0c\u79f0\u4e3a\u504f\u5dee\u56e0\u5b50\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f53\u540c\u4e00LLM\u65e2\u7528\u4e8e\u751f\u6210\u57fa\u51c6\u6570\u636e\u53c8\u7528\u4e8e\u6267\u884c\u4efb\u52a1\u65f6\u5f15\u5165\u7684\u504f\u5dee\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u8f83\u5c0f\u7684LLMs\u5bf9\u5176\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u8868\u73b0\u51fa\u504f\u5dee\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u5219\u6ca1\u6709\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u57fa\u51c6\u7684\u6709\u6548\u6027\u56e0\u4efb\u52a1\u800c\u5f02\uff0c\u5b9e\u8df5\u8005\u5e94\u5c3d\u53ef\u80fd\u4f9d\u8d56\u7531\u591a\u4e2a\u8f83\u5927\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u3002 Gaurav Maheshwari PDF N/A Efficacy of Synthetic Data as a Benchmark Large language models (LLMs) have enabled a range of applications in zero-shot and few-shot learning settings, including the generation of synthetic datasets for training and testing. However, to reliably use these synthetic datasets, it is essential to understand how representative they are of real-world data. We investigate this by assessing the effectiveness of generating synthetic data through LLM and using it as a benchmark for various NLP tasks. Our experiments across six datasets, and three different tasks, show that while synthetic data can effectively capture performance of various methods for simpler tasks, such as intent classification, it falls short for more complex tasks like named entity recognition. Additionally, we propose a new metric called the bias factor, which evaluates the biases introduced when the same LLM is used to both generate benchmarking data and to perform the tasks. We find that smaller LLMs exhibit biases towards their own generated data, whereas larger models do not. Overall, our findings suggest that the effectiveness of synthetic data as a benchmark varies depending on the task, and that practitioners should rely on data generated from multiple larger models whenever possible. \u4f7f\u7528\u6559\u5e08\u6307\u5bfc\u7684\u6df7\u6dc6\u7c7b\u6307\u4ee4\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u58f0\u5b66\u573a\u666f\u5206\u7c7b \u5728\u672c\u6280\u672f\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86SNTL-NTU\u56e2\u961f\u9488\u5bf92024\u5e74DCASE\u6311\u6218\u8d5b\u4e2d\u7684\u4efb\u52a11\u2014\u2014\u6570\u636e\u9ad8\u6548\u4f4e\u590d\u6742\u5ea6\u58f0\u5b66\u573a\u666f\u5206\u7c7b\u7684\u63d0\u4ea4\u5185\u5bb9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u7cfb\u7edf\u6765\u5e94\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5bf9\u4e8e\u5c0f\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6211\u4eec\u901a\u8fc7\u51cf\u5c11\u57fa\u7ebf\u6a21\u578b\u7684\u57fa\u672c\u901a\u9053\u6570\u91cf\u6765\u964d\u4f4e\u5176\u590d\u6742\u6027\uff0c\u5e76\u5f15\u5165\u4e86mixup\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4ee5\u589e\u52a0\u8bad\u7ec3\u6837\u672c\u7684\u591a\u6837\u6027\u3002\u5bf9\u4e8e\u8f83\u5927\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6211\u4eec\u4f7f\u7528FocusNet\u4e3a\u591a\u4e2aPatchout faSt Spectrogram Transformer (PaSST)\u6a21\u578b\u548c\u57fa\u4e8e\u539f\u59cb\u91c7\u6837\u738744.1 kHz\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u63d0\u4f9b\u6df7\u6dc6\u7c7b\u4fe1\u606f\u3002\u6211\u4eec\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5c06\u96c6\u6210\u6a21\u578b\u84b8\u998f\u5230\u57fa\u7ebf\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u5728TAU Urban Acoustic Scene 2022 Mobile\u5f00\u53d1\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8fd9\u4e9b\u7cfb\u7edf\uff0c\u5206\u522b\u5728(100, 50, 25, 10, 5)%\u7684\u5206\u5272\u6bd4\u4f8b\u4e0b\uff0c\u4e09\u79cd\u7cfb\u7edf\u7684\u6700\u9ad8\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u5206\u522b\u4e3a(62.21, 59.82, 56.81, 53.03, 47.97)%\u3002 Jin Jie Sean Yeo PDF N/A Data Efficient Acoustic Scene Classification using Teacher-Informed Confusing Class Instruction In this technical report, we describe the SNTL-NTU team's submission for Task 1 Data-Efficient Low-Complexity Acoustic Scene Classification of the detection and classification of acoustic scenes and events (DCASE) 2024 challenge. Three systems are introduced to tackle training splits of different sizes. For small training splits, we explored reducing the complexity of the provided baseline model by reducing the number of base channels. We introduce data augmentation in the form of mixup to increase the diversity of training samples. For the larger training splits, we use FocusNet to provide confusing class information to an ensemble of multiple Patchout faSt Spectrogram Transformer (PaSST) models and baseline models trained on the original sampling rate of 44.1 kHz. We use Knowledge Distillation to distill the ensemble model to the baseline student model. Training the systems on the TAU Urban Acoustic Scene 2022 Mobile development dataset yielded the highest average testing accuracy of (62.21, 59.82, 56.81, 53.03, 47.97)% on split (100, 50, 25, 10, 5)% respectively over the three systems. \u4f7f\u7528\u674e\u7fa4\u65b9\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u673a\u5668\u4eba \u5904\u7406\u673a\u5668\u4eba\u548c\u7269\u4f53\u7684\u65b9\u5411\u662f\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u5173\u952e\u65b9\u9762\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u65b9\u5411\u65f6\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u5230\u4f8b\u5982\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u6d41\u7a0b\u65f6\uff0c\u5f80\u5f80\u7f3a\u4e4f\u6570\u5b66\u4e0a\u7684\u6b63\u786e\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5e26\u6709\u65b9\u5411\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7f51\u7edc\u8f93\u5165\u548c\u8f93\u51fa\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9075\u5faa\u65b9\u5411\u7684\u674e\u7fa4\u7ed3\u6784\u3002\u7ed3\u679c\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u79cd\u6613\u4e8e\u5b9e\u73b0\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u76f4\u63a5\u4e0e\u73b0\u6709\u7684\u5b66\u4e60\u5e93\u4e00\u8d77\u4f7f\u7528\uff0c\u5e76\u4e14\u6bd4\u5176\u4ed6\u5e38\u89c1\u7684\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u8868\u73b0\u663e\u8457\u66f4\u597d\u3002\u6211\u4eec\u7b80\u8981\u4ecb\u7ecd\u4e86\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u4eba\u65b9\u5411\u7684\u674e\u7406\u8bba\uff0c\u4ee5\u6fc0\u53d1\u5e76\u6982\u8ff0\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u968f\u540e\uff0c\u5bf9\u72b6\u6001\u548c\u52a8\u4f5c\u7684\u4e0d\u540c\u65b9\u5411\u8868\u793a\u7ec4\u5408\u8fdb\u884c\u4e86\u5f7b\u5e95\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\uff0c\u5305\u62ec\u76f4\u63a5\u65b9\u5411\u63a7\u5236\u3001\u672b\u7aef\u6267\u884c\u5668\u65b9\u5411\u63a7\u5236\u548c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u4f18\u8d8a\u3002 Martin Schuck PDF N/A Reinforcement Learning with Lie Group Orientations for Robotics Handling orientations of robots and objects is a crucial aspect of many applications. Yet, ever so often, there is a lack of mathematical correctness when dealing with orientations, especially in learning pipelines involving, for example, artificial neural networks. In this paper, we investigate reinforcement learning with orientations and propose a simple modification of the network's input and output that adheres to the Lie group structure of orientations. As a result, we obtain an easy and efficient implementation that is directly usable with existing learning libraries and achieves significantly better performance than other common orientation representations. We briefly introduce Lie theory specifically for orientations in robotics to motivate and outline our approach. Subsequently, a thorough empirical evaluation of different combinations of orientation representations for states and actions demonstrates the superior performance of our proposed approach in different scenarios, including: direct orientation control, end effector orientation control, and pick-and-place tasks."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}