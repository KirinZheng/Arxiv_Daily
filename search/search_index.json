{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract \u7528\u4e8e\u4e73\u817a\u6444\u5f71\u7684\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u56fe\u548c\u591a\u5c3a\u5ea6\u5bf9\u9f50 \u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u7531\u4e8e\u8fd9\u4e9b\u9650\u5236\uff0c\u73b0\u6709\u7684CLIP\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u80f8\u90e8X\u5149\u7b49\u6709\u4e30\u5bcc\u56fe\u50cf\u62a5\u544a\u6570\u636e\u7684\u6a21\u6001\u4e0a\uff0c\u8bb8\u591a\u5176\u4ed6\u91cd\u8981\u6a21\u6001\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u6b64\uff0c\u6211\u4eec\u9996\u6b21\u5c06\u5b8c\u6574\u7684CLIP\u6a21\u578b\u5e94\u7528\u4e8e\u4e73\u817a\u6444\u5f71\uff0c\u8fd9\u9762\u4e34\u7740\u663e\u8457\u7684\u6311\u6218\uff0c\u5305\u62ec\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u611f\u5174\u8da3\u533a\u57df\u5c0f\u4ee5\u53ca\u6570\u636e\u4e0d\u5e73\u8861\u3002\u6211\u4eec\u9996\u5148\u4e3a\u4e73\u817a\u6444\u5f71\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5176\u591a\u89c6\u89d2\u7279\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bf9\u79f0\u5c40\u90e8\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u66f4\u597d\u5730\u5173\u6ce8\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u7ec6\u8282\u7279\u5f81\u3002\u6700\u540e\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u5148\u8bad\u7ec3\u4e86\u533b\u5b66\u77e5\u8bc6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u9650\u5236\u3002\u6211\u4eec\u7684\u591a\u89c6\u89d2\u548c\u591a\u5c3a\u5ea6\u5bf9\u9f50\uff08MaMA\uff09\u65b9\u6cd5\u5728\u4e24\u4e2a\u5927\u578b\u771f\u5b9e\u4e16\u754c\u4e73\u817a\u6444\u5f71\u6570\u636e\u96c6EMBED\u548cRSNA-Mammo\u4e0a\uff0c\u9488\u5bf9\u4e09\u79cd\u4e0d\u540c\u4efb\u52a1\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a\u6700\u5927\u57fa\u7ebf\u768452%\u3002 Yuexi Du PDF N/A Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography Contrastive Language-Image Pre-training (CLIP) shows promise in medical image analysis but requires substantial data and computational resources. Due to these restrictions, existing CLIP applications in medical imaging focus mainly on modalities like chest X-rays that have abundant image-report data available, leaving many other important modalities under-explored. Here, we propose the first adaptation of the full CLIP model to mammography, which presents significant challenges due to labeled data scarcity, high-resolution images with small regions of interest, and data imbalance. We first develop a specialized supervision framework for mammography that leverages its multi-view nature. Furthermore, we design a symmetric local alignment module to better focus on detailed features in high-resolution images. Lastly, we incorporate a parameter-efficient fine-tuning approach for large language models pre-trained with medical knowledge to address data limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms state-of-the-art baselines for three different tasks on two large real-world mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared with the largest baseline. \u5f00\u653e\u4e16\u754c\u8bc4\u4f30\uff1a\u68c0\u7d22\u591a\u6837\u5316\u89c6\u89d2 \u6211\u4eec\u7814\u7a76\u5982\u4f55\u68c0\u7d22\u4e00\u7ec4\u6db5\u76d6\u590d\u6742\u4e14\u6709\u4e89\u8bae\u95ee\u9898\u7684\u5404\u79cd\u89c2\u70b9\u7684\u6587\u6863\uff08\u4f8b\u5982\uff0cChatGPT\u662f\u5426\u5f0a\u5927\u4e8e\u5229\uff1f\uff09\u3002\u6211\u4eec\u7cbe\u5fc3\u7b56\u5212\u4e86\u4e00\u4e2a\u4e3b\u89c2\u95ee\u9898\u68c0\u7d22\u591a\u6837\u6027\u57fa\u51c6\uff08BERDS\uff09\uff0c\u5176\u4e2d\u6bcf\u4e2a\u793a\u4f8b\u5305\u542b\u4e00\u4e2a\u95ee\u9898\u53ca\u5176\u76f8\u5173\u7684\u591a\u6837\u5316\u89c2\u70b9\uff0c\u8fd9\u4e9b\u89c2\u70b9\u6765\u6e90\u4e8e\u8c03\u67e5\u95ee\u5377\u548c\u8fa9\u8bba\u7f51\u7ad9\u3002\u5728\u6b64\u6570\u636e\u4e0a\uff0c\u7ed3\u5408\u8bed\u6599\u5e93\u7684\u68c0\u7d22\u5668\u88ab\u8bc4\u4f30\u4ee5\u5448\u73b0\u5305\u542b\u591a\u6837\u5316\u89c2\u70b9\u7684\u6587\u6863\u96c6\u3002\u6211\u4eec\u7684\u6846\u67b6\u4e0e\u5927\u591a\u6570\u68c0\u7d22\u4efb\u52a1\u4e0d\u540c\uff0c\u56e0\u4e3a\u6587\u6863\u7684\u76f8\u5173\u6027\u4e0d\u80fd\u901a\u8fc7\u7b80\u5355\u7684\u5b57\u7b26\u4e32\u5339\u914d\u6765\u51b3\u5b9a\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u7528\u4e8e\u5224\u65ad\u6bcf\u4e2a\u68c0\u7d22\u5230\u7684\u6587\u6863\u662f\u5426\u5305\u542b\u67d0\u79cd\u89c2\u70b9\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u6599\u5e93\uff08\u7ef4\u57fa\u767e\u79d1\u3001\u7f51\u9875\u5feb\u7167\u4ee5\u53ca\u4f7f\u7528\u641c\u7d22\u5f15\u64ce\u68c0\u7d22\u9875\u9762\u5373\u65f6\u6784\u5efa\u7684\u8bed\u6599\u5e93\uff09\u4e0e\u68c0\u7d22\u5668\u914d\u5bf9\u65f6\u7684\u6027\u80fd\u3002\u68c0\u7d22\u591a\u6837\u5316\u6587\u6863\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u68c0\u7d22\u5668\u7684\u8f93\u51fa\u4ec5\u572833.74%\u7684\u793a\u4f8b\u4e2d\u6db5\u76d6\u4e86\u6240\u6709\u89c2\u70b9\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u67e5\u8be2\u6269\u5c55\u548c\u4ee5\u591a\u6837\u6027\u4e3a\u91cd\u70b9\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u68c0\u7d22\u5668\u7684\u76f2\u4ece\u6027\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u4e3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u7684\u68c0\u7d22\u591a\u6837\u6027\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 Hung-Ting Chen PDF N/A Open-World Evaluation for Retrieving Diverse Perspectives We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model based automatic evaluator that decides whether each retrieved document contains a perspective. This allows us to evaluate the performance of three different types of corpus (Wikipedia, web snapshot, and corpus constructed on the fly with retrieved pages from the search engine) paired with retrievers. Retrieving diverse documents remains challenging, with the outputs from existing retrievers covering all perspectives on only 33.74% of the examples. We further study the impact of query expansion and diversity-focused reranking approaches and analyze retriever sycophancy. Together, we lay the foundation for future studies in retrieval diversity handling complex queries. \u5728\u672a\u76f4\u63a5\u53d1\u73b0\u7280\u725b\u7684\u60c5\u51b5\u4e0b\u5bfb\u627e\u7280\u725b\uff1a\u5229\u7528\u5357\u975e\u7280\u725b\u6816\u606f\u5730\u7684\u591a\u6a21\u6001\u5f71\u50cf\u8fdb\u884c\u4e3b\u52a8\u5b66\u4e60 \u5730\u7403\u4e0a\u8bb8\u591a\u8ff7\u4eba\u7684\u5927\u578b\u52a8\u7269\u6b63\u56e0\u4eba\u7c7b\u6d3b\u52a8\u800c\u6fd2\u4e34\u706d\u7edd\uff0c\u5c24\u5176\u662f\u7280\u725b\uff0c\u7531\u4e8e\u975e\u6d32\u7684\u76d7\u730e\u5371\u673a\uff0c\u5b83\u4eec\u9762\u4e34\u706d\u7edd\u7684\u98ce\u9669\u3002\u76d1\u6d4b\u7280\u725b\u7684\u79fb\u52a8\u5bf9\u5176\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u5e78\u7684\u662f\uff0c\u7531\u4e8e\u7280\u725b\u884c\u8e2a\u9690\u79d8\uff0c\u8fd9\u4e00\u4efb\u52a1\u53d8\u5f97\u5f02\u5e38\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u7ed8\u5236\u96c6\u4f53\u6392\u6cc4\u5730\u70b9\uff08\u79f0\u4e3amidden\uff09\u7684\u5730\u56fe\uff0c\u8fd9\u4e9b\u5730\u70b9\u63d0\u4f9b\u4e86\u5173\u4e8e\u7280\u725b\u7a7a\u95f4\u884c\u4e3a\u7684\u4fe1\u606f\uff0c\u5bf9\u53cd\u76d7\u730e\u3001\u7ba1\u7406\u548c\u518d\u5f15\u5165\u5de5\u4f5c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u672c\u6587\u9996\u6b21\u901a\u8fc7\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u5229\u7528\u9065\u611f\u7684\u70ed\u6210\u50cf\u3001RGB\u548cLiDAR\u56fe\u50cf\uff0c\u5728\u88ab\u52a8\u548c\u4e3b\u52a8\u5b66\u4e60\u73af\u5883\u4e2d\u68c0\u6d4b\u5e76\u7ed8\u5236\u4e86\u7280\u725bmidden\u7684\u4f4d\u7f6e\u3002\u7531\u4e8e\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u56e0\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86MultimodAL\uff0c\u8fd9\u662f\u4e00\u4e2a\u91c7\u7528\u6392\u5e8f\u6280\u672f\u548c\u591a\u6a21\u6001\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ee594%\u66f4\u5c11\u7684\u6807\u7b7e\u5b9e\u73b0\u4e86\u4e0e\u88ab\u52a8\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5904\u7406\u7c7b\u4f3c\u89c4\u6a21\u7684\u6570\u636e\u96c6\u65f6\uff0c\u53ef\u4ee5\u8282\u7701\u8d85\u8fc776\u5c0f\u65f6\u7684\u6807\u6ce8\u65f6\u95f4\u3002\u51fa\u4e4e\u610f\u6599\u7684\u662f\uff0c\u6211\u4eec\u7684midden\u5730\u56fe\u63ed\u793a\u4e86\u7280\u725bmidden\u5e76\u975e\u968f\u673a\u5206\u5e03\u5728\u6574\u4e2a\u666f\u89c2\u4e2d\uff0c\u800c\u662f\u805a\u96c6\u5206\u5e03\u7684\u3002\u56e0\u6b64\uff0c\u62a4\u6797\u5458\u5e94\u91cd\u70b9\u90e8\u7f72\u5728\u9ad8\u5bc6\u5ea6midden\u533a\u57df\uff0c\u4ee5\u52a0\u5f3a\u53cd\u76d7\u730e\u5de5\u4f5c\uff0c\u8fd9\u4e0e\u8054\u5408\u56fd\u76ee\u680715.7\u76f8\u4e00\u81f4\u3002 Lucia Gordon PDF N/A Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats Much of Earth's charismatic megafauna is endangered by human activities, particularly the rhino, which is at risk of extinction due to the poaching crisis in Africa. Monitoring rhinos' movement is crucial to their protection but has unfortunately proven difficult because rhinos are elusive. Therefore, instead of tracking rhinos, we propose the novel approach of mapping communal defecation sites, called middens, which give information about rhinos' spatial behavior valuable to anti-poaching, management, and reintroduction efforts. This paper provides the first-ever mapping of rhino midden locations by building classifiers to detect them using remotely sensed thermal, RGB, and LiDAR imagery in passive and active learning settings. As existing active learning methods perform poorly due to the extreme class imbalance in our dataset, we design MultimodAL, an active learning system employing a ranking technique and multimodality to achieve competitive performance with passive learning models with 94% fewer labels. Our methods could therefore save over 76 hours in labeling time when used on a similarly-sized dataset. Unexpectedly, our midden map reveals that rhino middens are not randomly distributed throughout the landscape; rather, they are clustered. Consequently, rangers should be targeted at areas with high midden densities to strengthen anti-poaching efforts, in line with UN Target 15.7. MALPOLON\uff1a\u4e00\u79cd\u7528\u4e8e\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u7684\u6846\u67b6 \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMALPOLON\u7684\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff08deep-SDM\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u7528Python\u7f16\u5199\uff0c\u57fa\u4e8ePyTorch\u5e93\u6784\u5efa\uff0c\u65e8\u5728\u4e3a\u4ec5\u6709\u4e00\u822cPython\u8bed\u8a00\u6280\u80fd\u7684\u7528\u6237\uff08\u5982\u751f\u6001\u5efa\u6a21\u5b66\u8005\uff09\u63d0\u4f9b\u4fbf\u5229\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u8f7b\u677e\u8fdb\u884c\u6df1\u5ea6\u7269\u79cd\u5206\u5e03\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5e76\u5206\u4eab\u76f8\u5173\u6210\u679c\u3002\u8fd9\u4e9b\u7528\u6237\u5bf9\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6784\u5efa\u65b0\u7684\u7269\u79cd\u5206\u5e03\u6a21\u578b\u611f\u5174\u8da3\u3002\u66f4\u9ad8\u7ea7\u7684\u7528\u6237\u4e5f\u53ef\u4ee5\u901a\u8fc7\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7\u91cd\u5199\u73b0\u6709\u7c7b\u6765\u8fd0\u884c\u66f4\u5177\u4f53\u7684\u5b9e\u9a8c\uff0c\u540c\u65f6\u5229\u7528\u4e00\u952e\u5f0f\u793a\u4f8b\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u6216\u63d0\u4f9b\u7684\u539f\u59cb\u53ca\u9884\u5904\u7406\u6570\u636e\u96c6\u3002\u8be5\u6846\u67b6\u5728GitHub\u548cPyPi\u4e0a\u5f00\u6e90\uff0c\u5e76\u9644\u6709\u8be6\u5c3d\u7684\u6587\u6863\u548c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u4f7f\u7528\u793a\u4f8b\u3002MALPOLON\u63d0\u4f9b\u4e86\u7b80\u4fbf\u7684\u5b89\u88c5\u65b9\u5f0f\u3001\u57fa\u4e8eYAML\u7684\u914d\u7f6e\u3001\u5e76\u884c\u8ba1\u7b97\u3001\u591aGPU\u5229\u7528\u3001\u57fa\u51c6\u548c\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\uff0c\u4ee5\u53ca\u4e30\u5bcc\u7684\u6559\u7a0b\u548c\u6587\u6863\uff0c\u65e8\u5728\u63d0\u5347\u751f\u6001\u5b66\u8005\u548c\u7814\u7a76\u4eba\u5458\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6027\u80fd\u6269\u5c55\u6027\u3002 Theo Larcher PDF N/A MALPOLON: A Framework for Deep Species Distribution Modeling This paper describes a deep-SDM framework, MALPOLON. Written in Python and built upon the PyTorch library, this framework aims to facilitate training and inferences of deep species distribution models (deep-SDM) and sharing for users with only general Python language skills (e.g., modeling ecologists) who are interested in testing deep learning approaches to build new SDMs. More advanced users can also benefit from the framework's modularity to run more specific experiments by overriding existing classes while taking advantage of press-button examples to train neural networks on multiple classification tasks using custom or provided raw and pre-processed datasets. The framework is open-sourced on GitHub and PyPi along with extensive documentation and examples of use in various scenarios. MALPOLON offers straightforward installation, YAML-based configuration, parallel computing, multi-GPU utilization, baseline and foundational models for benchmarking, and extensive tutorials/documentation, aiming to enhance accessibility and performance scalability for ecologists and researchers. \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7528\u4e8e\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u7535\u5f71\u5206\u5272 \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SSP\uff09\u5728\u4ece\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65b9\u9762\u663e\u793a\u51fa\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u53ef\u80fd\u5bf9\u81ea\u52a8\u5316\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\uff08CMR\uff09\u77ed\u8f74\u7535\u5f71\u5206\u5272\u6709\u7528\u3002\u7136\u800c\uff0c\u5173\u4e8eSSP\u5bf9\u5206\u5272\u76ca\u5904\u7684\u4e0d\u4e00\u81f4\u62a5\u544a\u4f7f\u5f97\u5c06\u5176\u5e94\u7528\u4e8eCMR\u53d8\u5f97\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30SSP\u65b9\u6cd5\u5728CMR\u7535\u5f71\u5206\u5272\u4e2d\u7684\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u4f7f\u7528\u4e86296\u540d\u53d7\u8bd5\u8005\u7684\u77ed\u8f74\u7535\u5f71\u5806\u6808\uff08\u517190618\u5f202D\u5207\u7247\uff09\u8fdb\u884c\u672a\u6807\u6ce8\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u4e86\u56db\u79cdSSP\u65b9\u6cd5\uff1aSimCLR\u3001\u4f4d\u7f6e\u5bf9\u6bd4\u5b66\u4e60\u3001DINO\u548c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u3002\u5bf9\u4e8e\u6bcf\u79cdSSP\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u76842D\u57fa\u7ebf\u6a21\u578b\uff0c\u4f7f\u7528\u4e86\u4e0d\u540c\u6570\u91cf\u7684\u53d7\u8bd5\u8005\u5b50\u96c6\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e0e\u57fa\u7ebf\u6a21\u578b\u5728140\u540d\u53d7\u8bd5\u8005\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u4f7f\u75283D Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u8fdb\u884c\u6bd4\u8f83\u3002SSP\u65b9\u6cd5\u5728\u6700\u5927\u6709\u76d1\u7763\u5fae\u8c03\u5b50\u96c6\u4e0a\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u6ca1\u6709\u6027\u80fd\u63d0\u5347\uff08DSC = 0.89\uff09\u3002\u5f53\u4ec5\u670910\u540d\u53d7\u8bd5\u8005\uff08231\u5f202D\u5207\u7247\uff09\u53ef\u7528\u4e8e\u6709\u76d1\u7763\u8bad\u7ec3\u65f6\uff0c\u4f7f\u7528MIM\u7684SSP\uff08DSC = 0.86\uff09\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff08DSC = 0.82\uff09\u3002\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\uff0cSSP\u5bf9CMR\u7535\u5f71\u5206\u5272\u662f\u6709\u4ef7\u503c\u7684\uff0c\u4f46\u5728\u6709\u5145\u8db3\u6807\u6ce8\u6570\u636e\u65f6\uff0c\u5e76\u4e0d\u4f1a\u5bf9\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6709\u6240\u5e2e\u52a9\u3002\u6b64\u5916\uff0cSSP\u65b9\u6cd5\u7684\u9009\u62e9\u4e5f\u5f88\u91cd\u8981\u3002\u4ee3\u7801\u5df2\u5728\u4ee5\u4e0b\u516c\u5f00\u53ef\u7528\uff1ahttps://github.com/q-cardIA/ssp-cmr-cine-segmentation\u3002 Rob A. J. de Mooij PDF N/A Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation Self-supervised pretraining (SSP) has shown promising results in learning from large unlabeled datasets and, thus, could be useful for automated cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However, inconsistent reports of the benefits of SSP for segmentation have made it difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP methods for CMR cine segmentation.   To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were used for unlabeled pretraining with four SSP methods; SimCLR, positional contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying numbers of subjects were used for supervised fine-tuning of 2D models for each SSP method, as well as to train a 2D baseline model from scratch. The fine-tuned models were compared to the baseline using the 3D Dice similarity coefficient (DSC) in a test dataset of 140 subjects.   The SSP methods showed no performance gains with the largest supervised fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects (231 2D slices) are available for supervised training, SSP using MIM (DSC = 0.86) improves over training from scratch (DSC = 0.82).   This study found that SSP is valuable for CMR cine segmentation when labeled training data is scarce, but does not aid state-of-the-art deep learning methods when ample labeled data is available. Moreover, the choice of SSP method is important. The code is publicly available at: https://github.com/q-cardIA/ssp-cmr-cine-segmentation \u5728\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e4b\u524d\u63a8\u65ad\u4eba\u7c7b\u7684\u610f\u56fe \u4e3a\u4e86\u8ba9AI\u4ee3\u7406\u5bf9\u4eba\u7c7b\u6709\u6240\u5e2e\u52a9\uff0c\u5b83\u4eec\u5e94\u5f53\u80fd\u591f\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5b8c\u6210\u65e5\u5e38\u7684\u5408\u4f5c\u4efb\u52a1\u3002\u7136\u800c\uff0c\u771f\u5b9e\u7684\u4eba\u7c7b\u6307\u4ee4\u672c\u8eab\u5c31\u5177\u6709\u6a21\u7cca\u6027\uff0c\u56e0\u4e3a\u8bf4\u8bdd\u8005\u5047\u8bbe\u542c\u8005\u5bf9\u5176\u9690\u85cf\u7684\u76ee\u6807\u548c\u610f\u56fe\u6709\u8db3\u591f\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u6807\u51c6\u7684\u8bed\u8a00\u57fa\u7840\u548c\u89c4\u5212\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8fd9\u79cd\u6a21\u7cca\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u5c06\u4eba\u7c7b\u7684\u5185\u5728\u76ee\u6807\u5efa\u6a21\u4e3a\u73af\u5883\u4e2d\u7684\u989d\u5916\u90e8\u5206\u53ef\u89c2\u5bdf\u56e0\u7d20\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u540d\u4e3a\u201c\u9075\u5faa\u6307\u4ee4\u4e0e\u793e\u4f1a\u5316\u53ca\u5177\u8eab\u63a8\u7406\u201d\uff08FISER\uff09\uff0c\u65e8\u5728\u66f4\u597d\u5730\u9075\u5faa\u5408\u4f5c\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002\u6211\u4eec\u7684\u6846\u67b6\u660e\u786e\u5730\u5c06\u4eba\u7c7b\u76ee\u6807\u548c\u610f\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u63a8\u65ad\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5e76\u5728\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5HandMeThat\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5236\u5b9a\u884c\u52a8\u8ba1\u5212\u4e4b\u524d\uff0c\u5229\u7528\u793e\u4f1a\u63a8\u7406\u660e\u786e\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\u7684\u65b9\u6cd5\u4f18\u4e8e\u7eaf\u7cb9\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u4e0e\u4e00\u4e9b\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5305\u62ec\u5728\u6700\u5927\u53ef\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u201c\u601d\u7ef4\u94fe\u201d\u63d0\u793a\uff0c\u53d1\u73b0FISER\u5728\u6240\u7814\u7a76\u7684\u5177\u8eab\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u8fbe\u5230\u4e86HandMeThat\u4e0a\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002 Yanming Wan PDF N/A Infer Human's Intentions Before Following Natural Language Instructions For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat. \u901a\u8fc7\u7edf\u8ba1\u7269\u7406\u5b66\u548c\u63a7\u5236\u7406\u8bba\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u7684\u6700\u4f73\u534f\u8bae \u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1\u65f6\uff0c\u5f80\u5f80\u4f1a\u9047\u5230\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5728\u65b0\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4f1a\u964d\u4f4e\u4e4b\u524d\u5df2\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5728\u9884\u5b9a\u4e49\u8bad\u7ec3\u534f\u8bae\u4e0b\u5206\u6790\u5408\u6210\u6846\u67b6\u4e2d\u7684\u5b66\u4e60\u66f2\u7ebf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u534f\u8bae\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u5176\u6700\u4f18\u6027\u7684\u575a\u5b9e\u7406\u8bba\u57fa\u7840\u3002\u672c\u6587\u901a\u8fc7\u7ed3\u5408\u4f7f\u7528\u7edf\u8ba1\u7269\u7406\u6280\u672f\u63a8\u5bfc\u51fa\u7684\u8bad\u7ec3\u52a8\u529b\u5b66\u7cbe\u786e\u65b9\u7a0b\u4e0e\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u65b9\u6cd5\u5e94\u7528\u4e8e\u6301\u7eed\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u95ee\u9898\u7684\u5e08\u751f\u6a21\u578b\uff0c\u83b7\u5f97\u4e86\u6700\u5927\u5316\u6027\u80fd\u540c\u65f6\u6700\u5c0f\u5316\u9057\u5fd8\u7684\u4efb\u52a1\u9009\u62e9\u534f\u8bae\u7406\u8bba\u3002\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u975e\u5e73\u51e1\u4f46\u53ef\u89e3\u91ca\u7684\u7b56\u7565\uff0c\u4ee5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63ed\u793a\u4e86\u6700\u4f18\u5b66\u4e60\u534f\u8bae\u5982\u4f55\u8c03\u8282\u5df2\u786e\u7acb\u6548\u5e94\uff0c\u4f8b\u5982\u4efb\u52a1\u76f8\u4f3c\u6027\u5bf9\u9057\u5fd8\u7684\u5f71\u54cd\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7406\u8bba\u53d1\u73b0\u3002 Francesco Mori PDF N/A Optimal Protocols for Continual Learning via Statistical Physics and Control Theory Artificial neural networks often struggle with catastrophic forgetting when learning multiple tasks sequentially, as training on new tasks degrades the performance on previously learned ones. Recent theoretical work has addressed this issue by analysing learning curves in synthetic frameworks under predefined training protocols. However, these protocols relied on heuristics and lacked a solid theoretical foundation assessing their optimality. In this paper, we fill this gap combining exact equations for training dynamics, derived using statistical physics techniques, with optimal control methods. We apply this approach to teacher-student models for continual learning and multi-task problems, obtaining a theory for task-selection protocols maximising performance while minimising forgetting. Our theoretical analysis offers non-trivial yet interpretable strategies for mitigating catastrophic forgetting, shedding light on how optimal learning protocols can modulate established effects, such as the influence of task similarity on forgetting. Finally, we validate our theoretical findings on real-world data. \u5177\u6709\u591a\u4e2a\u89c4\u5212\u89c6\u91ce\u7684\u9006\u5f3a\u5316\u5b66\u4e60 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u4e2a\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u95ee\u9898\uff0c\u5176\u4e2d\u4e13\u5bb6\u4eec\u5728\u5171\u4eab\u5956\u52b1\u51fd\u6570\u4e0b\u8fdb\u884c\u89c4\u5212\uff0c\u4f46\u5177\u6709\u4e0d\u540c\u7684\u3001\u672a\u77e5\u7684\u89c4\u5212\u89c6\u91ce\u3002\u5728\u6ca1\u6709\u6298\u6263\u56e0\u5b50\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5956\u52b1\u51fd\u6570\u5177\u6709\u66f4\u5927\u7684\u53ef\u884c\u89e3\u96c6\uff0c\u8fd9\u4f7f\u5f97\u73b0\u6709\u7684IRL\u65b9\u6cd5\u66f4\u96be\u8bc6\u522b\u51fa\u4e00\u4e2a\u5956\u52b1\u51fd\u6570\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u80fd\u591f\u5b66\u4e60\u5177\u6709\u4ee3\u7406\u7279\u5b9a\u6298\u6263\u56e0\u5b50\u7684\u5168\u5c40\u591a\u4ee3\u7406\u5956\u52b1\u51fd\u6570\u7684\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u591f\u91cd\u5efa\u4e13\u5bb6\u7b56\u7565\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u8fd9\u4e24\u79cd\u7b97\u6cd5\u7684\u5956\u52b1\u51fd\u6570\u548c\u6298\u6263\u56e0\u5b50\u7684\u53ef\u884c\u89e3\u7a7a\u95f4\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u5b66\u5956\u52b1\u51fd\u6570\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002 Jiayu Yao PDF N/A Inverse Reinforcement Learning with Multiple Planning Horizons In this work, we study an inverse reinforcement learning (IRL) problem where the experts are planning under a shared reward function but with different, unknown planning horizons. Without the knowledge of discount factors, the reward function has a larger feasible solution set, which makes it harder for existing IRL approaches to identify a reward function. To overcome this challenge, we develop algorithms that can learn a global multi-agent reward function with agent-specific discount factors that reconstruct the expert policies. We characterize the feasible solution space of the reward function and discount factors for both algorithms and demonstrate the generalizability of the learned reward function across multiple domains. \u91cd\u8bbf\u4e00\u5207\uff1a\u901a\u8fc7\u56fe\u50cf\u5206\u6bb5\u68c0\u7d22\u5b9e\u73b0\u89c6\u89c9\u5730\u70b9\u8bc6\u522b \u51c6\u786e\u8bc6\u522b\u91cd\u8bbf\u5730\u70b9\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u8fdb\u884c\u5b9a\u4f4d\u548c\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u8981\u6c42\u89c6\u89c9\u8868\u5f81\u5728\u76f8\u673a\u89c6\u89d2\u548c\u573a\u666f\u5916\u89c2\u5b58\u5728\u5f3a\u70c8\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u72ec\u7279\u6027\u3002\u73b0\u6709\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6d41\u7a0b\u5bf9\u201c\u6574\u5f20\u201d\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\u5e76\u641c\u7d22\u5339\u914d\u3002\u8fd9\u5728\u5339\u914d\u4ece\u4e0d\u540c\u76f8\u673a\u89c6\u89d2\u62cd\u6444\u7684\u540c\u4e00\u5730\u70b9\u7684\u4e24\u5f20\u56fe\u50cf\u65f6\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff1a\u201c\u91cd\u53e0\u90e8\u5206\u7684\u76f8\u4f3c\u6027\u53ef\u80fd\u88ab\u975e\u91cd\u53e0\u90e8\u5206\u7684\u4e0d\u76f8\u4f3c\u6027\u6240\u4e3b\u5bfc\u201d\u3002\u6211\u4eec\u901a\u8fc7\u7f16\u7801\u548c\u641c\u7d22\u201c\u56fe\u50cf\u7247\u6bb5\u201d\u800c\u975e\u6574\u5f20\u56fe\u50cf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u5f00\u653e\u96c6\u56fe\u50cf\u5206\u5272\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u201c\u6709\u610f\u4e49\u201d\u7684\u5b9e\u4f53\uff08\u5373\u7269\u4f53\u548c\u80cc\u666f\uff09\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u521b\u5efa\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u8868\u5f81\uff0c\u5373\u7531\u591a\u4e2a\u8fde\u63a5\u7247\u6bb5\u53ca\u5176\u76f8\u90bb\u7247\u6bb5\u7684\u91cd\u53e0\u5b50\u56fe\u7ec4\u6210\u7684\u96c6\u5408\uff0c\u79f0\u4e3a\u8d85\u7ea7\u7247\u6bb5\uff08SuperSegment\uff09\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9ad8\u6548\u5730\u5c06\u8fd9\u4e9b\u8d85\u7ea7\u7247\u6bb5\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u5411\u91cf\u8868\u793a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u805a\u5408\u56e0\u5b50\u5316\u8868\u793a\u65b9\u6cd5\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u68c0\u7d22\u8fd9\u4e9b\u90e8\u5206\u8868\u793a\u6bd4\u57fa\u4e8e\u6574\u5f20\u56fe\u50cf\u7684\u68c0\u7d22\u80fd\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u53ec\u56de\u7387\u3002\u6211\u4eec\u7684\u57fa\u4e8e\u7247\u6bb5\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aSegVLAD\uff0c\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e3a\u5730\u70b9\u8bc6\u522b\u8bbe\u5b9a\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u901a\u7528\u548c\u4efb\u52a1\u4e13\u7528\u7684\u56fe\u50cf\u7f16\u7801\u5668\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u4e00\u4e2a\u7269\u4f53\u5b9e\u4f8b\u68c0\u7d22\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u201c\u91cd\u8bbf\u4efb\u4f55\u4e8b\u7269\u201d\u7684\u6f5c\u529b\uff0c\u8be5\u4efb\u52a1\u901a\u8fc7\u8bc6\u522b\u7279\u5b9a\u5730\u70b9\u7684\u76ee\u6807\u7269\u4f53\uff0c\u8fde\u63a5\u4e86\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u548c\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u8fd9\u4e24\u4e2a\u4e0d\u540c\u7684\u7814\u7a76\u9886\u57df\u3002\u6e90\u4ee3\u7801\uff1ahttps://github.com/AnyLoc/Revisit-Anything\u3002 Kartik Garg PDF N/A Revisit Anything: Visual Place Recognition via Image Segment Retrieval Accurately recognizing a revisited place is crucial for embodied agents to localize and navigate. This requires visual representations to be distinct, despite strong variations in camera viewpoint and scene appearance. Existing visual place recognition pipelines encode the \"whole\" image and search for matches. This poses a fundamental challenge in matching two images of the same place captured from different camera viewpoints: \"the similarity of what overlaps can be dominated by the dissimilarity of what does not overlap\". We address this by encoding and searching for \"image segments\" instead of the whole images. We propose to use open-set image segmentation to decompose an image into `meaningful' entities (i.e., things and stuff). This enables us to create a novel image representation as a collection of multiple overlapping subgraphs connecting a segment with its neighboring segments, dubbed SuperSegment. Furthermore, to efficiently encode these SuperSegments into compact vector representations, we propose a novel factorized representation of feature aggregation. We show that retrieving these partial representations leads to significantly higher recognition recall than the typical whole image based retrieval. Our segments-based approach, dubbed SegVLAD, sets a new state-of-the-art in place recognition on a diverse selection of benchmark datasets, while being applicable to both generic and task-specialized image encoders. Finally, we demonstrate the potential of our method to ``revisit anything'' by evaluating our method on an object instance retrieval task, which bridges the two disparate areas of research: visual place recognition and object-goal navigation, through their common aim of recognizing goal objects specific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything. IFCap\uff1a\u9762\u5411\u96f6\u6837\u672c\u63cf\u8ff0\u751f\u6210\u7684\u7c7b\u56fe\u50cf\u68c0\u7d22\u4e0e\u57fa\u4e8e\u9891\u7387\u7684\u5b9e\u4f53\u8fc7\u6ee4 \u6700\u8fd1\u5728\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u9886\u57df\u7684\u8fdb\u5c55\u63a2\u7d22\u4e86\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u6210\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\u7684\u5c40\u9650\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u8bad\u7ec3\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u6587\u672c\u6570\u636e\u4e0e\u5728\u63a8\u7406\u671f\u95f4\u4f7f\u7528\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7c7b\u56fe\u50cf\u68c0\u7d22\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u6587\u672c\u7279\u5f81\u4e0e\u89c6\u89c9\u76f8\u5173\u7279\u5f81\u5bf9\u9f50\uff0c\u4ee5\u7f13\u89e3\u6a21\u6001\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u878d\u5408\u6a21\u5757\uff0c\u5c06\u68c0\u7d22\u5230\u7684\u63cf\u8ff0\u4e0e\u8f93\u5165\u7279\u5f81\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u751f\u6210\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u7684\u5b9e\u4f53\u8fc7\u6ee4\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u8d28\u91cf\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u79f0\u4e4b\u4e3aIFCap\uff08$\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity Filtering for Zero-shot $\\textbf{Cap}$tioning\uff09\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u5728\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u548c\u89c6\u9891\u63cf\u8ff0\u751f\u6210\u65b9\u9762\uff0c\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u6587\u672c\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u63cf\u8ff0\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002 Soeun Lee PDF N/A IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, we propose a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. Our method further enhances the accuracy of generated captions by designing a Fusion Module that integrates retrieved captions with input features. Additionally, we introduce a Frequency-based Entity Filtering technique that significantly improves caption quality. We integrate these methods into a unified framework, which we refer to as IFCap ($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity Filtering for Zero-shot $\\textbf{Cap}$tioning). Through extensive experimentation, our straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training. \u63ed\u793a\u9884\u8bad\u7ec3\u5728\u76f4\u63a5\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u4f5c\u7528 \u76f4\u63a5\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u7cfb\u7edf\u5728\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u9047\u5230\u4e86\u4e00\u4e2a\u91cd\u8981\u7f3a\u9677\u3002\u5e38\u89c1\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e0a\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u635f\u5931\u6548\u7387\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u7cfb\u7edf\u3001\u4f20\u7edf\u65b9\u6cd5\u4ee5\u53ca\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u7cfb\u7edf\u7684\u8bad\u7ec3\u52a8\u6001\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u6a21\u578b\u5728\u9884\u6d4b\u65f6\u96be\u4ee5\u4ece\u8bed\u97f3\u8f93\u5165\u4e2d\u6574\u5408\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5047\u8bbe\u8fd9\u4e00\u95ee\u9898\u6e90\u4e8e\u6709\u6548\u8bad\u7ec3\u76f4\u63a5\u8bed\u97f3\u7ffb\u8bd1\u7f16\u7801\u5668\u7684\u56f0\u96be\u3002\u867d\u7136\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u9700\u8981\u540c\u65f6\u5b66\u4e60\u58f0\u5b66\u548c\u8bed\u4e49\u5efa\u6a21\uff0c\u4f46\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u53ea\u9700\u4e13\u6ce8\u4e8e\u540e\u8005\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u5728\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b\u4e2d\u8fdb\u884c\u7ec6\u5fae\u8c03\u6574\uff0c\u4ee5\u5728\u8bad\u7ec3\u7684\u65e9\u671f\u6b65\u9aa4\u4e2d\u6574\u5408\u6e90\u4fe1\u606f\u3002\u6211\u4eec\u8868\u660e\uff0c\u901a\u8fc7\u8fd9\u4e00\u8c03\u6574\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002 Belen Alastruey PDF N/A Unveiling the Role of Pretraining in Direct Speech Translation Direct speech-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic speech recognition, hence losing efficiency in the training process. In this study, we compare the training dynamics of a system using a pretrained encoder, the conventional approach, and one trained from scratch. We observe that, throughout the training, the randomly initialized model struggles to incorporate information from the speech inputs for its predictions. Hence, we hypothesize that this issue stems from the difficulty of effectively training an encoder for direct speech translation. While a model trained from scratch needs to learn acoustic and semantic modeling simultaneously, a pretrained one can just focus on the latter. Based on these findings, we propose a subtle change in the decoder cross-attention to integrate source information from earlier steps in training. We show that with this change, the model trained from scratch can achieve comparable performance to the pretrained one, while reducing the training time. EMOVA\uff1a\u8d4b\u4e88\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u52a8\u7684\u60c5\u611f\uff0c\u4f7f\u5176\u80fd\u591f\u770b\u3001\u542c\u548c\u8bf4 GPT-4o\uff0c\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u591a\u6837\u5316\u60c5\u611f\u548c\u8bed\u8c03\u7684\u8bed\u97f3\u5bf9\u8bdd\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u5168\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u4e00\u4e2a\u91cc\u7a0b\u7891\u3002\u7136\u800c\uff0c\u5728\u5f00\u6e90\u793e\u533a\u4e2d\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u611f\u77e5\u548c\u751f\u6210\u56fe\u50cf\u3001\u6587\u672c\u548c\u8bed\u97f3\u7684\u5168\u6d41\u7a0b\u5904\u7406\uff0c\u5e76\u5229\u7528\u516c\u5f00\u53ef\u7528\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u8bed\u97f3\u5904\u7406\uff0c\u800c\u8bed\u97f3-\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u4e0a\u4ecd\u7136\u6709\u9650\u751a\u81f3\u7f3a\u5931\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EMOVA\uff08\u60c5\u611f\u5168\u5728\u7684\u8bed\u97f3\u52a9\u624b\uff09\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u5148\u7684\u89c6\u89c9-\u8bed\u8a00\u6027\u80fd\u3002\u901a\u8fc7\u4e00\u4e2a\u8bed\u4e49-\u58f0\u5b66\u89e3\u8026\u7684\u8bed\u97f3\u6807\u8bb0\u5668\uff0c\u6211\u4eec\u60ca\u8bb6\u5730\u53d1\u73b0\uff0c\u5168\u6a21\u6001\u5bf9\u9f50\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u548c\u8bed\u97f3\u80fd\u529b\uff0c\u76f8\u6bd4\u4e8e\u76f8\u5e94\u53cc\u6a21\u6001\u5bf9\u9f50\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u98ce\u683c\u6a21\u5757\uff0c\u7528\u4e8e\u7075\u6d3b\u7684\u8bed\u97f3\u98ce\u683c\u63a7\u5236\uff08\u5982\u60c5\u611f\u548c\u97f3\u8c03\uff09\u3002\u9996\u6b21\uff0cEMOVA\u5728\u89c6\u89c9-\u8bed\u8a00\u548c\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u540c\u65f6\u652f\u6301\u5e26\u6709\u751f\u52a8\u60c5\u611f\u7684\u5168\u6a21\u6001\u8bed\u97f3\u5bf9\u8bdd\u3002 Kai Chen PDF N/A EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible speech style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions. \u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u6790\u8bf4\u670d\u6027\u6587\u672c\u4e2d\u7684\u6743\u529b\u8bcd\u6c47 \u5f71\u54cd\u529b\u8bcd\u6c47\u662f\u6307\u80fd\u591f\u5f15\u53d1\u5f3a\u70c8\u60c5\u611f\u53cd\u5e94\u5e76\u663e\u8457\u5f71\u54cd\u8bfb\u8005\u884c\u4e3a\u7684\u672f\u8bed\uff0c\u5728\u8425\u9500\u3001\u653f\u6cbb\u548c\u52b1\u5fd7\u5199\u4f5c\u7b49\u9886\u57df\u4e2d\u626e\u6f14\u7740\u5173\u952e\u89d2\u8272\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u5b9a\u4e49\u8bcd\u5178\u548cPython\u4e2d\u7684TextBlob\u5e93\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u6790\u8bf4\u670d\u6027\u6587\u672c\u4e2d\u5f71\u54cd\u529b\u8bcd\u6c47\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u8bc6\u522b\u7ed9\u5b9a\u6587\u672c\u4e2d\u5f71\u54cd\u529b\u8bcd\u6c47\u7684\u5b58\u5728\u548c\u9891\u7387\uff0c\u6211\u4eec\u65e8\u5728\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\u5e76\u5206\u6790\u5b83\u4eec\u5bf9\u60c5\u611f\u548c\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u8003\u5bdf\u4e86\u8de8\u591a\u4e2a\u9886\u57df\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u4f9b\u5173\u4e8e\u5f71\u54cd\u529b\u8bcd\u6c47\u6548\u679c\u7684\u89c1\u89e3\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u3001\u5e7f\u544a\u5546\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u5b9e\u9645\u5e94\u7528\u3002 Sahil Garje PDF N/A Automated Detection and Analysis of Power Words in Persuasive Text Using Natural Language Processing Power words are terms that evoke strong emotional responses and significantly influence readers' behavior, playing a crucial role in fields like marketing, politics, and motivational writing. This study proposes a methodology for the automated detection and analysis of power words in persuasive text using a custom lexicon and the TextBlob library in Python. By identifying the presence and frequency of power words within a given text, we aim to classify and analyze their impact on sentiment and reader engagement. This research examines diverse datasets across various domains to provide insights into the effectiveness of power words, offering practical applications for content creators, advertisers, and policymakers. FlowBench\uff1a\u4e00\u4e2a\u9762\u5411\u590d\u6742\u51e0\u4f55\u4f53\u7684\u6d41\u4f53\u6a21\u62df\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5 \u6a21\u62df\u6d41\u4f53\u5728\u4efb\u610f\u5f62\u72b6\u5468\u56f4\u7684\u6d41\u52a8\u662f\u89e3\u51b3\u5404\u79cd\u5de5\u7a0b\u95ee\u9898\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e0a\u6a21\u62df\u6d41\u52a8\u7269\u7406\u4ecd\u7136\u5728\u6570\u503c\u4e0a\u5177\u6709\u6311\u6218\u6027\u4e14\u8ba1\u7b97\u8d44\u6e90\u5bc6\u96c6\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u4f20\u7edf\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6c42\u89e3\u5668\u65f6\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u521b\u5efa\u5feb\u901f\u4e14\u9002\u5e94\u6027\u5f3a\u7684PDE\u6c42\u89e3\u5668\u7684\u8bf1\u4eba\u673a\u4f1a\u3002\u7136\u800c\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u6027\u80fd\u7684\u57fa\u51c6\u6570\u636e\u96c6\u975e\u5e38\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u6d41\u52a8\u7269\u7406\u3002\u6211\u4eec\u5f15\u5165\u4e86FlowBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc710,000\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5668\uff0c\u76ee\u524d\u6bd4\u4efb\u4f55\u516c\u5f00\u53ef\u7528\u7684\u6d41\u52a8\u7269\u7406\u6570\u636e\u96c6\u90fd\u8981\u5927\u3002FlowBench\u5305\u542b\u8de8\u8d8a\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff08\u53c2\u6570\u5316\u4e0e\u975e\u53c2\u6570\u5316\uff09\u7684\u6d41\u52a8\u6a21\u62df\u6570\u636e\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u6d41\u52a8\u6761\u4ef6\uff08\u96f7\u8bfa\u6570\u548c\u683c\u62c9\u8096\u592b\u6570\uff09\uff0c\u6355\u6349\u4e86\u591a\u79cd\u6d41\u52a8\u73b0\u8c61\uff08\u7a33\u6001\u4e0e\u77ac\u6001\uff1b\u5f3a\u5236\u5bf9\u6d41\u4e0e\u81ea\u7531\u5bf9\u6d41\uff09\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u4e8c\u7ef4\u548c\u4e09\u7ef4\u3002FlowBench\u5305\u542b\u8d85\u8fc710,000\u4e2a\u6570\u636e\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u662f\u4f7f\u7528\u4e00\u4e2a\u7ecf\u8fc7\u5145\u5206\u9a8c\u8bc1\u7684\u6a21\u62df\u5668\u6846\u67b6\u8fdb\u884c\u5b8c\u5168\u89e3\u6790\u7684\u76f4\u63a5\u6570\u503c\u6a21\u62df\u7684\u7ed3\u679c\uff0c\u8be5\u6846\u67b6\u4e13\u4e3a\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e2d\u7684\u4f20\u8f93\u73b0\u8c61\u5efa\u6a21\u800c\u8bbe\u8ba1\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u6837\u672c\uff0c\u6211\u4eec\u5305\u542b\u4e86\u5728\u4e09\u79cd\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u901f\u5ea6\u3001\u538b\u529b\u548c\u6e29\u5ea6\u573a\u6570\u636e\uff0c\u4ee5\u53ca\u51e0\u4e2a\u4e0e\u5de5\u7a0b\u76f8\u5173\u7684\u6c47\u603b\u7edf\u8ba1\u7279\u5f81\uff08\u5982\u5347\u529b\u7cfb\u6570\u548c\u963b\u529b\u7cfb\u6570\uff0c\u4ee5\u53ca\u52aa\u585e\u5c14\u6570\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5305\u542b\u4e86\u6bcf\u4e2a\u5f62\u72b6\u7684\u63a9\u7801\u548c\u5e26\u7b26\u53f7\u8ddd\u79bb\u573a\u3002\u6211\u4eec\u8bbe\u60f3FlowBench\u5c06\u6709\u52a9\u4e8e\u8bc4\u4f30\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u3001\u8026\u5408\u6d41\u52a8\u73b0\u8c61\u548c\u6570\u636e\u5145\u5206\u6027\u5bf9\u5f53\u524d\u53ca\u672a\u6765\u795e\u7ecfPDE\u6c42\u89e3\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u5217\u4e3e\u4e86\u51e0\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u5e2e\u52a9\u5bf9\u795e\u7ecfPDE\u6c42\u89e3\u5668\u7684\u6027\u80fd\u8fdb\u884c\u6392\u5e8f\u3002\u6211\u4eec\u57fa\u51c6\u6d4b\u8bd5\u4e86\u51e0\u79cd\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5305\u62ecFNO\u3001CNO\u3001WNO\u548cDeepONet\u3002 Ronak Tali PDF N/A FlowBench: A Large Scale Benchmark for Flow Simulation over Complex Geometries Simulating fluid flow around arbitrary shapes is key to solving various engineering problems. However, simulating flow physics across complex geometries remains numerically challenging and computationally resource-intensive, particularly when using conventional PDE solvers. Machine learning methods offer attractive opportunities to create fast and adaptable PDE solvers. However, benchmark datasets to measure the performance of such methods are scarce, especially for flow physics across complex geometries. We introduce FlowBench, a dataset for neural simulators with over 10K samples, which is currently larger than any publicly available flow physics dataset. FlowBench contains flow simulation data across complex geometries (\\textit{parametric vs. non-parametric}), spanning a range of flow conditions (\\textit{Reynolds number and Grashoff number}), capturing a diverse array of flow phenomena (\\textit{steady vs. transient; forced vs. free convection}), and for both 2D and 3D. FlowBench contains over 10K data samples, with each sample the outcome of a fully resolved, direct numerical simulation using a well-validated simulator framework designed for modeling transport phenomena in complex geometries. For each sample, we include velocity, pressure, and temperature field data at 3 different resolutions and several summary statistics features of engineering relevance (such as coefficients of lift and drag, and Nusselt numbers). %Additionally, we include masks and signed distance fields for each shape. We envision that FlowBench will enable evaluating the interplay between complex geometry, coupled flow phenomena, and data sufficiency on the performance of current, and future, neural PDE solvers. We enumerate several evaluation metrics to help rank order the performance of neural PDE solvers. We benchmark the performance of several baseline methods including FNO, CNO, WNO, and DeepONet. \u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4ee3\u7801\u7684\u7ec4\u5408\u786c\u5ea6 -- \u4e00\u4e2a\u6982\u7387\u89c6\u89d2 \u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u7b49\u590d\u6742\u5206\u6790\u4efb\u52a1\u65f6\uff0c\u4e00\u79cd\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u5728\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5185\u91c7\u6837\u6574\u4e2a\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u5b50\u4efb\u52a1\u5206\u89e3\uff08\u5373\u601d\u7ef4\u94fe\uff09\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6307\u51faLLM\u5728\u540c\u4e00\u4e0a\u4e0b\u6587\u7a97\u53e3\u5185\u6267\u884c\u591a\u4e2a\u5b50\u4efb\u52a1\u7684\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\u2014\u2014\u5373\u4e0a\u4e0b\u6587\u4e2d\u7684\u7ec4\u5408\u96be\u5ea6\uff0c\u8fd9\u8868\u660e\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5206\u5e03\u5206\u89e3\u95ee\u9898\u5177\u6709\u4f18\u52bf\u3002\u7ec4\u5408\u96be\u5ea6\u901a\u8fc7\u751f\u6210\u590d\u6742\u5ea6\u6307\u6807\u6765\u91cf\u5316\uff0c\u5373\u91c7\u6837\u81f3\u5c11\u4e00\u4e2a\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u6240\u9700\u7684LLM\u751f\u6210\u6b21\u6570\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u76f8\u5bf9\u4e8e\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u4e4b\u95f4\u5206\u5e03\u4efb\u52a1\uff0c\u5728\u540c\u4e00\u4e0a\u4e0b\u6587\u4e2d\u89e3\u51b3\u7ec4\u5408\u95ee\u9898\u7684\u751f\u6210\u590d\u6742\u5ea6\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u4e14\u968f\u7740\u89e3\u51b3\u65b9\u6848\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u8fd9\u4e00\u5dee\u8ddd\u5448\u6307\u6570\u589e\u957f\u3002\u6211\u4eec\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002 Yotam Wolf PDF N/A Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically. \u4ece\u5bf9\u6297\u89d2\u5ea6\u770bAI\u5b89\u5168\u7684\u673a\u5668\u9057\u5fd8\u673a\u5236 \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\uff0c\u4ee5\u62d2\u7edd\u56de\u7b54\u5173\u4e8e\u5371\u9669\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u4fdd\u62a4\u63aa\u65bd\u5f80\u5f80\u53ef\u4ee5\u88ab\u7ed5\u8fc7\u3002\u9057\u5fd8\u65b9\u6cd5\u65e8\u5728\u5b8c\u5168\u79fb\u9664\u6a21\u578b\u7684\u5371\u9669\u80fd\u529b\uff0c\u4f7f\u5176\u5bf9\u653b\u51fb\u8005\u4e0d\u53ef\u8bbf\u95ee\u3002\u672c\u7814\u7a76\u4ece\u5bf9\u6297\u6027\u89d2\u5ea6\u63a2\u8ba8\u4e86\u9057\u5fd8\u4e0e\u4f20\u7edf\u5b89\u5168\u540e\u8bad\u7ec3\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u5148\u524d\u88ab\u8ba4\u4e3a\u5bf9\u9057\u5fd8\u65e0\u6548\u7684\u73b0\u6709\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5728\u8c28\u614e\u5e94\u7528\u65f6\u53ef\u4ee5\u6210\u529f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u591a\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u6062\u590d\u4e86\u5927\u591a\u6570\u88ab\u8ba4\u4e3a\u5df2\u9057\u5fd8\u7684\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u572810\u4e2a\u4e0d\u76f8\u5173\u7684\u793a\u4f8b\u4e0a\u8fdb\u884c\u5fae\u8c03\u6216\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u79fb\u9664\u7279\u5b9a\u65b9\u5411\uff0c\u53ef\u4ee5\u6062\u590d\u4f7f\u7528RMU\uff08\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u65b9\u6cd5\uff09\u7f16\u8f91\u7684\u6a21\u578b\u7684\u5927\u591a\u6570\u5371\u9669\u80fd\u529b\u3002\u6211\u4eec\u7684\u53d1\u73b0\u6311\u6218\u4e86\u5f53\u524d\u9057\u5fd8\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5bf9\u5176\u76f8\u5bf9\u4e8e\u5b89\u5168\u8bad\u7ec3\u7684\u4f18\u52bf\u63d0\u51fa\u4e86\u8d28\u7591\u3002 Jakub \u0141ucki PDF N/A An Adversarial Perspective on Machine Unlearning for AI Safety Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training. DARE\uff1a\u5177\u6709\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u591a\u6837\u5316\u89c6\u89c9\u95ee\u7b54 \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6269\u5c55\u4e86\u4ec5\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4ec5\u89c6\u89c9\u6a21\u578b\u7684\u663e\u8457\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u4ece\u591a\u6a21\u6001\u89c6\u89c9\u6587\u672c\u8f93\u5165\u4e2d\u5b66\u4e60\u548c\u5904\u7406\u4fe1\u606f\u3002\u5c3d\u7ba1\u73b0\u4ee3VLMs\u5728\u8bb8\u591a\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u8bb8\u591a\u5173\u952e\u7684\u89c6\u89c9\u8bed\u8a00\uff08VL\uff09\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u7136\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\u8ba1\u6570\u548c\u7a7a\u95f4\u63a8\u7406\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5bf9\u6307\u4ee4\u548c/\u6216\u8bc4\u4f30\u534f\u8bae\u7684\u5c0f\u53d8\u5316\u53ef\u80fd\u975e\u5e38\u8106\u5f31\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u8bc4\u4f30\u5b83\u4eec\u7684\u9c81\u68d2\u6027\uff08\u6216\u8005\u66f4\u786e\u5207\u5730\u8bf4\uff0c\u662f\u7f3a\u4e4f\u9c81\u68d2\u6027\uff09\u3002\u4e3a\u4e86\u5c06\u5177\u6709\u6311\u6218\u6027\u7684VL\u573a\u666f\u4e0e\u5168\u9762\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u7ed3\u5408\u8d77\u6765\uff0c\u6211\u4eec\u5f15\u5165\u4e86DARE\uff0c\u5373\u591a\u6837\u5316\u7684\u89c6\u89c9\u95ee\u7b54\u4e0e\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u521b\u5efa\u548c\u7b56\u5212\u7684\u591a\u9879\u9009\u62e9VQA\u57fa\u51c6\u3002DARE\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u7c7b\u522b\u4e0a\u8bc4\u4f30VLM\u7684\u6027\u80fd\uff0c\u5e76\u5305\u62ec\u56db\u4e2a\u57fa\u4e8e\u63d0\u793a\u3001\u7b54\u6848\u9009\u9879\u5b50\u96c6\u3001\u8f93\u51fa\u683c\u5f0f\u548c\u6b63\u786e\u7b54\u6848\u6570\u91cf\u7684\u53d8\u5316\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u5728\u5176\u4ed6\u4e00\u7cfb\u5217\u53d1\u73b0\u4e2d\uff0c\u6211\u4eec\u62a5\u544a\u79f0\uff0c\u6700\u5148\u8fdb\u7684VLMs\u5728\u5927\u591a\u6570\u7c7b\u522b\u7684\u95ee\u9898\u4e0a\u4ecd\u7136\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u5728\u6d4b\u8bd5\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4e2d\u65e0\u6cd5\u6301\u7eed\u53d1\u6325\u5176\u6700\u4f73\u6027\u80fd\u3002\u5728\u9009\u9879\u5b50\u96c6\u4e2d\u7684\u6700\u5dee\u60c5\u51b5\u6027\u80fd\u6bd4\u6807\u51c6\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4f4e\u8fbe34%\u3002\u5f00\u6e90VLMs\u5982LLaVA 1.6\u548cIdefics2\u7684\u9c81\u68d2\u6027\u65e0\u6cd5\u4e0e\u95ed\u6e90\u6a21\u578b\u5982GPT-4\u548cGemini\u76f8\u5ab2\u7f8e\uff0c\u4f46\u5373\u4f7f\u662f\u540e\u8005\u5bf9\u4e0d\u540c\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e5f\u975e\u5e38\u8106\u5f31\u3002 Hannah Sterz PDF N/A DARE: Diverse Visual Question Answering with Robustness Evaluation Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, and are able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE, Diverse Visual Question Answering with Robustness Evaluation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of: prompts, the subsets of answer options, the output format and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. The worst case performance across the subsets of options is up to 34% below the performance in the standard case. The robustness of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the closed-source models such as GPT-4 and Gemini, but even the latter remain very brittle to different variations. \u5c06\u8d85\u5bfc\u5149\u7535\u7f51\u7edc\u4e0e\u7ecf\u5178\u795e\u7ecf\u52a8\u529b\u5b66\u8054\u7cfb\u8d77\u6765 \u7531\u8d85\u5bfc\u5149\u7535\u7a81\u89e6\u3001\u6811\u7a81\u548c\u795e\u7ecf\u5143\u7ec4\u6210\u7684\u7535\u8def\uff0c\u5176\u63cf\u8ff0\u4f9d\u8d56\u4e8e\u6570\u503c\u590d\u6742\u4e14\u5f62\u5f0f\u6666\u6da9\u7684\u8026\u5408\u5fae\u5206\u65b9\u7a0b\u3002\u53c2\u8003\u6587\u732e1\u8868\u660e\uff0c\u8d85\u5bfc\u73af\u72b6\u795e\u7ecf\u5143\u7684\u73b0\u8c61\u5b66\u6a21\u578b\u6d88\u9664\u4e86\u6c42\u89e3\u63cf\u8ff0\u7a81\u89e6\u548c\u6811\u7a81\u7684\u7ea6\u745f\u592b\u68ee\u7535\u8def\u65b9\u7a0b\u7684\u9700\u6c42\u3002\u8be5\u6a21\u578b\u7684\u6700\u521d\u76ee\u6807\u662f\u51cf\u5c11\u6a21\u62df\u6240\u9700\u7684\u65f6\u95f4\uff0c\u7136\u800c\uff0c\u5176\u989d\u5916\u7684\u597d\u5904\u662f\u63d0\u9ad8\u4e86\u5bf9\u5e95\u5c42\u795e\u7ecf\u7535\u8def\u64cd\u4f5c\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u589e\u5f3a\u4e86\u5173\u4e8e\u73af\u72b6\u795e\u7ecf\u5143\u4e0e\u5176\u4ed6\u7269\u7406\u7cfb\u7edf\u8fde\u63a5\u7684\u6982\u5ff5\u6e05\u6670\u5ea6\u3002\u5c3d\u7ba1\u539f\u59cb\u6a21\u578b\u901a\u8fc7\u4ec5\u8003\u8651\u6811\u7a81\u8f93\u51fa\u7684\u4f4e\u901a\u7248\u672c\u7b80\u5316\u4e86\u7ea6\u745f\u592b\u68ee\u7ed3\u52a8\u529b\u5b66\u7684\u5904\u7406\uff0c\u4f46\u8be5\u6a21\u578b\u5bf9\u7531\u534a\u5bfc\u4f53\u53d1\u5c04\u7535\u8def\u4ea7\u751f\u7684\u5c16\u5cf0\u91c7\u7528\u4e86\u5c34\u5c2c\u7684\u5904\u7406\u65b9\u5f0f\uff0c\u8fd9\u9700\u8981\u660e\u786e\u68c0\u67e5\u9608\u503c\u4ea4\u53c9\uff0c\u5e76\u5bf9\u8fbe\u5230\u80de\u4f53\u9608\u503c\u7684\u65f6\u95f4\u6b65\u957f\u8fdb\u884c\u4e0d\u540c\u7684\u5904\u7406\u3002\u5728\u6b64\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u8be5\u6a21\u578b\uff0c\u4ee5\u7b80\u5316\u6765\u81ea\u80de\u4f53\u7684\u5c16\u5cf0\u5904\u7406\uff0c\u518d\u6b21\u5229\u7528\u4e86\u795e\u7ecf\u7cfb\u7edf\u4e2d\u5c16\u5cf0\u4e8b\u4ef6\u7684\u4e0b\u6e38\u63a5\u6536\u8005\u51e0\u4e4e\u603b\u662f\u6267\u884c\u4f4e\u901a\u6ee4\u6ce2\u7684\u4e8b\u5b9e\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u7b2c\u4e00\u548c\u7b2c\u4e8c\u73b0\u8c61\u5b66\u6a21\u578b\u4e4b\u95f4\u7684\u6bd4\u8f83\uff0c\u91cf\u5316\u4e86\u989d\u5916\u8fd1\u4f3c\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u786e\u5b9a\u4e86\u6269\u5c55\u6a21\u578b\u5de5\u4f5c\u826f\u597d\u7684\u7535\u8def\u53c2\u6570\u7a7a\u95f4\u533a\u57df\uff0c\u4ee5\u53ca\u5176\u8868\u73b0\u4e0d\u4f73\u7684\u533a\u57df\u3002\u5bf9\u4e8e\u67d0\u4e9b\u7535\u8def\u53c2\u6570\uff0c\u53ef\u4ee5\u8868\u793a\u4e0b\u6e38\u6811\u7a81\u5bf9\u5355\u4e2a\u5c16\u5cf0\u4ee5\u53ca\u5c16\u5cf0\u91cd\u5408\u6216\u5e8f\u5217\u7684\u54cd\u5e94\uff0c\u8fd9\u8868\u660e\u8be5\u6a21\u578b\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5316\u4e3a\u901f\u7387\u7f16\u7801\u3002\u4e3b\u5bfc\u65b9\u7a0b\u51e0\u4e4e\u4e0e\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u4e2d\u7528\u4e8e\u5efa\u6a21\u6cc4\u6f0f\u79ef\u5206\u5668\u6811\u7a81\u548c\u795e\u7ecf\u5143\u7684\u65b9\u7a0b\u76f8\u540c\u3002 Jeffrey M. Shainline PDF N/A Relating Superconducting Optoelectronic Networks to Classical Neurodynamics The circuits comprising superconducting optoelectronic synapses, dendrites, and neurons are described by numerically cumbersome and formally opaque coupled differential equations. Reference 1 showed that a phenomenological model of superconducting loop neurons eliminates the need to solve the Josephson circuit equations that describe synapses and dendrites. The initial goal of the model was to decrease the time required for simulations, yet an additional benefit of the model was increased transparency of the underlying neural circuit operations and conceptual clarity regarding the connection of loop neurons to other physical systems. Whereas the original model simplified the treatment of the Josephson-junction dynamics, essentially by only considering low-pass versions of the dendritic outputs, the model resorted to an awkward treatment of spikes generated by semiconductor transmitter circuits that required explicitly checking for threshold crossings and distinct treatment of time steps wherein somatic threshold is reached. Here we extend that model to simplify the treatment of spikes coming from somas, again making use of the fact that in neural systems the downstream recipients of spike events almost always perform low-pass filtering. We provide comparisons between the first and second phenomenological models, quantifying the accuracy of the additional approximations. We identify regions of circuit parameter space in which the extended model works well and regions where it works poorly. For some circuit parameters it is possible to represent the downstream dendritic response to a single spike as well as coincidences or sequences of spikes, indicating the model is not simply a reduction to rate coding. The governing equations are shown to be nearly identical to those ubiquitous in the neuroscience literature for modeling leaky-integrator dendrites and neurons. \u57fa\u4e8e\u7ec6\u80de\u5d4c\u5165\u56fe\u7684\u7a7a\u95f4\u65f6\u95f4\u5b66\u4e60 \u6570\u636e\u9a71\u52a8\u7684\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u8fd1\u671f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5176\u4e2d\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u88ab\u5f00\u53d1\u51fa\u6765\u3002\u7279\u522b\u662f\u57fa\u4e8e\u7f51\u683c\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u9884\u6d4b\u4efb\u610f\u51e0\u4f55\u57df\u4e0a\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\u65b9\u9762\u5c55\u73b0\u4e86\u663e\u8457\u6f5c\u529b\u3002\u7136\u800c\uff0cGNNs\u4e2d\u73b0\u6709\u7684\u8282\u70b9-\u8fb9\u6d88\u606f\u4f20\u9012\u673a\u5236\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u80de\u5d4c\u5165\u5f0fGNN\u6a21\u578b\uff08\u7b80\u79f0CeGNN\uff09\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u5b66\u4e60\u65f6\u7a7a\u52a8\u529b\u5b66\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5728\u8282\u70b9-\u8fb9\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7ec6\u80de\u5c5e\u6027\uff0c\u66f4\u597d\u5730\u6355\u6349\u533a\u57df\u7279\u5f81\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u8fd9\u79cd\u7b56\u7565\u5b9e\u8d28\u4e0a\u5c06\u5c40\u90e8\u805a\u5408\u65b9\u6848\u4ece\u4e00\u9636\uff08\u4f8b\u5982\u4ece\u8fb9\u5230\u8282\u70b9\uff09\u5347\u7ea7\u5230\u66f4\u9ad8\u9636\uff08\u4f8b\u5982\u4ece\u4f53\u79ef\u5230\u8fb9\u518d\u5230\u8282\u70b9\uff09\uff0c\u4ece\u800c\u5728\u6d88\u606f\u4f20\u9012\u4e2d\u5229\u7528\u4e86\u4f53\u79ef\u4fe1\u606f\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u589e\u5f3a\u5757\uff0c\u901a\u8fc7\u5c06\u6f5c\u5728\u7279\u5f81\u89c6\u4e3a\u57fa\u51fd\u6570\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8CeGNN\u7684\u6027\u80fd\u5e76\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002\u5728\u591a\u79cd\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u548c\u4e00\u9879\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCeGNN\u76f8\u8f83\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u51e0\u4e2a\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u4e0a\u5c06\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u591a\u8fbe\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002 Yuan Mi PDF N/A Spatiotemporal Learning on Cell-embedded Graphs Data-driven simulation of physical systems has recently kindled significant attention, where many neural models have been developed. In particular, mesh-based graph neural networks (GNNs) have demonstrated significant potential in predicting spatiotemporal dynamics across arbitrary geometric domains. However, the existing node-edge message passing mechanism in GNNs limits the model's representation learning ability. In this paper, we proposed a cell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics with lifted performance. Specifically, we introduce a learnable cell attribution to the node-edge message passing process, which better captures the spatial dependency of regional features. Such a strategy essentially upgrades the local aggregation scheme from the first order (e.g., from edge to node) to a higher order (e.g., from volume to edge and then to node), which takes advantage of volumetric information in message passing. Meanwhile, a novel feature-enhanced block is designed to further improve the performance of CeGNN and relieve the over-smoothness problem, via treating the latent features as basis functions. The extensive experiments on various PDE systems and one real-world dataset demonstrate that CeGNN achieves superior performance compared with other baseline models, particularly reducing the prediction error with up to 1 orders of magnitude on several PDE systems. \u591a\u8bed\u8a00\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e0e\u63a8\u7406\u8bc4\u4f30 \u6700\u8fd1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65b9\u9762\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u5176\u4e2d\u4e00\u4e9b\u5728\u5408\u6210\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u6587\u672c\u4e0a\uff0c\u5e76\u4e14\u6d89\u53ca\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u5355\u4e2a\u76ee\u6807\u53e5\u5b50\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7814\u7a76\u4e86LLM\u6027\u80fd\u5982\u4f55\u6cdb\u5316\u5230\u5305\u542b\u591a\u4e2a\u9690\u85cf\u76ee\u6807\u53e5\u5b50\u7684\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u3002\u6211\u4eec\u5168\u9762\u8bc4\u4f30\u4e86\u51e0\u79cd\u957f\u4e0a\u4e0b\u6587LLM\u5728\u4e94\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u8d8a\u5357\u8bed\u3001\u5370\u5c3c\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\u548c\u7d22\u9a6c\u91cc\u8bed\uff09\u4e0a\u7684\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u3002\u8fd9\u4e9b\u8bed\u8a00\u5171\u4eab\u62c9\u4e01\u5b57\u6bcd\uff0c\u4f46\u5c5e\u4e8e\u4e0d\u540c\u7684\u8bed\u8a00\u5bb6\u65cf\u548c\u8d44\u6e90\u6c34\u5e73\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u8bed\u8a00\u4e4b\u95f4\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5982Gemini-1.5\u548cGPT-4o\uff0c\u5728\u82f1\u8bed\u4e2d\u8fbe\u5230\u7ea696%\u7684\u51c6\u786e\u7387\uff0c\u800c\u5728\u7d22\u9a6c\u91cc\u8bed\u4e2d\u4ec5\u4e3a\u7ea636%\uff0c\u5f53\u5904\u7406\u5355\u4e2a\u76ee\u6807\u53e5\u5b50\u65f6\u3002\u7136\u800c\uff0c\u5f53\u5904\u7406\u4e09\u4e2a\u76ee\u6807\u53e5\u5b50\u65f6\uff0c\u8fd9\u4e00\u51c6\u786e\u7387\u5728\u82f1\u8bed\u4e2d\u4e0b\u964d\u523040%\uff0c\u5728\u7d22\u9a6c\u91cc\u8bed\u4e2d\u4e0b\u964d\u52300%\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u5728\u5904\u7406\u66f4\u957f\u4e0a\u4e0b\u6587\u3001\u589e\u52a0\u76ee\u6807\u53e5\u5b50\u6570\u91cf\u6216\u8d44\u6e90\u6c34\u5e73\u8f83\u4f4e\u7684\u8bed\u8a00\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\u3002 Ameeta Agrawal PDF N/A Multilingual Evaluation of Long Context Retrieval and Reasoning Recent large language models (LLMs) demonstrate impressive capabilities in handling long contexts, some exhibiting near-perfect recall on synthetic retrieval tasks. However, these evaluations have mainly focused on English text and involved a single target sentence within lengthy contexts. Our work investigates how LLM performance generalizes to multilingual settings with multiple hidden target sentences. We comprehensively evaluate several long-context LLMs on retrieval and reasoning tasks across five languages: English, Vietnamese, Indonesian, Swahili, and Somali. These languages share the Latin script but belong to distinct language families and resource levels. Our analysis reveals a significant performance gap between languages. The best-performing models such as Gemini-1.5 and GPT-4o, achieve around 96% accuracy in English to around 36% in Somali with a single target sentence. However, this accuracy drops to 40% in English and 0% in Somali when dealing with three target sentences. Our findings highlight the challenges long-context LLMs face when processing longer contexts, an increase in the number of target sentences, or languages of lower resource levels. \u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u548c\u65f6\u7a7a\u6838\u7684\u5b89\u5168\u65f6\u53d8\u4f18\u5316 \u786e\u4fdd\u5b89\u5168\u662f\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u4f8b\u5982\u673a\u5668\u4eba\u6280\u672f\u6216\u8fc7\u7a0b\u63a7\u5236\u3002\u5e95\u5c42\u7cfb\u7edf\u7684\u590d\u6742\u6027\u901a\u5e38\u4f7f\u5f97\u5bfb\u627e\u6700\u4f18\u51b3\u7b56\u53d8\u5f97\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u514b\u670d\u5728\u672a\u77e5\u65f6\u95f4\u53d8\u5316\u7684\u5b89\u5168\u7ea6\u675f\u4e0b\u4f18\u5316\u672a\u77e5\u65f6\u95f4\u53d8\u5316\u5956\u52b1\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86TVSafeOpt\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u4e14\u5e26\u6709\u65f6\u7a7a\u6838\u7684\u65b0\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u9700\u8981\u663e\u5f0f\u53d8\u5316\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\u5b89\u5168\u5730\u8ddf\u8e2a\u65f6\u95f4\u53d8\u5316\u7684\u5b89\u5168\u533a\u57df\u3002\u5f53\u4f18\u5316\u95ee\u9898\u53d8\u5f97\u5e73\u7a33\u65f6\uff0c\u6211\u4eec\u4e5f\u4e3a\u8be5\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6700\u4f18\u6027\u4fdd\u8bc1\u3002\u6211\u4eec\u5c55\u793a\u4e86TVSafeOpt\u5728\u5408\u6210\u6570\u636e\u4e0a\u4e0eSafeOpt\u76f8\u6bd4\uff0c\u5728\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002\u5728\u4e00\u4e2a\u6d89\u53ca\u6c14\u4f53\u538b\u7f29\u673a\u7684\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u4e2d\u7684\u8bc4\u4f30\u8bc1\u5b9e\uff0cTVSafeOpt\u5728\u89e3\u51b3\u5177\u6709\u672a\u77e5\u5956\u52b1\u548c\u5b89\u5168\u51fd\u6570\u7684\u65f6\u95f4\u53d8\u5316\u4f18\u5316\u95ee\u9898\u65f6\u80fd\u591f\u786e\u4fdd\u5b89\u5168\u3002 Jialin Li PDF N/A Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel Ensuring safety is a key aspect in sequential decision making problems, such as robotics or process control. The complexity of the underlying systems often makes finding the optimal decision challenging, especially when the safety-critical system is time-varying. Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSafeOpt, a new algorithm built on Bayesian optimization with a spatio-temporal kernel. The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection. Optimality guarantees are also provided for the algorithm when the optimization problem becomes stationary. We show that TVSafeOpt compares favorably against SafeOpt on synthetic data, both regarding safety and optimality. Evaluation on a realistic case study with gas compressors confirms that TVSafeOpt ensures safety when solving time-varying optimization problems with unknown reward and safety functions. PhoCoLens\uff1a\u65e0\u900f\u955c\u6210\u50cf\u4e2d\u7684\u903c\u771f\u4e00\u81f4\u91cd\u5efa \u65e0\u900f\u955c\u76f8\u673a\u5728\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u6210\u672c\u65b9\u9762\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u900f\u955c\u7684\u7cfb\u7edf\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u7531\u4e8e\u6ca1\u6709\u805a\u7126\u900f\u955c\uff0c\u65e0\u900f\u955c\u76f8\u673a\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u7b97\u6cd5\u4ece\u591a\u8def\u590d\u7528\u6d4b\u91cf\u4e2d\u6062\u590d\u573a\u666f\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u7b97\u6cd5\u5728\u9762\u5bf9\u4e0d\u51c6\u786e\u7684\u524d\u5411\u6210\u50cf\u6a21\u578b\u548c\u4e0d\u8db3\u7684\u5148\u9a8c\u4fe1\u606f\u65f6\uff0c\u96be\u4ee5\u91cd\u5efa\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e00\u81f4\u4e14\u903c\u771f\u7684\u65e0\u900f\u955c\u56fe\u50cf\u91cd\u5efa\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u91c7\u7528\u7a7a\u95f4\u53d8\u5316\u7684\u53cd\u5377\u79ef\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u7cbe\u786e\u91cd\u5efa\u4f4e\u9891\u5185\u5bb9\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u70b9\u6269\u6563\u51fd\u6570\uff08PSF\uff09\u5728\u6574\u4e2a\u76f8\u673a\u89c6\u573a\u4e2d\u7684\u53d8\u5316\uff0c\u4ece\u800c\u786e\u4fdd\u6570\u636e\u4e00\u81f4\u6027\u3002\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u6765\u589e\u5f3a\u56fe\u50cf\u7684\u903c\u771f\u5ea6\u3002\u901a\u8fc7\u4ee5\u7b2c\u4e00\u9636\u6bb5\u83b7\u53d6\u7684\u4f4e\u9891\u5185\u5bb9\u4e3a\u6761\u4ef6\uff0c\u6269\u6563\u6a21\u578b\u6709\u6548\u5730\u91cd\u5efa\u4e86\u901a\u5e38\u5728\u65e0\u900f\u955c\u6210\u50cf\u8fc7\u7a0b\u4e2d\u4e22\u5931\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u3002\u4e0e\u73b0\u6709\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u8fd9\u4e00\u70b9\u901a\u8fc7\u4e24\u79cd\u6d41\u884c\u7684\u65e0\u900f\u955c\u7cfb\u7edf\u2014\u2014PhlatCam\u548cDiffuserCam\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://phocolens.github.io/\u3002 Xin Cai PDF N/A PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam. Project website: https://phocolens.github.io/. \u4f7f\u7528\u6269\u6563\u7684\u8054\u5408\u5b9a\u4f4d\u4e0e\u89c4\u5212 \u6269\u6563\u6a21\u578b\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u95ee\u9898\uff0c\u5982\u64cd\u63a7\u548c\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5c06\u5176\u5e94\u7528\u4e8e\u7aef\u5230\u7aef\u5bfc\u822a\u2014\u2014\u5305\u62ec\u611f\u77e5\u548c\u89c4\u5212\u2014\u2014\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u5728\u5df2\u77e5\u4f46\u4efb\u610f\u76842D\u73af\u5883\u4e2d\u8054\u5408\u6267\u884c\u5168\u5c40\u5b9a\u4f4d\u548c\u8def\u5f84\u89c4\u5212\u7684\u95ee\u9898\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u7ed9\u5b9a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684LIDAR\u626b\u63cf\u3001\u4efb\u610f\u5730\u56fe\u548c\u671f\u671b\u76ee\u6807\u4f4d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5168\u5c40\u53c2\u8003\u6846\u67b6\u4e2d\u751f\u6210\u65e0\u78b0\u649e\u8def\u5f84\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728SE(2)\u8def\u5f84\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u6269\u6563\uff0c\u5e76\u63cf\u8ff0\u4e86\u5982\u4f55\u6839\u636e\u969c\u788d\u7269\u548c\u4f20\u611f\u5668\u89c2\u6d4b\u6765\u8c03\u8282\u53bb\u566a\u8fc7\u7a0b\u3002\u5728\u6211\u4eec\u7684\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u8c03\u8282\u6280\u672f\u80fd\u591f\u63a8\u5e7f\u5230\u4e0e\u8bad\u7ec3\u73af\u5883\u5916\u89c2\u663e\u8457\u4e0d\u540c\u7684\u73b0\u5b9e\u5730\u56fe\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u6a21\u578b\u51c6\u786e\u63cf\u8ff0\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4eff\u771f\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u6a21\u578b\u4f5c\u4e3a\u5b9e\u65f6\u7aef\u5230\u7aef\u5b9a\u4f4d\u548c\u89c4\u5212\u5806\u6808\u7684\u4f7f\u7528\u3002 L. Lao Beyer PDF N/A Joint Localization and Planning using Diffusion Diffusion models have been successfully applied to robotics problems such as manipulation and vehicle path planning. In this work, we explore their application to end-to-end navigation -- including both perception and planning -- by considering the problem of jointly performing global localization and path planning in known but arbitrary 2D environments. In particular, we introduce a diffusion model which produces collision-free paths in a global reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired goal position. To this end, we implement diffusion in the space of paths in SE(2), and describe how to condition the denoising process on both obstacles and sensor observations. In our evaluation, we show that the proposed conditioning techniques enable generalization to realistic maps of considerably different appearance than the training environment, demonstrate our model's ability to accurately describe ambiguous solutions, and run extensive simulation experiments showcasing our model's use as a real-time, end-to-end localization and planning stack. LoopSR\uff1a\u7ec8\u8eab\u7b56\u7565\u9002\u5e94\u817f\u8db3\u673a\u5668\u4eba\u7684\u5faa\u73af\u4eff\u771f\u4e0e\u73b0\u5b9e \u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u901a\u8fc7\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8db3\u5f0f\u8fd0\u52a8\u4e2d\u7684\u663e\u8457\u4e14\u53ef\u6cdb\u5316\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u50cf\u9886\u57df\u968f\u673a\u5316\u8fd9\u6837\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u6709\u671b\u4f7f\u7b56\u7565\u5bf9\u591a\u6837\u5316\u7684\u73af\u5883\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u6839\u636e\u65e0\u514d\u8d39\u5348\u9910\u5b9a\u7406\uff0c\u8fd9\u79cd\u5168\u9762\u6027\u53ef\u80fd\u4f1a\u524a\u5f31\u7b56\u7565\u5728\u4efb\u4f55\u7279\u5b9a\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u65f6\u4ea7\u751f\u6b21\u4f18\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoopSR\u7684\u7ec8\u8eab\u7b56\u7565\u9002\u5e94\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u8f68\u8ff9\u6295\u5f71\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u76f8\u5e94\u5730\u5728\u6a21\u62df\u4e2d\u91cd\u5efa\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4ee5\u8fdb\u884c\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u6211\u4eec\u91c7\u7528\u4e86\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u63d0\u53d6\u73b0\u5b9e\u4e16\u754c\u52a8\u529b\u5b66\u7684\u7279\u5f81\u3002\u6301\u7eed\u8bad\u7ec3\u7684\u6a21\u62df\u53c2\u6570\u662f\u901a\u8fc7\u7ed3\u5408\u89e3\u7801\u5668\u9884\u6d4b\u7684\u53c2\u6570\u548c\u4ece\u6a21\u62df\u8f68\u8ff9\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u7684\u53c2\u6570\u5f97\u51fa\u7684\u3002\u901a\u8fc7\u5229\u7528\u6301\u7eed\u8bad\u7ec3\uff0cLoopSR\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u4ec5\u4f7f\u7528\u6709\u9650\u7684\u6570\u636e\u91cf\u5c31\u80fd\u5728\u6a21\u62df\u5230\u6a21\u62df\u548c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u5353\u8d8a\u7684\u6027\u80fd\u3002 Peilin Wu PDF N/A LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots Reinforcement Learning (RL) has shown its remarkable and generalizable capability in legged locomotion through sim-to-real transfer. However, while adaptive methods like domain randomization are expected to make policy more robust to diverse environments, such comprehensiveness potentially detracts from the policy's performance in any specific environment according to the No Free Lunch theorem, leading to a suboptimal solution once deployed in the real world. To address this issue, we propose a lifelong policy adaptation framework named LoopSR, which utilizes a transformer-based encoder to project real-world trajectories into a latent space, and accordingly reconstruct the real-world environments back in simulation for further improvement. Autoencoder architecture and contrastive learning methods are adopted to better extract the characteristics of real-world dynamics. The simulation parameters for continual training are derived by combining predicted parameters from the decoder with retrieved parameters from the simulation trajectory dataset. By leveraging the continual training, LoopSR achieves superior data efficiency compared with strong baselines, with only a limited amount of data to yield eminent performance in both sim-to-sim and sim-to-real experiments. \u9ad8\u7ef4\u5206\u7c7b\u95ee\u9898\u7684\u7ef4\u5ea6\u65e0\u5173\u5b66\u4e60\u7387 \u6211\u4eec\u7814\u7a76\u4e86\u5728$RBV^2$\u7a7a\u95f4\u4e2d\u5177\u6709\u51b3\u7b56\u8fb9\u754c\u7684\u5206\u7c7b\u51fd\u6570\u7684\u903c\u8fd1\u548c\u4f30\u8ba1\u95ee\u9898\u3002$RBV^2$\u7c7b\u578b\u7684\u51fd\u6570\u81ea\u7136\u5730\u4f5c\u4e3a\u6b63\u5219\u5316\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u95ee\u9898\u7684\u89e3\u51fa\u73b0\uff0c\u5e76\u4e14\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5728\u4e0d\u53d7\u5230\u7ef4\u5ea6\u707e\u96be\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\u903c\u8fd1\u8fd9\u4e9b\u51fd\u6570\u3002\u6211\u4eec\u5bf9\u73b0\u6709\u7ed3\u679c\u8fdb\u884c\u4e86\u4fee\u6539\uff0c\u4ee5\u8bc1\u660e\u6bcf\u4e2a$RBV^2$\u51fd\u6570\u90fd\u53ef\u4ee5\u901a\u8fc7\u5177\u6709\u6709\u754c\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u903c\u8fd1\u3002\u968f\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u5177\u6709\u6709\u754c\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u903c\u8fd1\u5206\u7c7b\u51fd\u6570\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e9b\u754c\u9650\u6765\u91cf\u5316\u4f30\u8ba1\u901f\u7387\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u6570\u503c\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6b63\u5219\u6027\u6761\u4ef6\u5bf9\u51b3\u7b56\u8fb9\u754c\u7684\u5f71\u54cd\u3002 Andres Felipe Lerma-Pineda PDF N/A Dimension-independent learning rates for high-dimensional classification problems We study the problem of approximating and estimating classification functions that have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$ type arise naturally as solutions of regularized neural network learning problems and neural networks can approximate these functions without the curse of dimensionality. We modify existing results to show that every $RBV^2$ function can be approximated by a neural network with bounded weights. Thereafter, we prove the existence of a neural network with bounded weights approximating a classification function. And we leverage these bounds to quantify the estimation rates. Finally, we present a numerical study that analyzes the effect of different regularity conditions on the decision boundaries. \u4ece\u7eb5\u5411\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u63d0\u53d6\u60c5\u611f\u805a\u5408\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u9002\u914d\u5668\u8fdb\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406 \u672c\u6587\u63d0\u51fa\u5c06\u65f6\u95f4\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7eb5\u5411\u5206\u6790\u7684\u5de5\u5177\u3002\u6211\u4eec\u5728\u4e00\u7ec4\u82f1\u56fdTwitter\u7528\u6237\u7684\u5b8c\u6574\u65f6\u95f4\u7ebf\u4e0a\u5bf9Llama 3 8B\u8fdb\u884c\u4e86\u65f6\u95f4\u9002\u914d\u5668\u7684\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528\u65e2\u5b9a\u95ee\u5377\u63d0\u53d6\u4e86\u60c5\u611f\u548c\u6001\u5ea6\u7684\u7eb5\u5411\u805a\u5408\u6570\u636e\u3002\u6211\u4eec\u5c06\u4f30\u8ba1\u7ed3\u679c\u4e0e\u4ee3\u8868\u6027\u7684\u82f1\u56fd\u8c03\u67e5\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u53d1\u73b0\u591a\u4e2a\u96c6\u4f53\u60c5\u611f\u5b58\u5728\u663e\u8457\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002\u6240\u83b7\u5f97\u7684\u4f30\u8ba1\u503c\u5728\u591a\u4e2a\u8bad\u7ec3\u79cd\u5b50\u548c\u63d0\u793a\u516c\u5f0f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u4e0e\u4f7f\u7528\u4f20\u7edf\u5206\u7c7b\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\u96c6\u4f53\u60c5\u611f\u63d0\u53d6\u7ed3\u679c\u4e00\u81f4\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u901a\u8fc7\u65f6\u95f4\u9002\u914d\u5668\u5c06LLMs\u4e2d\u7684\u60c5\u611f\u5206\u6790\u6269\u5c55\u5230\u7eb5\u5411\u8bbe\u7f6e\u7684\u5de5\u4f5c\u3002\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7684\u7eb5\u5411\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002 Georg Ahnert PDF N/A Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users, and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We validate our estimates against representative British survey data and find strong positive, significant correlations for several collective emotions. The obtained estimates are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. To the best of our knowledge, this is the first work to extend the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. Our work enables new approaches towards the longitudinal analysis of social media data. \u52a8\u6001\u56fe\u4e0a\u7684Transformer\u7684\u8d85\u62c9\u666e\u62c9\u65af\u7f16\u7801 \u5168\u8fde\u63a5\u56fe\u53d8\u6362\u5668\uff08GT\uff09\u5728\u9759\u6001\u56fe\u793e\u533a\u4e2d\u8fc5\u901f\u5d2d\u9732\u5934\u89d2\uff0c\u6210\u4e3a\u6d88\u606f\u4f20\u9012\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u540e\u8005\u5b58\u5728\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u3001\u8fc7\u5ea6\u6324\u538b\u548c\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u5feb\u7167\u4e2d\u5c06\u6240\u6709\u8282\u70b9\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e92\u8054\uff0cGT\u5931\u53bb\u4e86\u7ed3\u6784\u548c\u65f6\u95f4\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u7a7a\u53d8\u6362\u5668\u7684\u8d85\u62c9\u666e\u62c9\u65af\u7f16\u7801\uff08SLATE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u7f16\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u5229\u7528GT\u67b6\u6784\u7684\u540c\u65f6\u4fdd\u7559\u65f6\u7a7a\u4fe1\u606f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5c06\u79bb\u6563\u65f6\u95f4\u52a8\u6001\u56fe\u8f6c\u5316\u4e3a\u591a\u5c42\u56fe\uff0c\u5e76\u5229\u7528\u5176\u76f8\u5173\u8d85\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7684\u8c31\u7279\u6027\u3002\u6211\u4eec\u7684\u7b2c\u4e8c\u4e2a\u8d21\u732e\u662f\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u8282\u70b9\u95f4\u7684\u6210\u5bf9\u5173\u7cfb\uff0c\u4e3a\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u63d0\u4f9b\u51c6\u786e\u7684\u8fb9\u8868\u793a\u3002SLATE\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4f17\u591a\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u5faa\u73af\u6a21\u578b\uff08\u5982LSTM\uff09\u4ee5\u53ca\u52a8\u6001\u56fe\u53d8\u6362\u5668\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u91cd\u73b0\u6211\u4eec\u7ed3\u679c\u7684\u8bf4\u660e\u5c06\u5f00\u6e90\u3002 Yannis Karmim PDF N/A Supra-Laplacian Encoding for Transformer on Dynamic Graphs Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching. However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention, GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information. Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix. Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction. SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g LSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions to reproduce our results will be open-sourced. \u53bb\u4e2d\u5fc3\u5316\u8d44\u6e90\u5206\u914d\u591a\u7528\u6237\u8bed\u4e49\u901a\u4fe1\u7684\u8d85\u535a\u5f08\u7406\u8bba \u8bed\u4e49\u901a\u4fe1\uff08SC\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u901a\u4fe1\u8303\u5f0f\uff0c\u65e0\u7ebf\u8bbe\u5907\u4ec5\u4ece\u6570\u636e\u6e90\u53d1\u9001\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u4f9d\u8d56\u8ba1\u7b97\u8d44\u6e90\u6765\u518d\u751f\u7f3a\u5931\u7684\u6570\u636e\u70b9\u3002\u7136\u800c\uff0c\u7531\u4e8e\u534f\u8c03\u6240\u9700\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u591a\u7528\u6237SC\u7cfb\u7edf\u7684\u8bbe\u8ba1\u53d8\u5f97\u66f4\u52a0\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u5b66\u4e60\u8bed\u4e49\u8bed\u8a00\u548c\u6267\u884c\u8d44\u6e90\u5206\u914d\u7684\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u672a\u80fd\u6355\u6349\u5230\u591a\u7528\u6237SC\u4e2d\u6d89\u53ca\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6743\u8861\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u6563\u5f0f\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u5206\u914d\u6846\u67b6\u3002\u901a\u8fc7\u5e94\u7528\u65af\u5854\u514b\u5c14\u4f2f\u683c\u8d85\u535a\u5f08\u7406\u8bba\uff0c\u89e3\u51b3\u4e86\u5728\u5206\u6563\u5f0f\u73af\u5883\u4e2d\u9ad8\u6548\u5206\u914d\u901a\u4fe1\u548c\u8ba1\u7b97\u8d44\u6e90\uff08\u7528\u4e8e\u63a8\u7406\uff09\u4ee5\u6700\u5927\u5316\u7ec8\u7aef\u7528\u6237\u4efb\u52a1\u4f53\u9a8c\u8d28\u91cf\u7684\u6311\u6218\u3002\u5229\u7528\u4e8c\u7ea7\u8d85\u535a\u5f08\u7684\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u5206\u6790\u516c\u5f0f\u6765\u6a21\u62df\u7528\u6237\u5bf9\u5f7c\u6b64\u901a\u4fe1\u548c\u63a7\u5236\u7b56\u7565\u7684\u8bef\u89e3\u3002\u6b64\u5916\uff0c\u5bf9\u5b66\u4e60\u5230\u7684\u8d44\u6e90\u5206\u914d\u534f\u8bae\u7684\u5747\u8861\u5206\u6790\u8003\u5bdf\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u7b56\u7565\u5728\u8003\u8651\u8bef\u89e3\u7684\u60c5\u51b5\u4e0b\u6536\u655b\u5230\u5c40\u90e8\u65af\u5854\u514b\u5c14\u4f2f\u683c\u5747\u8861\u7684\u60c5\u51b5\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u672a\u8003\u8651\u8bef\u89e3\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65af\u5854\u514b\u5c14\u4f2f\u683c\u8d85\u535a\u5f08\u5728\u4fdd\u6301\u7528\u6237\u9ad8\u4f53\u9a8c\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528\u3002 Christo Kurisummoottil Thomas PDF N/A Hypergame Theory for Decentralized Resource Allocation in Multi-user Semantic Communications Semantic communications (SC) is an emerging communication paradigm in which wireless devices can send only relevant information from a source of data while relying on computing resources to regenerate missing data points. However, the design of a multi-user SC system becomes more challenging because of the computing and communication overhead required for coordination. Existing solutions for learning the semantic language and performing resource allocation often fail to capture the computing and communication tradeoffs involved in multiuser SC. To address this gap, a novel framework for decentralized computing and communication resource allocation in multiuser SC systems is proposed. The challenge of efficiently allocating communication and computing resources (for reasoning) in a decentralized manner to maximize the quality of task experience for the end users is addressed through the application of Stackelberg hyper game theory. Leveraging the concept of second-level hyper games, novel analytical formulations are developed to model misperceptions of the users about each other's communication and control strategies. Further, equilibrium analysis of the learned resource allocation protocols examines the convergence of the computing and communication strategies to a local Stackelberg equilibria, considering misperceptions. Simulation results show that the proposed Stackelberg hyper game results in efficient usage of communication and computing resources while maintaining a high quality of experience for the users compared to state-of-the-art that does not account for the misperceptions. HydraViT\uff1a\u4e3a\u53ef\u6269\u5c55\u7684ViT\u5806\u53e0\u5934\u90e8 Vision Transformers\uff08ViTs\uff09\u7684\u67b6\u6784\uff0c\u7279\u522b\u662f\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u673a\u5236\uff0c\u5bf9\u786c\u4ef6\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u9700\u6c42\u3002\u5728\u5177\u6709\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72ViTs\uff0c\u4f8b\u5982\u624b\u673a\uff0c\u9700\u8981\u591a\u79cd\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u9700\u8981\u5206\u522b\u8bad\u7ec3\u548c\u5b58\u50a8\u6bcf\u4e2a\u6240\u9700\u7684\u6a21\u578b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86HydraViT\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5806\u53e0\u6ce8\u610f\u529b\u5934\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684ViT\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53cd\u590d\u6539\u53d8\u6bcf\u4e2a\u5c42\u7684\u5d4c\u5165\u7ef4\u5ea6\u5927\u5c0f\u53ca\u5176\u5728MHA\u4e2d\u76f8\u5e94\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\uff0cHydraViT\u5f15\u5165\u4e86\u591a\u4e2a\u5b50\u7f51\u7edc\u3002\u56e0\u6b64\uff0cHydraViT\u5728\u5e7f\u6cdb\u7684\u786c\u4ef6\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHydraViT\u5728\u5b9e\u73b0\u5177\u6709\u591a\u8fbe10\u4e2a\u5b50\u7f51\u7edc\u7684\u53ef\u6269\u5c55ViT\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u8986\u76d6\u4e86\u5e7f\u6cdb\u7684\u8d44\u6e90\u7ea6\u675f\u3002\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cHydraViT\u5728ImageNet-1K\u4e0a\u4ee5\u76f8\u540c\u7684GMACs\u5b9e\u73b0\u4e86\u9ad8\u8fbe5\u4e2a\u767e\u5206\u70b9\u7684\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u4ee5\u76f8\u540c\u7684\u541e\u5410\u91cf\u5b9e\u73b0\u4e86\u9ad8\u8fbe7\u4e2a\u767e\u5206\u70b9\u7684\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u4f7f\u5176\u6210\u4e3a\u786c\u4ef6\u53ef\u7528\u6027\u591a\u6837\u6216\u968f\u65f6\u95f4\u53d8\u5316\u573a\u666f\u4e0b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002\u6e90\u4ee3\u7801\u53ef\u5728https://github.com/ds-kiel/HydraViT\u83b7\u53d6\u3002 Janek Haberer PDF N/A HydraViT: Stacking Heads for a Scalable ViT The architecture of Vision Transformers (ViTs), particularly the Multi-head Attention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs on devices with varying constraints, such as mobile phones, requires multiple models of different sizes. However, this approach has limitations, such as training and storing each required model separately. This paper introduces HydraViT, a novel approach that addresses these limitations by stacking attention heads to achieve a scalable ViT. By repeatedly changing the size of the embedded dimensions throughout each layer and their corresponding number of attention heads in MHA during training, HydraViT induces multiple subnetworks. Thereby, HydraViT achieves adaptability across a wide spectrum of hardware environments while maintaining performance. Our experimental results demonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10 subnetworks, covering a wide range of resource constraints. HydraViT achieves up to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time. Source code available at https://github.com/ds-kiel/HydraViT. BEATS\uff1a\u5229\u7528BackVerify\u548c\u57fa\u4e8e\u81ea\u9002\u5e94\u6d88\u6b67\u7684\u9ad8\u6548\u6811\u641c\u7d22\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u80fd\u529b \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6570\u5b66\u7684\u4e25\u8c28\u6027\u548c\u903b\u8f91\u6027\uff0c\u5b83\u4eec\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65f6\u4ecd\u7136\u9762\u4e34\u56f0\u96be\u3002\u5148\u524d\u7684\u7814\u7a76\u91c7\u7528\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u6765\u63d0\u5347LLMs\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u52aa\u529b\uff0c\u5176\u6027\u80fd\u4ecd\u4e0d\u5c3d\u5982\u4eba\u610f\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0cBEATS\uff0c\u4ee5\u589e\u5f3a\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u65b0\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u6307\u5bfc\u6a21\u578b\u9010\u6b65\u91cd\u5199\u3001\u63a8\u8fdb\u5e76\u57fa\u4e8e\u524d\u4e00\u6b65\u751f\u6210\u7b54\u6848\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u540e\u9a8c\u8bc1\u6280\u672f\uff0c\u4f7f\u7528LLMs\u6765\u9a8c\u8bc1\u751f\u6210\u7b54\u6848\u7684\u6b63\u786e\u6027\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u526a\u679d\u6811\u641c\u7d22\u6765\u4f18\u5316\u641c\u7d22\u65f6\u95f4\uff0c\u540c\u65f6\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06Qwen2-7b-Instruct\u7684\u5206\u6570\u4ece36.94\u63d0\u5347\u81f361.52\uff0c\u8d85\u8fc7\u4e86GPT4\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u768442.5\u5206\u3002 Linzhuang Sun PDF N/A BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search Large Language Models (LLMs) have exhibited exceptional performance across a broad range of tasks and domains. However, they still encounter difficulties in solving mathematical problems due to the rigorous and logical nature of mathematics. Previous studies have employed techniques such as supervised fine-tuning (SFT), prompt engineering, and search-based methods to improve the mathematical problem-solving abilities of LLMs. Despite these efforts, their performance remains suboptimal and demands substantial computational resources. To address this issue, we propose a novel approach, BEATS, to enhance mathematical problem-solving abilities. Our method leverages newly designed prompts that guide the model to iteratively rewrite, advance by one step, and generate answers based on previous steps. Additionally, we introduce a new back-verification technique that uses LLMs to validate the correctness of the generated answers. Furthermore, we employ a pruning tree search to optimize search time while achieving strong performance. Notably, our method improves Qwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the MATH benchmark. \u89c6\u89c9\u8bed\u8a00\u7ec4\u5408\u6027\u7684\u8270\u96be\u771f\u76f8 \u51e0\u9879\u57fa\u51c6\u6d4b\u8bd5\u5f97\u51fa\u7ed3\u8bba\uff0c\u6211\u4eec\u6700\u597d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982CLIP\uff09\u5728\u7ec4\u5408\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u7ed9\u5b9a\u4e00\u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4f1a\u8003\u5bdf\u6a21\u578b\u5728\u4e00\u7ec4\u7ec4\u5408\u6027\u5e72\u6270\u9879\u4e2d\u8bc6\u522b\u5176\u76f8\u5173\u63cf\u8ff0\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u8fd1\u671f\u6d8c\u73b0\u51fa\u7684\u4e00\u7cfb\u5217\u63d0\u6848\u901a\u8fc7\u4f7f\u7528\u5e72\u6270\u9879\u4f5c\u4e3a\u786c\u8d1f\u6837\u672c\u5bf9CLIP\u8fdb\u884c\u5fae\u8c03\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u6548\u679c\u3002\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\uff0c\u8fd9\u4e9b\u6539\u8fdb\u5b9e\u9645\u4e0a\u88ab\u663e\u8457\u5938\u5927\u4e86\u2014\u2014\u56e0\u4e3a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5e76\u672a\u63a2\u7a76\u5fae\u8c03\u540e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5bf9\u786c\u6b63\u6837\u672c\u4fdd\u6301\u4e0d\u53d8\u6027\u3002\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u4e00\u4e2a\u5305\u542b112,382\u4e2a\u786c\u8d1f\u6837\u672c\u548c\u786c\u6b63\u6837\u672c\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6211\u4eec\u53d1\u73b0\u7eb3\u5165\u786c\u6b63\u6837\u672c\u4f1a\u4f7fCLIP\u7684\u6027\u80fd\u4e0b\u964d12.9%\uff0c\u800c\u4eba\u7c7b\u5728\u8fd9\u9879\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5219\u8f7b\u677e\u8fbe\u523099%\u3002\u4ec5\u4f7f\u7528\u786c\u8d1f\u6837\u672c\u5fae\u8c03\u7684CLIP\u6027\u80fd\u4e0b\u964d\u5e45\u5ea6\u66f4\u5927\uff0c\u9ad8\u8fbe38.7%\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u968f\u540e\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,775,259\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u8bad\u7ec3\u96c6\uff0c\u5176\u4e2d\u65e2\u6709\u786c\u8d1f\u6837\u672c\u4e5f\u6709\u786c\u6b63\u6837\u672c\u7684\u63cf\u8ff0\u3002\u901a\u8fc7\u540c\u65f6\u8bad\u7ec3\u8fd9\u4e24\u7c7b\u6837\u672c\uff0c\u6211\u4eec\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u770b\u5230\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4e5f\u5728\u786c\u6b63\u6837\u672c\u4e0a\u7684\u8868\u73b0\u6709\u6240\u6539\u5584\uff0c\u8fd9\u8868\u660e\u7ec4\u5408\u6027\u65b9\u9762\u7684\u6539\u8fdb\u66f4\u4e3a\u7a33\u5065\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u4e25\u683c\u6d4b\u8bd5\u5e76\u63d0\u5347CLIP\u5bf9\u76f8\u5173\u201c\u6b63\u201d\u6982\u5ff5\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002 Amita Kamath PDF N/A The Hard Positive Truth about Vision-Language Compositionality Several benchmarks have concluded that our best vision-language models (e.g., CLIP) are lacking in compositionality. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. In response, a surge of recent proposals show improvements by finetuning CLIP with distractors as hard negatives. Our investigations reveal that these improvements have, in fact, been significantly overstated -- because existing benchmarks do not probe whether finetuned vision-language models remain invariant to hard positives. By curating an evaluation dataset with 112,382 hard negatives and hard positives, we uncover that including hard positives decreases CLIP's performance by 12.9%, while humans perform effortlessly at 99%. CLIP finetuned with hard negatives results in an even larger decrease, up to 38.7%. With this finding, we then produce a 1,775,259 image-text training set with both hard negative and hard positive captions. By training with both, we see improvements on existing benchmarks while simultaneously improving performance on hard positives, indicating a more robust improvement in compositionality. Our work suggests the need for future research to rigorously test and improve CLIP's understanding of semantic relationships between related \"positive\" concepts. \u9488\u5bf9LLMs\u7684\u5f31\u5230\u5f3a\u540e\u95e8\u653b\u51fb\uff1a\u57fa\u4e8e\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f \u5c3d\u7ba1\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u80fd\u529b\u800c\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u88ab\u8bc1\u660e\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\u3002\u8fd9\u4e9b\u653b\u51fb\u901a\u8fc7\u6bd2\u5316\u8bad\u7ec3\u6837\u672c\u548c\u5168\u53c2\u6570\u5fae\u8c03\u5f15\u5165\u76ee\u6807\u6f0f\u6d1e\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u540e\u95e8\u653b\u51fb\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5c24\u5176\u662f\u968f\u7740LLMs\u89c4\u6a21\u7684\u589e\u52a0\u3002\u6b64\u5916\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u53d7\u9650\u7684\u53c2\u6570\u66f4\u65b0\u53ef\u80fd\u4f1a\u963b\u788d\u89e6\u53d1\u5668\u4e0e\u76ee\u6807\u6807\u7b7e\u7684\u5bf9\u9f50\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u9a8c\u8bc1\u4e86\u4f7f\u7528PEFT\u7684\u540e\u95e8\u653b\u51fb\u53ef\u80fd\u5728\u5b9e\u73b0\u53ef\u884c\u6027\u80fd\u65b9\u9762\u9047\u5230\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u4f7f\u7528PEFT\u7684\u540e\u95e8\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\uff08W2SAttack\uff09\u7684\u7531\u5f31\u5230\u5f3a\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u7b97\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u5168\u53c2\u6570\u5fae\u8c03\u6bd2\u5316\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002\u7136\u540e\uff0c\u6559\u5e08\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\u5c06\u540e\u95e8\u79d8\u5bc6\u4f20\u9012\u7ed9\u5927\u89c4\u6a21\u5b66\u751f\u6a21\u578b\uff0c\u540e\u8005\u91c7\u7528PEFT\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cW2SAttack\u6709\u53ef\u80fd\u589e\u5f3a\u540e\u95e8\u653b\u51fb\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5728\u56db\u4e2a\u8bed\u8a00\u6a21\u578b\u3001\u56db\u4e2a\u540e\u95e8\u653b\u51fb\u7b97\u6cd5\u548c\u4e24\u79cd\u4e0d\u540c\u67b6\u6784\u7684\u6559\u5e08\u6a21\u578b\u4e0a\u5c55\u793a\u4e86W2SAttack\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9PEFT\u7684\u540e\u95e8\u653b\u51fb\u6210\u529f\u7387\u63a5\u8fd1100%\u3002 Shuai Zhao PDF N/A Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation Despite being widely applied due to their exceptional capabilities, Large Language Models (LLMs) have been proven to be vulnerable to backdoor attacks. These attacks introduce targeted vulnerabilities into LLMs by poisoning training samples and full-parameter fine-tuning. However, this kind of backdoor attack is limited since they require significant computational resources, especially as the size of LLMs increases. Besides, parameter-efficient fine-tuning (PEFT) offers an alternative but the restricted parameter updating may impede the alignment of triggers with target labels. In this study, we first verify that backdoor attacks with PEFT may encounter challenges in achieving feasible performance. To address these issues and improve the effectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack algorithm from weak to strong based on contrastive knowledge distillation (W2SAttack). Specifically, we poison small-scale language models through full-parameter fine-tuning to serve as the teacher model. The teacher model then covertly transfers the backdoor to the large-scale student model through contrastive knowledge distillation, which employs PEFT. Theoretical analysis reveals that W2SAttack has the potential to augment the effectiveness of backdoor attacks. We demonstrate the superior performance of W2SAttack on classification tasks across four language models, four backdoor attack algorithms, and two different architectures of teacher models. Experimental results indicate success rates close to 100% for backdoor attacks targeting PEFT. \u5173\u4e8e\u6280\u672f\u672f\u8bed\u7ffb\u8bd1\uff1a\u673a\u5668\u7ffb\u8bd1\u7f29\u7565\u8bed\u7684\u7ffb\u8bd1\u6d41\u7a0b \u4e13\u4e1a\u7ffb\u8bd1\u4eba\u5458\u5c06\u6587\u6863\u4ece\u6e90\u8bed\u8a00\uff08SL\uff09\u7ffb\u8bd1\u4e3a\u76ee\u6807\u8bed\u8a00\uff08TL\uff09\u7684\u5178\u578b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u4e0d\u603b\u662f\u4e13\u6ce8\u4e8e\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u6240\u505a\u7684\u4e8b\u60c5\u2014\u2014\u9884\u6d4b\u4e00\u7cfb\u5217\u5355\u8bcd\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\u5c3d\u7ba1\u50cf\u82f1\u8bed\u548c\u6cd5\u8bed\u8fd9\u6837\u7684\u9ad8\u8d44\u6e90\u8bed\u8a00\u5728\u4f7f\u7528BLEU\u548cCOMET\u7b49\u5e38\u89c1\u5ea6\u91cf\u6807\u51c6\u8fdb\u884c\u6d4b\u91cf\u65f6\uff0c\u636e\u62a5\u9053\u5df2\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u91cd\u8981\u7684\u6b65\u9aa4\u88ab\u5ffd\u7565\u4e86\uff1a\u6280\u672f\u672f\u8bed\u7684\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u7f29\u7565\u8bcd\u3002\u4e00\u4e9b\u516c\u5f00\u53ef\u7528\u7684\u6700\u5148\u8fdb\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5982\u8c37\u6b4c\u7ffb\u8bd1\uff0c\u5728\u5904\u7406\u7f29\u7565\u8bcd\u65f6\u53ef\u80fd\u4f1a\u51fa\u73b0\u9519\u8bef\u2014\u2014\u6839\u636e\u6211\u4eec\u7684\u7814\u7a76\uff0c\u9519\u8bef\u7387\u9ad8\u8fbe50%\u3002\u672c\u6587\u901a\u8fc7\u5728SL-TL\uff08FR-EN\uff09\u7ffb\u8bd1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u589e\u52a0\u4e00\u4e2a\u6b65\u9aa4\u6765\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u7684\u7f29\u7565\u8bcd\u6b67\u4e49\u95ee\u9898\uff0c\u9996\u5148\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7f29\u7565\u8bcd\u8bed\u6599\u5e93\u4f9b\u516c\u4f17\u4f7f\u7528\uff0c\u7136\u540e\u5b9e\u9a8c\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u9608\u503c\u7b97\u6cd5\uff0c\u4e0e\u8c37\u6b4c\u7ffb\u8bd1\u548cOpusMT\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u8fd110%\u7684\u63d0\u5347\u3002 Richard Yue PDF N/A On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms The typical workflow for a professional translator to translate a document from its source language (SL) to a target language (TL) is not always focused on what many language models in natural language processing (NLP) do - predict the next word in a series of words. While high-resource languages like English and French are reported to achieve near human parity using common metrics for measurement such as BLEU and COMET, we find that an important step is being missed: the translation of technical terms, specifically acronyms. Some state-of-the art machine translation systems like Google Translate which are publicly available can be erroneous when dealing with acronyms - as much as 50% in our findings. This article addresses acronym disambiguation for MT systems by proposing an additional step to the SL-TL (FR-EN) translation workflow where we first offer a new acronym corpus for public consumption and then experiment with a search-based thresholding algorithm that achieves nearly 10% increase when compared to Google Translate and OpusMT. \u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ece\u7ffb\u8bd1\u8bb0\u5fc6\u4e2d\u9884\u6d4b\u673a\u5668\u7ffb\u8bd1\u7684\u951a\u5b9a\u6587\u672c \u7ffb\u8bd1\u8bb0\u5fc6\u5e93\uff08TMs\uff09\u662f\u4e13\u4e1a\u7ffb\u8bd1\u5de5\u5177\u2014\u2014\u8ba1\u7b97\u673a\u8f85\u52a9\u7ffb\u8bd1\uff08CAT\uff09\u5de5\u5177\u7684\u6838\u5fc3\u3002\u4e3a\u4e86\u4f7f\u7528CAT\u5de5\u5177\u8fdb\u884c\u7ffb\u8bd1\uff0c\u8bd1\u8005\u5229\u7528TM\u6765\u6536\u96c6\u4e0e\u6240\u9700\u7ffb\u8bd1\u7247\u6bb5\uff08s'\uff09\u76f8\u4f3c\u7684\u7ffb\u8bd1\u3002\u8bb8\u591aCAT\u5de5\u5177\u63d0\u4f9b\u6a21\u7cca\u5339\u914d\u7b97\u6cd5\uff0c\u4ee5\u5728TM\u4e2d\u5b9a\u4f4d\u4e0es'\u8ddd\u79bb\u76f8\u8fd1\u7684\u7247\u6bb5\uff08s\uff09\u3002\u5728\u627e\u5230\u4e24\u4e2a\u76f8\u4f3c\u7247\u6bb5\u540e\uff0cCAT\u5de5\u5177\u4f1a\u5c55\u793a\u5305\u542b\u6e90\u8bed\u8a00\u7247\u6bb5\u53ca\u5176\u76ee\u6807\u8bed\u8a00\u7ffb\u8bd1\u7684\u5e73\u884c\u7247\u6bb5\uff08s, t\uff09\u3002\u6b64\u5916\uff0cCAT\u5de5\u5177\u8fd8\u5305\u542b\u6a21\u7cca\u5339\u914d\u4fee\u590d\uff08FMR\uff09\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u4f1a\u81ea\u52a8\u4f7f\u7528TM\u4e2d\u7684\u5e73\u884c\u7247\u6bb5\u6765\u521b\u5efa\u65b0\u7684TM\u6761\u76ee\uff0c\u8fd9\u4e9b\u6761\u76ee\u5305\u542b\u5bf9\u539f\u59cb\u5185\u5bb9\u7684\u4fee\u6539\u7248\u672c\uff0c\u65e8\u5728\u4f5c\u4e3as'\u7684\u7ffb\u8bd1\u3002\u5927\u591a\u6570FMR\u6280\u672f\u4f7f\u7528\u673a\u5668\u7ffb\u8bd1\u4f5c\u4e3a\u201c\u4fee\u590d\u201d\u90a3\u4e9b\u9700\u8981\u4fee\u6539\u7684\u8bcd\u6c47\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5bf9\u4e8e\u90a3\u4e9b\u951a\u5b9a\u7684\u8bcd\u6c47\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5176\u4ed6\u6280\u672f\uff0c\u5982Word2Vec\u3001BERT\uff0c\u751a\u81f3\u662fChatGPT\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5bf9\u4e8e\u9075\u5faa\u8fde\u7eed\u8bcd\u888b\uff08CBOW\uff09\u8303\u5f0f\u7684\u951a\u5b9a\u8bcd\u6c47\uff0cWord2Vec\u3001BERT\u548cGPT-4\u53ef\u4ee5\u5728\u4ece\u6cd5\u8bed\u5230\u82f1\u8bed\u7684\u951a\u5b9a\u8bcd\u6c47\u7ffb\u8bd1\u4e2d\u5b9e\u73b0\u4e0e\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u76f8\u4f3c\u751a\u81f3\u66f4\u597d\u7684\u7ed3\u679c\u3002 Richard Yue PDF N/A Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods Translation memories (TMs) are the backbone for professional translation tools called computer-aided translation (CAT) tools. In order to perform a translation using a CAT tool, a translator uses the TM to gather translations similar to the desired segment to translate (s'). Many CAT tools offer a fuzzy-match algorithm to locate segments (s) in the TM that are close in distance to s'. After locating two similar segments, the CAT tool will present parallel segments (s, t) that contain one segment in the source language along with its translation in the target language. Additionally, CAT tools contain fuzzy-match repair (FMR) techniques that will automatically use the parallel segments from the TM to create new TM entries containing a modified version of the original with the idea in mind that it will be the translation of s'. Most FMR techniques use machine translation as a way of \"repairing\" those words that have to be modified. In this article, we show that for a large part of those words which are anchored, we can use other techniques that are based on machine learning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we show that for anchored words that follow the continuous bag-of-words (CBOW) paradigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for some cases, better results than neural machine translation for translating anchored words from French to English. \u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u81ea\u9002\u5e94\u6d41\u5904\u7406 \u5f53\u524d\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u60c5\u666f\u4e2d\uff0c\u6570\u636e\u91cf\u6b63\u5728\u6301\u7eed\u589e\u957f\uff0c\u8fd9\u4e9b\u6570\u636e\u4ee5\u6301\u7eed\u6d41\u7684\u5f62\u5f0f\u751f\u6210\uff0c\u9700\u8981\u65b0\u9896\u7684\u67b6\u6784\u548c\u903b\u8f91\u89e3\u51b3\u65b9\u6848\u6765\u8fdb\u884c\u5904\u7406\u3002\u5c06\u6570\u636e\u5904\u7406\u63a8\u5411\u8ba1\u7b97\u9891\u8c31\u7684\u8fb9\u7f18\uff0c\u53ef\u4ee5\u786e\u4fdd\u66f4\u597d\u7684\u8d1f\u8f7d\u5206\u5e03\uff0c\u539f\u5219\u4e0a\u8fd8\u80fd\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u3002\u7136\u800c\uff0c\u7ba1\u7406\u8fd9\u6837\u7684\u7ed3\u6784\u662f\u590d\u6742\u7684\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u786e\u4fdd\u5e94\u7528\u7a0b\u5e8f\u6240\u6709\u8005\u548c\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u8005\u6307\u5b9a\u7684\u8981\u6c42\uff08\u4e5f\u79f0\u4e3a\u670d\u52a1\u7ea7\u522b\u76ee\u6807\uff0cSLOs\uff09\u65f6\u3002\u5c3d\u7ba1\u6709\u5927\u91cf\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u63d0\u6848\uff0c\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u4ecd\u5728\u52aa\u529b\u4fdd\u8bc1\u957f\u671f\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u4ee5\u53ca\u7cbe\u786e\u7684\u6545\u969c\u6392\u67e5\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\uff08AIF\uff09\u7684\u65b0\u578bML\u8303\u5f0f\u2014\u2014\u8fd9\u662f\u4e00\u4e2a\u6765\u81ea\u795e\u7ecf\u79d1\u5b66\u7684\u89c2\u5ff5\uff0c\u63cf\u8ff0\u4e86\u5927\u8111\u5982\u4f55\u6301\u7eed\u9884\u6d4b\u548c\u8bc4\u4f30\u611f\u5b98\u4fe1\u606f\u4ee5\u51cf\u5c11\u957f\u671f\u610f\u5916\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u5f02\u6784\u7684\u771f\u5b9e\u6d41\u5904\u7406\u7528\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u8fd9\u4e00\u8303\u5f0f\u5e76\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u4e2d\u57fa\u4e8eAIF\u7684\u4ee3\u7406\u6301\u7eed\u4f18\u5316\u4e86\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u670d\u52a1\u7684\u4e09\u4e2aSLO\u7684\u6ee1\u8db3\u60c5\u51b5\u3002\u8be5\u4ee3\u7406\u4f7f\u7528\u56e0\u679c\u77e5\u8bc6\u9010\u6b65\u7406\u89e3\u5176\u884c\u52a8\u4e0e\u8981\u6c42\u6ee1\u8db3\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5e94\u4f18\u5148\u8003\u8651\u54ea\u4e9b\u914d\u7f6e\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u4ee3\u7406\u6700\u591a\u9700\u8981\u4e09\u5341\u6b21\u8fed\u4ee3\u5373\u53ef\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u5c55\u793a\u4e86\u5728\u77ed\u65f6\u95f4\u5185\u63d0\u4f9b\u7cbe\u786e\u7ed3\u679c\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f97\u76ca\u4e8eAIF\u53ca\u5176\u56e0\u679c\u7ed3\u6784\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4fdd\u8bc1\u4e86\u51b3\u7b56\u8fc7\u7a0b\u7684\u5b8c\u5168\u900f\u660e\u6027\uff0c\u4f7f\u5f97\u7ed3\u679c\u7684\u89e3\u91ca\u548c\u6545\u969c\u6392\u67e5\u53d8\u5f97\u8f7b\u800c\u6613\u4e3e\u3002 Boris Sedlak PDF N/A Adaptive Stream Processing on Edge Devices through Active Inference The current scenario of IoT is witnessing a constant increase on the volume of data, which is generated in constant stream, calling for novel architectural and logical solutions for processing it. Moving the data handling towards the edge of the computing spectrum guarantees better distribution of load and, in principle, lower latency and better privacy. However, managing such a structure is complex, especially when requirements, also referred to Service Level Objectives (SLOs), specified by applications' owners and infrastructure managers need to be ensured. Despite the rich number of proposals of Machine Learning (ML) based management solutions, researchers and practitioners yet struggle to guarantee long-term prediction and control, and accurate troubleshooting. Therefore, we present a novel ML paradigm based on Active Inference (AIF) -- a concept from neuroscience that describes how the brain constantly predicts and evaluates sensory information to decrease long-term surprise. We implement it and evaluate it in a heterogeneous real stream processing use case, where an AIF-based agent continuously optimizes the fulfillment of three SLOs for three autonomous driving services running on multiple devices. The agent used causal knowledge to gradually develop an understanding of how its actions are related to requirements fulfillment, and which configurations to favor. Through this approach, our agent requires up to thirty iterations to converge to the optimal solution, showing the capability of offering accurate results in a short amount of time. Furthermore, thanks to AIF and its causal structures, our method guarantees full transparency on the decision making, making the interpretation of the results and the troubleshooting effortless. \u6837\u672c\u538b\u7f29\u91ca\u653e\uff1a\u5b9e\u503c\u635f\u5931\u7684\u65b0\u6cdb\u5316\u754c \u6837\u672c\u538b\u7f29\u7406\u8bba\u4e3a\u90a3\u4e9b\u53ef\u4ee5\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b50\u96c6\u548c\u4e00\u4e2a\uff08\u77ed\uff09\u6d88\u606f\u5b57\u7b26\u4e32\uff08\u901a\u5e38\u5b9a\u4e49\u4e3a\u4e8c\u8fdb\u5236\u5e8f\u5217\uff09\u5b8c\u5168\u5b9a\u4e49\u7684\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u6cdb\u5316\u4fdd\u8bc1\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3a\u96f6\u4e00\u635f\u5931\u63d0\u4f9b\u4e86\u6cdb\u5316\u8fb9\u754c\uff0c\u8fd9\u5728\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65f6\u5c24\u5176\u5177\u6709\u9650\u5236\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u63a8\u5bfc\u65b0\u7684\u6837\u672c\u538b\u7f29\u8fb9\u754c\u7684\u4e00\u822c\u6846\u67b6\uff0c\u8fd9\u4e9b\u8fb9\u754c\u9002\u7528\u4e8e\u5b9e\u503c\u635f\u5931\u3002\u6211\u4eec\u901a\u8fc7\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\uff08\u4f8b\u5982\u795e\u7ecf\u7f51\u7edc\u548c\u51b3\u7b56\u68ee\u6797\uff09\u4e0a\u8bc4\u4f30\u8fd9\u4e9b\u8fb9\u754c\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u7d27\u5bc6\u5ea6\u548c\u591a\u529f\u80fd\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u662f\u4f7f\u7528Pick-To-Learn\uff08P2L\uff09\u5143\u7b97\u6cd5\u8bad\u7ec3\u7684\uff0c\u8be5\u7b97\u6cd5\u5c06\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5668\u7684\u8bad\u7ec3\u65b9\u6cd5\u8f6c\u5316\u4e3a\u4ea7\u751f\u6837\u672c\u538b\u7f29\u9884\u6d4b\u5668\u3002\u4e0e\u73b0\u6709\u7684P2L\u8fb9\u754c\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u8fb9\u754c\u5728\u975e\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\u4ecd\u7136\u6709\u6548\u3002 Mathieu Bazinet PDF N/A Sample compression unleashed : New generalization bounds for real valued losses The sample compression theory provides generalization guarantees for predictors that can be fully defined using a subset of the training dataset and a (short) message string, generally defined as a binary sequence. Previous works provided generalization bounds for the zero-one loss, which is restrictive, notably when applied to deep learning approaches. In this paper, we present a general framework for deriving new sample compression bounds that hold for real-valued losses. We empirically demonstrate the tightness of the bounds and their versatility by evaluating them on different types of models, e.g., neural networks and decision forests, trained with the Pick-To-Learn (P2L) meta-algorithm, which transforms the training method of any machine-learning predictor to yield sample-compressed predictors. In contrast to existing P2L bounds, ours are valid in the non-consistent case. \u667a\u80fd\u80fd\u6e90\u7ba1\u7406\uff1a\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u4e0e\u5145\u7535\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u8054\u7f51\u6280\u672f \u7535\u6c60\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u662f\u4e86\u89e3\u7535\u6c60\u5269\u4f59\u5bff\u547d\u548c\u5145\u7535\u9700\u6c42\u7684\u91cd\u8981\u53c2\u6570\u3002\u672c\u7814\u7a76\u9879\u76ee\u7684\u76ee\u6807\u662f\u5f00\u53d1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7535\u6c60RUL\u6570\u636e\u96c6\u6a21\u578b\u3002\u5f00\u53d1\u4e86\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5206\u7c7b\u8f66\u8f86\u7684RUL\uff0c\u5e76\u6a21\u62df\u4e86\u7269\u8054\u7f51\uff08IoT\uff09\u6982\u5ff5\u4ee5\u5b9e\u73b0\u5145\u7535\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u548c\u7ba1\u7406\u4efb\u4f55\u6545\u969c\u3002\u7ed8\u5236\u7684\u56fe\u8868\u5c55\u793a\u4e86\u4f7f\u7528Blynk IoT\u5e73\u53f0\u7684\u8f66\u8f7d\u53c2\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u53d1\u7684catboost\u3001\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u3001\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u548c\u6df7\u5408\u6a21\u578b\u80fd\u591f\u4ee5\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\u5c06RUL\u5206\u7c7b\u4e3a\u4e09\u4e2a\u7c7b\u522b\u3002\u901a\u8fc7tkinter GUI\u8f93\u5165\u6570\u636e\uff0c\u7528\u4e8e\u6a21\u62df\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u5145\u7535\uff0c\u5e76\u4e14\u901a\u8fc7pyserial\u540e\u7aef\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u8f93\u5165\u5230Esp-32\u5fae\u63a7\u5236\u5668\u4e2d\uff0c\u4ee5\u6839\u636e\u6a21\u578b\u7684\u9884\u6d4b\u5b9e\u73b0\u5145\u653e\u7535\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7269\u8054\u7f51\u7cfb\u7edf\uff0c\u53ef\u4ee5\u65ad\u5f00\u5145\u7535\u3001\u76d1\u63a7\u548c\u5206\u6790\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728MLP\u3001catboost\u6a21\u578b\u4e0a\u53ef\u4ee5\u83b7\u5f9799%\u7684\u51c6\u786e\u7387\uff0c\u5728GRU\u6a21\u578b\u4e0a\u4e5f\u53ef\u4ee5\u83b7\u5f97\u7c7b\u4f3c\u7684\u51c6\u786e\u7387\uff0c\u6700\u540e\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u89e6\u53d1\u7ee7\u7535\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5145\u7535\u548c\u8282\u80fd\u673a\u5236\u3002\u901a\u8fc7\u5c55\u793a\u57fa\u4e8eBlynk\u5e73\u53f0\u7684\u76d1\u63a7\u548c\u81ea\u52a8\u5316\u73b0\u8c61\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u76d1\u63a7\u53c2\u6570\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u521b\u65b0\u65b9\u6cd5\u3002 Biplov Paneru PDF N/A Intelligent Energy Management: Remaining Useful Life Prediction and Charging Automation System Comprised of Deep Learning and the Internet of Things Remaining Useful Life (RUL) of battery is an important parameter to know the battery's remaining life and need for recharge. The goal of this research project is to develop machine learning-based models for the battery RUL dataset. Different ML models are developed to classify the RUL of the vehicle, and the IoT (Internet of Things) concept is simulated for automating the charging system and managing any faults aligning. The graphs plotted depict the relationship between various vehicle parameters using the Blynk IoT platform. Results show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent Unit (GRU), and hybrid model developed could classify RUL into three classes with 99% more accuracy. The data is fed using the tkinter GUI for simulating artificial intelligence (AI)-based charging, and with a pyserial backend, data can be entered into the Esp-32 microcontroller for making charge discharge possible with the model's predictions. Also, with an IoT system, the charging can be disconnected, monitored, and analyzed for automation. The results show that an accuracy of 99% can be obtained on models MLP, catboost model and similar accuracy on GRU model can be obtained, and finally relay-based triggering can be made by prediction through the model used for automating the charging and energy-saving mechanism. By showcasing an exemplary Blynk platform-based monitoring and automation phenomenon, we further present innovative ways of monitoring parameters and automating the system. \u300aLou\u6570\u636e\u96c6\u300b\u2014\u2014\u63a2\u7d22\u6027\u522b\u516c\u5e73\u8bed\u8a00\u5bf9\u5fb7\u8bed\u6587\u672c\u5206\u7c7b\u7684\u5f71\u54cd \u6027\u522b\u516c\u5e73\u8bed\u8a00\uff0c\u4e00\u79cd\u4e0d\u65ad\u53d1\u5c55\u7684\u5fb7\u8bed\u8bed\u8a00\u53d8\u4f53\uff0c\u901a\u8fc7\u6db5\u76d6\u6240\u6709\u6027\u522b\u6216\u4f7f\u7528\u4e2d\u6027\u5f62\u5f0f\u6765\u4fc3\u8fdb\u5305\u5bb9\u6027\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u8d44\u6e90\u6765\u8bc4\u4f30\u8fd9\u79cd\u8bed\u8a00\u8f6c\u53d8\u5bf9\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u8fdb\u884c\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5e76\u672a\u5728\u8fd9\u79cd\u53d8\u4f53\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63a8\u51fa\u4e86Lou\uff0c\u8fd9\u662f\u9996\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u91cd\u6784\u6587\u672c\u7684\u5fb7\u8bed\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4e03\u4e2a\u4efb\u52a1\uff0c\u5982\u7acb\u573a\u68c0\u6d4b\u548c\u6bd2\u6027\u5206\u7c7b\u3002\u5728Lou\u4e0a\u8bc4\u4f3016\u4e2a\u5355\u8bed\u548c\u591a\u8bed\u8a00LMs\u7684\u7ed3\u679c\u663e\u793a\uff0c\u6027\u522b\u516c\u5e73\u8bed\u8a00\u663e\u8457\u5f71\u54cd\u9884\u6d4b\u7ed3\u679c\uff0c\u5305\u62ec\u6807\u7b7e\u7ffb\u8f6c\u3001\u786e\u5b9a\u6027\u964d\u4f4e\u4ee5\u53ca\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u53d8\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u4ecd\u7136\u6709\u6548\uff0c\u56e0\u4e3a\u539f\u59cb\u5b9e\u4f8b\u548c\u91cd\u6784\u5b9e\u4f8b\u7684LM\u6392\u540d\u5e76\u65e0\u663e\u8457\u5dee\u5f02\u3002\u5c3d\u7ba1\u6211\u4eec\u63d0\u4f9b\u4e86\u5173\u4e8e\u6027\u522b\u516c\u5e73\u8bed\u8a00\u5bf9\u5fb7\u8bed\u6587\u672c\u5206\u7c7b\u5f71\u54cd\u7684\u521d\u6b65\u89c1\u89e3\uff0c\u4f46\u8fd9\u4e9b\u53d1\u73b0\u5f88\u53ef\u80fd\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\uff0c\u56e0\u4e3a\u5728\u591a\u8bed\u8a00\u548c\u82f1\u8bedLMs\u4e2d\u89c2\u5bdf\u5230\u4e86\u4e00\u81f4\u7684\u6a21\u5f0f\u3002 Andreas Waldis PDF N/A The Lou Dataset -- Exploring the Impact of Gender-Fair Language in German Text Classification Gender-fair language, an evolving German linguistic variation, fosters inclusion by addressing all genders or using neutral forms. Nevertheless, there is a significant lack of resources to assess the impact of this linguistic shift on classification using language models (LMs), which are probably not trained on such variations. To address this gap, we present Lou, the first dataset featuring high-quality reformulations for German text classification covering seven tasks, like stance detection and toxicity classification. Evaluating 16 mono- and multi-lingual LMs on Lou shows that gender-fair language substantially impacts predictions by flipping labels, reducing certainty, and altering attention patterns. However, existing evaluations remain valid, as LM rankings of original and reformulated instances do not significantly differ. While we offer initial insights on the effect on German text classification, the findings likely apply to other languages, as consistent patterns were observed in multi-lingual and English LMs. \u5f00\u521b\u6027\u6587\u672c\u5230\u56fe\u50cf\u77e5\u8bc6\u7f16\u8f91\u7684\u53ef\u9760\u8bc4\u4f30\uff1a\u5229\u7528\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0e\u521b\u65b0\u6807\u51c6 \u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u5c06\u5176\u53c2\u6570\u7f16\u7801\u4e3a\u4e8b\u5b9e\u77e5\u8bc6\u3002\u8fd9\u4e9b\u53c2\u6570\u5316\u7684\u4e8b\u5b9e\u4f7f\u5f97\u751f\u6210\u903c\u771f\u7684\u56fe\u50cf\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u80fd\u4f1a\u53d8\u5f97\u8fc7\u65f6\uff0c\u4ece\u800c\u9519\u8bef\u5730\u53cd\u6620\u5f53\u524d\u7684\u4e16\u754c\u72b6\u6001\u3002\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u65e8\u5728\u6709\u9488\u5bf9\u6027\u5730\u66f4\u65b0\u6a21\u578b\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u9762\u5bf9\u7f16\u8f91\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u8bc4\u4f30\u6807\u51c6\u4e0d\u53ef\u9760\u7684\u53cc\u91cd\u6311\u6218\uff0cT2I\u77e5\u8bc6\u7f16\u8f91\u7684\u53d1\u5c55\u5728\u6709\u6548\u63a8\u5e7f\u6ce8\u5165\u77e5\u8bc6\u65b9\u9762\u9047\u5230\u4e86\u56f0\u96be\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2aT2I\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u5168\u9762\u6db5\u76d6\u4e86\u4e09\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u6211\u4eec\u7cbe\u5fc3\u5236\u4f5c\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\\textbf{CAKE}\uff0c\u5305\u62ec\u91ca\u4e49\u548c\u591a\u5bf9\u8c61\u6d4b\u8bd5\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u77e5\u8bc6\u63a8\u5e7f\u7684\u66f4\u7ec6\u81f4\u8bc4\u4f30\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u51c6\uff0c\\textbf{\u81ea\u9002\u5e94CLIP\u9608\u503c}\uff0c\u4ee5\u6709\u6548\u8fc7\u6ee4\u5f53\u524d\u6807\u51c6\u4e0b\u7684\u865a\u5047\u6210\u529f\u56fe\u50cf\uff0c\u5e76\u5b9e\u73b0\u53ef\u9760\u7684\u7f16\u8f91\u8bc4\u4f30\u3002\u6700\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\\textbf{MPE}\uff0c\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684T2I\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002MPE\u4e0d\u662f\u8c03\u6574\u53c2\u6570\uff0c\u800c\u662f\u7cbe\u786e\u8bc6\u522b\u5e76\u7f16\u8f91\u6761\u4ef6\u6587\u672c\u63d0\u793a\u4e2d\u7684\u8fc7\u65f6\u90e8\u5206\uff0c\u4ee5\u9002\u5e94\u6700\u65b0\u7684\u77e5\u8bc6\u3002MPE\u7684\u76f4\u63a5\u5b9e\u73b0\uff08\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\u7f16\u8f91\u5668\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u52aa\u529b\u80fd\u591f\u8fdb\u4e00\u6b65\u63a8\u52a8T2I\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u7684\u5fe0\u5b9e\u8bc4\u4f30\u3002 Hengrui Gu PDF N/A Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion During pre-training, the Text-to-Image (T2I) diffusion models encode factual knowledge into their parameters. These parameterized facts enable realistic image generation, but they may become obsolete over time, thereby misrepresenting the current state of the world. Knowledge editing techniques aim to update model knowledge in a targeted way. However, facing the dual challenges posed by inadequate editing datasets and unreliable evaluation criterion, the development of T2I knowledge editing encounter difficulties in effectively generalizing injected knowledge. In this work, we design a T2I knowledge editing framework by comprehensively spanning on three phases: First, we curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test, to enable more fine-grained assessment on knowledge generalization. Second, we propose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively filter out false successful images under the current criterion and achieve reliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but effective approach for T2I knowledge editing. Instead of tuning parameters, MPE precisely recognizes and edits the outdated part of the conditioning text-prompt to accommodate the up-to-date knowledge. A straightforward implementation of MPE (Based on in-context learning) exhibits better overall performance than previous model editors. We hope these efforts can further promote faithful evaluation of T2I knowledge editing methods. Atlas-Chat: \u9002\u5e94\u4f4e\u8d44\u6e90\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u65b9\u8a00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b \u6211\u4eec\u63a8\u51fa\u4e86Atlas-Chat\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u963f\u62c9\u4f2f\u65b9\u8a00\u5f00\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u5408\u3002\u805a\u7126\u4e8e\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\uff08\u53c8\u79f0Darija\uff09\uff0c\u6211\u4eec\u901a\u8fc7\u6574\u5408\u73b0\u6709\u7684Darija\u8bed\u8a00\u8d44\u6e90\u3001\u624b\u52a8\u548c\u5408\u6210\u5730\u521b\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u4e0b\u7684\u82f1\u8bed\u6307\u4ee4\u7ffb\u8bd1\uff0c\u6784\u5efa\u4e86\u6211\u4eec\u7684\u6307\u4ee4\u6570\u636e\u96c6\u3002\u7ecf\u8fc7\u6570\u636e\u96c6\u5fae\u8c03\u7684Atlas-Chat-9B\u548c2B\u6a21\u578b\uff0c\u5728\u9075\u5faaDarija\u6307\u4ee4\u548c\u6267\u884c\u6807\u51c6NLP\u4efb\u52a1\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\u3002\u7279\u522b\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728DarijaMMLU\u4e0a\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u5305\u62ecLLaMa\u3001Jais\u548cAceGPT\u5728\u5185\u7684\u6700\u5148\u8fdb\u548c\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684LLMs\uff0c\u4f8b\u5982\uff0c\u5728\u6211\u4eec\u65b0\u5f15\u5165\u7684\u6db5\u76d6\u5224\u522b\u548c\u751f\u6210\u4efb\u52a1\u7684Darija\u8bc4\u4f30\u5957\u4ef6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u66f4\u5927\u89c4\u6a21\u768413B\u6a21\u578b13%\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u5404\u79cd\u5fae\u8c03\u7b56\u7565\u548c\u57fa\u7840\u6a21\u578b\u9009\u62e9\u8fdb\u884c\u4e86\u5b9e\u9a8c\u5206\u6790\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u914d\u7f6e\u3002\u6240\u6709\u8d44\u6e90\u5747\u4e3a\u516c\u5f00\u53ef\u7528\uff0c\u6211\u4eec\u76f8\u4fe1\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u53d8\u4f53\u7684\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u8bed\u8a00\u53d8\u4f53\u5728\u5f53\u4ee3LLMs\u4e2d\u5f80\u5f80\u56e0\u6570\u636e\u4e30\u5bcc\u7684\u8bed\u8a00\u800c\u88ab\u5ffd\u89c6\u3002 Guokan Shang PDF N/A Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect We introduce Atlas-Chat, the first-ever collection of large language models specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource language variants, which are often neglected in favor of data-rich languages by contemporary LLMs. \u901a\u8fc7\u4f2a\u4ee3\u7801\u63d0\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u56fe\u63a8\u7406 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u6700\u8fd1\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u8fd9\u79cd\u6210\u529f\u4e5f\u6fc0\u53d1\u4e86\u5728\u56fe\u76f8\u5173\u4efb\u52a1\u4e2d\u4f7f\u7528LLMs\u7684\u5174\u8da3\u3002\u5176\u4e2d\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u662f\u5426\u80fd\u591f\u89e3\u51b3\u56fe\u95ee\u9898\uff0c\u4f8b\u5982\u8ba1\u7b97\u56fe\u7684\u8fde\u901a\u5206\u91cf\u6570\u91cf\u6216\u8ba1\u7b97\u4e24\u4e2a\u8282\u70b9\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u3002\u5c3d\u7ba1LLMs\u5177\u5907\u521d\u6b65\u7684\u56fe\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u89e3\u51b3\u4e00\u4e9b\u770b\u4f3c\u7b80\u5355\u7684\u95ee\u9898\u65f6\u4ecd\u53ef\u80fd\u9047\u5230\u56f0\u96be\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u901a\u8fc7\u4f2a\u4ee3\u7801\u6307\u4ee4\u8fdb\u884c\u63d0\u793a\u662f\u5426\u80fd\u63d0\u9ad8LLMs\u5728\u89e3\u51b3\u56fe\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4f2a\u4ee3\u7801\u6307\u4ee4\u901a\u5e38\u80fd\u63d0\u9ad8\u6240\u6709\u8003\u8651\u7684LLMs\u7684\u8868\u73b0\u3002\u56fe\u3001\u4f2a\u4ee3\u7801\u63d0\u793a\u548c\u8bc4\u4f30\u4ee3\u7801\u5747\u5df2\u516c\u5f00\u3002 Konstantinos Skianis PDF N/A Graph Reasoning with Large Language Models via Pseudo-code Prompting Large language models (LLMs) have recently achieved remarkable success in various reasoning tasks in the field of natural language processing. This success of LLMs has also motivated their use in graph-related tasks. Among others, recent work has explored whether LLMs can solve graph problems such as counting the number of connected components of a graph or computing the shortest path distance between two nodes. Although LLMs possess preliminary graph reasoning abilities, they might still struggle to solve some seemingly simple problems. In this paper, we investigate whether prompting via pseudo-code instructions can improve the performance of LLMs in solving graph problems. Our experiments demonstrate that using pseudo-code instructions generally improves the performance of all considered LLMs. The graphs, pseudo-code prompts, and evaluation code are publicly available. \u8bbe\u8ba1\u77ed\u9636\u6bb5CDC-XPUF\uff1a\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5e73\u8861\u53ef\u9760\u6027\u3001\u6210\u672c\u548c\u5b89\u5168\u6027 \u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u7684\u5feb\u901f\u6269\u5c55\u8981\u6c42\u5f3a\u5927\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\uff08PUFs\uff09\u901a\u8fc7\u5229\u7528\u786c\u4ef6\u56fa\u6709\u7684\u53d8\u5316\u751f\u6210\u72ec\u7279\u7684\u52a0\u5bc6\u5bc6\u94a5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684PUF\u5982\u4ef2\u88c1PUF\uff08APUF\uff09\u548c\u5f02\u6216\u4ef2\u88c1PUF\uff08XOR-PUF\uff09\u5bb9\u6613\u53d7\u5230\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u653b\u51fb\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u7ec4\u4ef6\u5dee\u5f02\u6027\u6311\u6218\u7684\u5f02\u6216PUF\uff08CDC-XPUF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8f83\u5c11\u63a2\u7d22\u7684\u53d8\u4f53\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u8106\u5f31\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684CDC-XPUF\u8bbe\u8ba1\uff0c\u8be5\u8bbe\u8ba1\u7ed3\u5408\u4e86\u9884\u9009\u7b56\u7565\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u4ee5\u51cf\u5c11\u786c\u4ef6\u5f00\u9500\u3002\u4e25\u683c\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u6211\u4eec\u7684\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u4fdd\u6301\u4e86\u5bf9ML\u653b\u51fb\u7684\u5f3a\u5927\u62b5\u6297\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u653b\u51fb\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86CDC-XPUF\u4f5c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u90e8\u7f72\u7684\u5b89\u5168\u4e14\u9ad8\u6548\u5019\u9009\u8005\u7684\u6f5c\u529b\u3002 Gaoxiang Li PDF N/A Designing Short-Stage CDC-XPUFs: Balancing Reliability, Cost, and Security in IoT Devices The rapid expansion of Internet of Things (IoT) devices demands robust and resource-efficient security solutions. Physically Unclonable Functions (PUFs), which generate unique cryptographic keys from inherent hardware variations, offer a promising approach. However, traditional PUFs like Arbiter PUFs (APUFs) and XOR Arbiter PUFs (XOR-PUFs) are susceptible to machine learning (ML) and reliability-based attacks. In this study, we investigate Component-Differentially Challenged XOR-PUFs (CDC-XPUFs), a less explored variant, to address these vulnerabilities. We propose an optimized CDC-XPUF design that incorporates a pre-selection strategy to enhance reliability and introduces a novel lightweight architecture to reduce hardware overhead. Rigorous testing demonstrates that our design significantly lowers resource consumption, maintains strong resistance to ML attacks, and improves reliability, effectively mitigating reliability-based attacks. These results highlight the potential of CDC-XPUFs as a secure and efficient candidate for widespread deployment in resource-constrained IoT systems. \u901a\u8fc7\u81ea\u76d1\u7763\u8868\u793a\u91cd\u65b0\u5ba1\u89c6\u60c5\u611f\u8bed\u97f3\u548c\u97f3\u4e50\u4e2d\u7684\u58f0\u5b66\u76f8\u4f3c\u6027 \u8bed\u97f3\u548c\u97f3\u4e50\u7684\u60c5\u611f\u8bc6\u522b\u7531\u4e8e\u5176\u58f0\u5b66\u91cd\u53e0\u800c\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u8fd9\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u5728\u8fd9\u4e9b\u9886\u57df\u4e4b\u95f4\u8f6c\u79fb\u77e5\u8bc6\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u8bed\u97f3\u548c\u97f3\u4e50\u4e4b\u95f4\u5171\u4eab\u7684\u58f0\u5b66\u7ebf\u7d22\uff0c\u7279\u522b\u662f\u7531\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u7f16\u7801\u7684\u7ebf\u7d22\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\uff0c\u56e0\u4e3a\u9488\u5bf9\u8bed\u97f3\u548c\u97f3\u4e50\u7684SSL\u6a21\u578b\u5f88\u5c11\u88ab\u5e94\u7528\u4e8e\u8de8\u9886\u57df\u7814\u7a76\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u60c5\u611f\u8bed\u97f3\u548c\u97f3\u4e50\u4e4b\u95f4\u7684\u58f0\u5b66\u76f8\u4f3c\u6027\uff0c\u9996\u5148\u5206\u6790\u4e86\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u548c\u97f3\u4e50\u60c5\u611f\u8bc6\u522b\uff08MER\uff09\u7684SSL\u6a21\u578b\u7684\u9010\u5c42\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u4e24\u9636\u6bb5\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6bd4\u8f83\u51e0\u79cd\u65b9\u6cd5\u6765\u6267\u884c\u8de8\u9886\u57df\u9002\u5e94\uff0c\u63a2\u8ba8\u4e86\u5229\u7528\u97f3\u4e50\u8fdb\u884cSER\u548c\u5229\u7528\u8bed\u97f3\u8fdb\u884cMER\u7684\u6709\u6548\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528Frechet\u97f3\u9891\u8ddd\u79bb\u63a2\u7d22\u4e86\u60c5\u611f\u8bed\u97f3\u548c\u97f3\u4e50\u4e4b\u95f4\u7684\u58f0\u5b66\u76f8\u4f3c\u6027\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u548c\u97f3\u4e50SSL\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u60c5\u611f\u504f\u5dee\u95ee\u9898\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u8bed\u97f3\u548c\u97f3\u4e50SSL\u6a21\u578b\u786e\u5b9e\u6355\u6349\u5230\u4e86\u5171\u4eab\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u4f46\u7531\u4e8e\u5176\u8bad\u7ec3\u7b56\u7565\u548c\u9886\u57df\u7279\u5f02\u6027\uff0c\u5b83\u4eec\u5728\u4e0d\u540c\u60c5\u611f\u4e0b\u7684\u884c\u4e3a\u53ef\u80fd\u4f1a\u6709\u6240\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u53ef\u4ee5\u901a\u8fc7\u76f8\u4e92\u5229\u7528\u77e5\u8bc6\u6765\u63d0\u9ad8SER\u548cMER\u7684\u6027\u80fd\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u60c5\u611f\u8bed\u97f3\u548c\u97f3\u4e50\u4e4b\u95f4\u7684\u58f0\u5b66\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u7a81\u663e\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u5728\u6539\u8fdbSER\u548cMER\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002 Yujia Sun PDF N/A Revisiting Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems. \u65e0\u6a21\u578b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u53d8\u98ce\u6761\u4ef6\u4e0b\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u59ff\u6001\u63a7\u5236\u4e2d\u7684\u6bd4\u8f83 \u672c\u6587\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e86\u65e0\u6a21\u578b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u59ff\u6001\u63a7\u5236\u4e2d\u7684\u6027\u80fd\uff0c\u4ee5PID\u4f5c\u4e3a\u53c2\u8003\u70b9\u3002\u6bd4\u8f83\u7684\u91cd\u70b9\u5728\u4e8e\u5b83\u4eec\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5904\u7406\u4e0d\u540c\u98de\u884c\u52a8\u529b\u5b66\u548c\u98ce\u6270\u52a8\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u65f6\u95f4\u5dee\u5206\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4ee3\u7406\u5728\u4e0d\u540c\u53c2\u8003\u96be\u5ea6\u4e0b\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8ePID\u63a7\u5236\u5668\u548c\u5176\u4ed6\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u975e\u7ebf\u6027\u98de\u884c\u72b6\u6001\u4e0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6267\u884c\u6ce2\u52a8\u4f5c\u4e3a\u8bc4\u4f30\u80fd\u91cf\u6548\u7387\u548c\u6267\u884c\u5668\u78e8\u635f\u7684\u5173\u952e\u6307\u6807\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6587\u732e\u4e2d\u7684\u4e24\u79cd\u4e0d\u540c\u65b9\u6cd5\uff1a\u52a8\u4f5c\u53d8\u5316\u60e9\u7f5a\u548c\u52a8\u4f5c\u7b56\u7565\u5e73\u6ed1\u7684\u6761\u4ef6\u5316\u3002\u6211\u4eec\u8fd8\u5206\u522b\u8bc4\u4f30\u4e86\u6240\u6709\u63a7\u5236\u65b9\u6cd5\u5728\u53d7\u5230\u968f\u673a\u6e4d\u6d41\u548c\u9635\u98ce\u5f71\u54cd\u65f6\u7684\u8868\u73b0\uff0c\u4ee5\u8861\u91cf\u5b83\u4eec\u5bf9\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u89c2\u5bdf\u5b83\u4eec\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6982\u8ff0\u5b83\u4eec\u5bf9\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5f62\u5f0f\u7684\u5f71\u54cd\u3002 David Olivares PDF N/A Model-Free versus Model-Based Reinforcement Learning for Fixed-Wing UAV Attitude Control Under Varying Wind Conditions This paper evaluates and compares the performance of model-free and model-based reinforcement learning for the attitude control of fixed-wing unmanned aerial vehicles using PID as a reference point. The comparison focuses on their ability to handle varying flight dynamics and wind disturbances in a simulated environment. Our results show that the Temporal Difference Model Predictive Control agent outperforms both the PID controller and other model-free reinforcement learning methods in terms of tracking accuracy and robustness over different reference difficulties, particularly in nonlinear flight regimes. Furthermore, we introduce actuation fluctuation as a key metric to assess energy efficiency and actuator wear, and we test two different approaches from the literature: action variation penalty and conditioning for action policy smoothness. We also evaluate all control methods when subject to stochastic turbulence and gusts separately, so as to measure their effects on tracking performance, observe their limitations and outline their implications on the Markov decision process formalism. EMMA-500\uff1a\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u9002\u5e94\u6027 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86EMMA-500\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff0c\u9488\u5bf9546\u79cd\u8bed\u8a00\u7684\u6587\u672c\u8fdb\u884c\u4e86\u6301\u7eed\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u7279\u522b\u662f\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8986\u76d6\u7387\u3002\u4e3a\u4e86\u652f\u6301\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u6211\u4eec\u7f16\u7e82\u4e86MaLA\u8bed\u6599\u5e93\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u878d\u5408\u4e86\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u7cbe\u9009\u6570\u636e\u96c6\u3002\u5229\u7528\u8fd9\u4e00\u8bed\u6599\u5e93\uff0c\u6211\u4eec\u5bf9Llama 2 7B\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5f62\u6210\u4e86EMMA-500\uff0c\u8be5\u6a21\u578b\u5728\u5305\u62ec\u4e00\u7cfb\u5217\u591a\u8bed\u8a00\u4efb\u52a1\u548c\u672c\u7814\u7a76\u4e2d\u5f00\u53d1\u7684\u5f00\u653e\u5f0f\u751f\u6210\u57fa\u51c6PolyWrite\u5728\u5185\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u6301\u7eed\u9884\u8bad\u7ec3\u5728\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u65b9\u9762\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u3001\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u8a00\u9002\u5e94\u6027\u3002 Shaoxiong Ji PDF N/A EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models In this work, we introduce EMMA-500, a large-scale multilingual language model continue-trained on texts across 546 languages designed for enhanced multilingual performance, focusing on improving language coverage for low-resource languages. To facilitate continual pre-training, we compile the MaLA corpus, a comprehensive multilingual dataset enriched with curated datasets across diverse domains. Leveraging this corpus, we conduct extensive continual pre-training of the Llama 2 7B model, resulting in EMMA-500, which demonstrates robust performance across a wide collection of benchmarks, including a comprehensive set of multilingual tasks and PolyWrite, an open-ended generation benchmark developed in this study. Our results highlight the effectiveness of continual pre-training in expanding large language models' language capacity, particularly for underrepresented languages, demonstrating significant gains in cross-lingual transfer, task generalization, and language adaptability. \u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e76\u884cCNN-GRU\u591a\u6e90\u6570\u636e\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u65b9\u6cd5 \u7cbe\u786e\u7684\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u5bf9\u4e8e\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u548c\u786e\u4fdd\u4f9b\u7535\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u8003\u8651\u5230\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u95ee\u9898\u4e0d\u4ec5\u6d89\u53ca\u5386\u53f2\u8d1f\u8377\u53d8\u5316\u7b49\u52a8\u6001\u56e0\u7d20\uff0c\u8fd8\u6d89\u53ca\u5728\u7279\u5b9a\u65f6\u671f\u5185\u4fdd\u6301\u4e0d\u53d8\u7684\u6c14\u5019\u6761\u4ef6\u7b49\u9759\u6001\u56e0\u7d20\u3002\u4ece\u6a21\u578b\u65e0\u5173\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u7ed3\u6784\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u52a8\u6001\u548c\u9759\u6001\u6570\u636e\u4e2d\u63d0\u53d6\u91cd\u8981\u4fe1\u606f\u3002\u9996\u5148\uff0c\u57fa\u4e8e\u590d\u6742\u6027\u5b66\u4e60\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u5e76\u884c\u7ed3\u6784\u96c6\u6210\u7684\u6a21\u578b\u76f8\u6bd4\u5355\u4e2a\u57fa\u5b66\u4e60\u5668\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u57fa\u5b66\u4e60\u5668\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\u8d8a\u9ad8\uff0c\u5e76\u884c\u7ed3\u6784\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u8d8a\u5f3a\u3002\u8fd9\u8868\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7ed3\u6784\u672c\u8eab\u8574\u542b\u7740\u91cd\u8981\u7684\u4fe1\u606f\u3002\u5728\u6b64\u7406\u8bba\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u5e76\u884c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09-\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u6ce8\u610f\u529b\u6a21\u578b\uff08PCGA\uff09\u6765\u89e3\u51b3\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u95ee\u9898\uff0c\u65e8\u5728\u6709\u6548\u6574\u5408\u52a8\u6001\u548c\u9759\u6001\u7279\u5f81\u7684\u5f71\u54cd\u3002CNN\u6a21\u5757\u8d1f\u8d23\u4ece\u9759\u6001\u6570\u636e\u4e2d\u6355\u6349\u7a7a\u95f4\u7279\u5f81\uff0c\u800cGRU\u6a21\u5757\u5219\u6355\u6349\u52a8\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u6ce8\u610f\u529b\u5c42\u65e8\u5728\u805a\u7126\u4e8e\u5e76\u884cCNN-GRU\u63d0\u53d6\u7684\u7a7a\u95f4-\u65f6\u95f4\u7279\u5f81\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u3002\u4e3a\u4e86\u9a8c\u8bc1\u5e76\u884c\u7ed3\u6784\u6a21\u578b\u5728\u63d0\u53d6\u548c\u6574\u5408\u591a\u6e90\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\u3002 Chao Min PDF N/A A multi-source data power load forecasting method using attention mechanism-based parallel cnn-gru Accurate power load forecasting is crucial for improving energy efficiency and ensuring power supply quality. Considering the power load forecasting problem involves not only dynamic factors like historical load variations but also static factors such as climate conditions that remain constant over specific periods. From the model-agnostic perspective, this paper proposes a parallel structure network to extract important information from both dynamic and static data. Firstly, based on complexity learning theory, it is demonstrated that models integrated through parallel structures exhibit superior generalization abilities compared to individual base learners. Additionally, the higher the independence between base learners, the stronger the generalization ability of the parallel structure model. This suggests that the structure of machine learning models inherently contains significant information. Building on this theoretical foundation, a parallel convolutional neural network (CNN)-gate recurrent unit (GRU) attention model (PCGA) is employed to address the power load forecasting issue, aiming to effectively integrate the influences of dynamic and static features. The CNN module is responsible for capturing spatial characteristics from static data, while the GRU module captures long-term dependencies in dynamic time series data. The attention layer is designed to focus on key information from the spatial-temporal features extracted by the parallel CNN-GRU. To substantiate the advantages of the parallel structure model in extracting and integrating multi-source information, a series of experiments are conducted. \u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u54cd\u5e94\u4e2d\u56e0\u679c\u5173\u7cfb\u7684\u65b9\u6cd5 \u9884\u6d4b\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u5728\u968f\u673a\u5bbd\u5e26\u6fc0\u52b1\u4e0b\u7684\u54cd\u5e94\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\uff08\u5982\u7ed3\u6784\u52a8\u529b\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\uff09\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u6784\u5efa\u6570\u636e\u9a71\u52a8\u6a21\u578b\u9700\u8981\u7cfb\u7edf\u7684\u8f93\u5165\u548c\u8f93\u51fa\u5b9e\u9a8c\u6d4b\u91cf\uff0c\u4f46\u5f88\u96be\u786e\u5b9a\u6a21\u578b\u4e2d\u7684\u4e0d\u51c6\u786e\u6027\u662f\u6e90\u4e8e\u5efa\u6a21\u9519\u8bef\u8fd8\u662f\u566a\u58f0\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u9ad8\u4fdd\u771f\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u5b58\u5728\u8f93\u51fa\u566a\u58f0\u7684\u7cfb\u7edf\u6d4b\u91cf\u6570\u636e\u4e2d\uff0c\u6309\u9891\u7387\u8bc6\u522b\u8f93\u5165-\u8f93\u51fa\u6570\u636e\u7684\u56e0\u679c\u6210\u5206\u3002\u901a\u8fc7\u4f7f\u7528\u73b0\u6709\u6a21\u578b\u8ba1\u7b97\u7684\u8f93\u51fa\u9884\u6d4b\uff0c\u4e0e\u566a\u58f0\u6d4b\u91cf\u7684\u8f93\u51fa\u6570\u636e\u8fdb\u884c\u6700\u4f18\u7ec4\u5408\uff0c\u4ee5\u9884\u6d4b\u7cfb\u7edf\u7684\u8f93\u5165\u3002\u8be5\u7b97\u6cd5\u7684\u53c2\u6570\u7528\u4e8e\u5e73\u8861\u8fd9\u4e24\u79cd\u8f93\u51fa\u4fe1\u53f7\uff0c\u5e76\u7528\u4e8e\u8ba1\u7b97\u975e\u7ebf\u6027\u76f8\u5e72\u5ea6\u91cf\u4f5c\u4e3a\u56e0\u679c\u5173\u7cfb\u7684\u5ea6\u91cf\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u3002\u76ee\u524d\uff0c\u5728\u6ca1\u6709\u5b8c\u6574\u57fa\u51c6\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5c1a\u65e0\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u65b9\u6848\u3002 Joseph Massingham PDF N/A A method for identifying causality in the response of nonlinear dynamical systems Predicting the response of nonlinear dynamical systems subject to random, broadband excitation is important across a range of scientific disciplines, such as structural dynamics and neuroscience. Building data-driven models requires experimental measurements of the system input and output, but it can be difficult to determine whether inaccuracies in the model stem from modelling errors or noise. This paper presents a novel method to identify the causal component of the input-output data from measurements of a system in the presence of output noise, as a function of frequency, without needing a high fidelity model. An output prediction, calculated using an available model, is optimally combined with noisy measurements of the output to predict the input to the system. The parameters of the algorithm balance the two output signals and are utilised to calculate a nonlinear coherence metric as a measure of causality. This method is applicable to a broad class of nonlinear dynamical systems. There are currently no solutions to this problem in the absence of a complete benchmark model. GPU\u5f20\u91cf\u6838\u5fc3\u4e0a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u4efb\u610f\u7cbe\u5ea6\u52a0\u901f \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u5c3d\u7ba1\u91cf\u5316\u65b9\u6cd5\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u4f46\u4efb\u610f\u7cbe\u5ea6\u7684\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u53d7\u5230\u6709\u9650\u7684GPU\u5f20\u91cf\u6838\u5fc3\u652f\u6301\u548c\u4e0d\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u9650\u5236\uff0c\u5bfc\u81f4\u52a0\u901f\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4efb\u610f\u7cbe\u5ea6LLMs\u7684\u7efc\u5408\u52a0\u901f\u65b9\u6848\u3002\u6838\u5fc3\u5728\u4e8e\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u53cc\u6781\u6027-INT\u6570\u636e\u683c\u5f0f\uff0c\u8be5\u683c\u5f0f\u4fc3\u8fdb\u4e86\u5e76\u884c\u8ba1\u7b97\u5e76\u652f\u6301\u5bf9\u79f0\u91cf\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6570\u636e\u5197\u4f59\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u79cd\u4efb\u610f\u7cbe\u5ea6\u7684\u77e9\u9635\u4e58\u6cd5\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5728\u6bd4\u7279\u7ea7\u522b\u5206\u89e3\u548c\u6062\u590d\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u7cbe\u5ea6\u540c\u65f6\u6700\u5927\u5316GPU\u5f20\u91cf\u6838\u5fc3\u7684\u5229\u7528\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u77e9\u9635\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u540e\u7eed\u8ba1\u7b97\u7684\u6570\u636e\u5e03\u5c40\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9762\u5411\u6570\u636e\u6062\u590d\u7684\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u6218\u7565\u6027\u5730\u5229\u7528\u5feb\u901f\u5171\u4eab\u5185\u5b58\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u6267\u884c\u901f\u5ea6\u5e76\u6700\u5c0f\u5316\u4e86\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\uff0c\u77e9\u9635\u4e58\u6cd5\u901f\u5ea6\u76f8\u6bd4NVIDIA\u7684CUTLASS\u63d0\u5347\u4e8613\u500d\u3002\u5f53\u96c6\u6210\u5230LLMs\u4e2d\u65f6\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u9ad8\u8fbe6.7\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u8fd9\u4e9b\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u6548\u7387\uff0c\u4f7f\u5f97LLMs\u7684\u5e94\u7528\u66f4\u52a0\u5e7f\u6cdb\u548c\u54cd\u5e94\u8fc5\u901f\u3002 Shaobo Ma PDF N/A Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores Large language models (LLMs) have been widely applied but face challenges in efficient inference. While quantization methods reduce computational demands, ultra-low bit quantization with arbitrary precision is hindered by limited GPU Tensor Core support and inefficient memory management, leading to suboptimal acceleration. To address these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs. At its core, we introduce a novel bipolar-INT data format that facilitates parallel computing and supports symmetric quantization, effectively reducing data redundancy. Building on this, we implement an arbitrary precision matrix multiplication scheme that decomposes and recovers matrices at the bit level, enabling flexible precision while maximizing GPU Tensor Core utilization. Furthermore, we develop an efficient matrix preprocessing method that optimizes data layout for subsequent computations. Finally, we design a data recovery-oriented memory management system that strategically utilizes fast shared memory, significantly enhancing kernel execution speed and minimizing memory access latency. Experimental results demonstrate our approach's effectiveness, with up to 13\\times speedup in matrix multiplication compared to NVIDIA's CUTLASS. When integrated into LLMs, we achieve up to 6.7\\times inference acceleration. These improvements significantly enhance LLM inference efficiency, enabling broader and more responsive applications of LLMs. \u5b9e\u65bd\u5317\u6b27-\u6ce2\u7f57\u7684\u6d77\u8054\u90a6\u5065\u5eb7\u6570\u636e\u7f51\u7edc\uff1a\u6848\u4f8b\u62a5\u544a \u80cc\u666f\uff1a\u8de8\u56fd\u754c\u96c6\u4e2d\u6536\u96c6\u548c\u5904\u7406\u533b\u7597\u6570\u636e\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5305\u62ec\u9690\u79c1\u95ee\u9898\u3001\u6570\u636e\u5f02\u8d28\u6027\u548c\u6cd5\u5f8b\u969c\u788d\u3002\u4e3a\u4e86\u5e94\u5bf9\u5176\u4e2d\u4e00\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u7ec4\u5efa\u4e86\u4e00\u4e2a\u8de8\u5b66\u79d1\u8054\u76df\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7531\u4e94\u4e2a\u56fd\u5bb6\u7684\u516d\u4e2a\u673a\u6784\u7ec4\u6210\u7684\u8054\u90a6\u5065\u5eb7\u6570\u636e\u7f51\u7edc\uff0c\u4ee5\u4fc3\u8fdb\u5317\u6b27-\u6ce2\u7f57\u7684\u6d77\u5730\u533a\u5728\u5065\u5eb7\u6570\u636e\u4e8c\u6b21\u4f7f\u7528\u65b9\u9762\u7684\u5408\u4f5c\u3002\u672c\u62a5\u544a\u65e8\u5728\u63d0\u4f9b\u6211\u4eec\u5728\u5f00\u53d1\u8be5\u7f51\u7edc\u8fc7\u7a0b\u4e2d\u7684\u65e9\u671f\u89c1\u89e3\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u91c7\u7528\u4e86\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u5b9e\u65bd\u79d1\u5b66\uff0c\u4ee5\u8bc4\u4f30\u5f71\u54cd\u6211\u4eec\u7f51\u7edc\u5b9e\u65bd\u7684\u56e0\u7d20\u3002\u7ed3\u679c\uff1a\u4ece\u6280\u672f\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u6a21\u62df\u76f8\u6bd4\uff0c\u8be5\u7f51\u7edc\u5728\u529f\u80fd\u4e0a\u6ca1\u6709\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002\u7ed3\u8bba\uff1a\u5c3d\u7ba1\u8de8\u5b66\u79d1\u65b9\u6cd5\u5728\u89e3\u51b3\u5efa\u7acb\u6b64\u7c7b\u534f\u4f5c\u7f51\u7edc\u6240\u9762\u4e34\u7684\u6311\u6218\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u76d1\u7ba1\u73af\u5883\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u663e\u8457\u7684\u8fd0\u8425\u6210\u672c\u3002 Taridzo Chomutare PDF N/A Implementing a Nordic-Baltic Federated Health Data Network: a case report Background: Centralized collection and processing of healthcare data across national borders pose significant challenges, including privacy concerns, data heterogeneity and legal barriers. To address some of these challenges, we formed an interdisciplinary consortium to develop a feder-ated health data network, comprised of six institutions across five countries, to facilitate Nordic-Baltic cooperation on secondary use of health data. The objective of this report is to offer early insights into our experiences developing this network. Methods: We used a mixed-method ap-proach, combining both experimental design and implementation science to evaluate the factors affecting the implementation of our network. Results: Technically, our experiments indicate that the network functions without significant performance degradation compared to centralized simu-lation. Conclusion: While use of interdisciplinary approaches holds a potential to solve challeng-es associated with establishing such collaborative networks, our findings turn the spotlight on the uncertain regulatory landscape playing catch up and the significant operational costs. \u4e00\u79cd\u7528\u4e8e\u51b7\u542f\u52a8\u548c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u63a8\u8350\u7684\u5355\u5206\u652f\u591a\u6a21\u6001\u5d4c\u5165\u7f51\u7edc \u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u91c7\u7528\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\uff0c\u5e76\u57fa\u4e8e\u8fc7\u53bb\u7684\u96c6\u4f53\u4e92\u52a8\u63d0\u4f9b\u63a8\u8350\u3002\u56e0\u6b64\uff0c\u5f53\u53ef\u7528\u7684\u4e92\u52a8\u5f88\u5c11\u6216\u6ca1\u6709\u65f6\uff0cCF\u7b97\u6cd5\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u8fd9\u79cd\u60c5\u51b5\u88ab\u79f0\u4e3a\u51b7\u542f\u52a8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u5f80\u7684\u5de5\u4f5c\u4f9d\u8d56\u4e8e\u540c\u65f6\u5229\u7528\u534f\u540c\u6570\u636e\u548c\u7528\u6237\u6216\u9879\u76ee\u4fa7\u4fe1\u606f\u7684\u6a21\u578b\u3002\u7c7b\u4f3c\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u8fd9\u4e9b\u6a21\u578b\u65e8\u5728\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7ed3\u5408\u534f\u540c\u548c\u5185\u5bb9\u8868\u793a\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u63a8\u8350\u6280\u672f\uff0c\u4f9d\u8d56\u4e8e\u4e00\u79cd\u540d\u4e3a\u63a8\u8350\u7684\u591a\u6a21\u6001\u5355\u5206\u652f\u5d4c\u5165\u7f51\u7edc\uff08SiBraR\uff09\u3002\u5229\u7528\u6743\u91cd\u5171\u4eab\uff0cSiBraR\u5728\u4e0d\u540c\u6a21\u6001\u4e0a\u4f7f\u7528\u76f8\u540c\u7684\u5355\u5206\u652f\u5d4c\u5165\u7f51\u7edc\u5bf9\u4ea4\u4e92\u6570\u636e\u548c\u591a\u6a21\u6001\u4fa7\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002\u8fd9\u4f7f\u5f97SiBraR\u5728\u5305\u62ec\u51b7\u542f\u52a8\u5728\u5185\u7684\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\u3002\u6211\u4eec\u5728\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u63a8\u8350\u9886\u57df\uff08\u97f3\u4e50\u3001\u7535\u5f71\u548c\u7535\u5b50\u5546\u52a1\uff09\u7684\u5927\u89c4\u6a21\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u5185\u5bb9\u4fe1\u606f\uff08\u97f3\u9891\u3001\u6587\u672c\u3001\u56fe\u50cf\u3001\u6807\u7b7e\u548c\u4e92\u52a8\uff09\uff0c\u7ed3\u679c\u663e\u793aSiBraR\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8eCF\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u5185\u5bb9\u63a8\u8350\u7cfb\u7edf\uff0c\u5728\u6696\u542f\u52a8\u573a\u666f\u4e2d\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u3002\u6211\u4eec\u5c55\u793a\u4e86SiBraR\u5728\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e2d\u7684\u63a8\u8350\u51c6\u786e\u6027\uff0c\u5e76\u8868\u660e\u8be5\u6a21\u578b\u80fd\u591f\u5c06\u4e0d\u540c\u6a21\u6001\u6620\u5c04\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u7684\u540c\u4e00\u533a\u57df\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u6a21\u6001\u5dee\u8ddd\u3002 Christian Ganh\u00f6r PDF N/A A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios Most recommender systems adopt collaborative filtering (CF) and provide recommendations based on past collective interactions. Therefore, the performance of CF algorithms degrades when few or no interactions are available, a scenario referred to as cold-start. To address this issue, previous work relies on models leveraging both collaborative data and side information on the users or items. Similar to multimodal learning, these models aim at combining collaborative and content representations in a shared embedding space. In this work we propose a novel technique for multimodal recommendation, relying on a multimodal Single-Branch embedding network for Recommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction data as well as multimodal side information using the same single-branch embedding network on different modalities. This makes SiBraR effective in scenarios of missing modality, including cold start. Our extensive experiments on large-scale recommendation datasets from three different recommendation domains (music, movie, and e-commerce) and providing multimodal content information (audio, text, image, labels, and interactions) show that SiBraR significantly outperforms CF as well as state-of-the-art content-based RSs in cold-start scenarios, and is competitive in warm scenarios. We show that SiBraR's recommendations are accurate in missing modality scenarios, and that the model is able to map different modalities to the same region of the shared embedding space, hence reducing the modality gap."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}