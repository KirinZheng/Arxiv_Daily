{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Arxiv Daily","text":"<p>DeepSeek\uff0c\u4f60\u8bb0\u4e00\u4e0b\u6211\u505a\u5982\u4e0b\u90e8\u7f72 -- 101</p>"},{"location":"arxiv_papers/","title":"Arxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract Molmo \u548c PixMo\uff1a\u4e3a\u6700\u5148\u8fdb\u7684\u8de8\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u5f00\u653e\u6743\u91cd\u548c\u5f00\u653e\u6570\u636e \u5f53\u4eca\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u662f\u4e13\u6709\u7684\u3002\u6700\u5f3a\u5927\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6765\u81ea\u4e13\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5408\u6210\u6570\u636e\u4ee5\u5b9e\u73b0\u826f\u597d\u6027\u80fd\uff0c\u5b9e\u9645\u4e0a\u662f\u5c06\u8fd9\u4e9b\u5c01\u95ed\u6a21\u578b\u63d0\u70bc\u4e3a\u5f00\u653e\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u793e\u533a\u4ecd\u7136\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u4ece\u5934\u6784\u5efa\u9ad8\u6027\u80fdVLM\u7684\u57fa\u7840\u77e5\u8bc6\u3002\u6211\u4eec\u63d0\u51fa\u4e86Molmo\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u5176\u5f00\u653e\u6027\u7c7b\u522b\u4e2d\u5904\u4e8e\u6700\u5148\u8fdb\u6c34\u5e73\u7684VLM\u65b0\u5bb6\u65cf\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u4e00\u79cd\u5168\u65b0\u3001\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5168\u7531\u4eba\u7c7b\u6807\u6ce8\u8005\u4f7f\u7528\u57fa\u4e8e\u8bed\u97f3\u7684\u63cf\u8ff0\u6536\u96c6\u7684\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u652f\u6301\u5e7f\u6cdb\u7684\u7528\u6237\u4ea4\u4e92\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u5fae\u8c03\u6570\u636e\u96c6\u6df7\u5408\uff0c\u5305\u62ec\u81ea\u7136\u73af\u5883\u4e2d\u7684\u95ee\u7b54\u548c\u521b\u65b0\u7684\u4e8c\u7ef4\u6307\u5411\u6570\u636e\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5bf9\u6a21\u578b\u67b6\u6784\u7ec6\u8282\u7684\u7cbe\u5fc3\u9009\u62e9\u3001\u7ecf\u8fc7\u826f\u597d\u8c03\u4f18\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ee5\u53ca\u6700\u5173\u952e\u7684\uff0c\u6211\u4eec\u65b0\u6536\u96c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u8fd9\u4e9b\u90fd\u5c06\u88ab\u53d1\u5e03\u3002Molmo\u5bb6\u65cf\u4e2d\u6700\u4f73\u768472B\u6a21\u578b\u4e0d\u4ec5\u5728\u5f00\u653e\u6743\u91cd\u548c\u6570\u636e\u6a21\u578b\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u4e14\u5728\u5b66\u672f\u57fa\u51c6\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4e0eGPT-4o\u3001Claude 3.5\u548cGemini 1.5\u7b49\u4e13\u6709\u7cfb\u7edf\u76f8\u6bd4\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u5c06\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u53d1\u5e03\u6240\u6709\u6a21\u578b\u6743\u91cd\u3001\u63cf\u8ff0\u548c\u5fae\u8c03\u6570\u636e\u4ee5\u53ca\u6e90\u4ee3\u7801\u3002\u9009\u5b9a\u7684\u6a21\u578b\u6743\u91cd\u3001\u63a8\u7406\u4ee3\u7801\u548c\u6f14\u793a\u53ef\u5728https://molmo.allenai.org\u83b7\u53d6\u3002 Matt Deitke PDF N/A Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&amp;A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org. DreamWaltz-G\uff1a\u4ece\u9aa8\u9abc\u5f15\u5bfc\u76842D\u6269\u6563\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u76843D\u9ad8\u65af\u5934\u50cf \u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e8c\u7ef4\u6269\u6563\u6a21\u578b\u548c\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u6587\u672c\u52303D\u5934\u50cf\u751f\u6210\u65b9\u9762\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u751f\u6210\u80fd\u591f\u8fdb\u884c\u8868\u60c5\u52a8\u753b\u7684\u9ad8\u8d28\u91cf3D\u5934\u50cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DreamWaltz-G\uff0c\u4e00\u79cd\u4ece\u6587\u672c\u751f\u6210\u53ef\u52a8\u753b3D\u5934\u50cf\u7684\u65b0\u578b\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u5728\u4e8e\u9aa8\u9abc\u5f15\u5bfc\u7684\u5206\u6570\u84b8\u998f\u548c\u6df7\u54083D\u9ad8\u65af\u5934\u50cf\u8868\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6240\u63d0\u51fa\u7684\u9aa8\u9abc\u5f15\u5bfc\u5206\u6570\u84b8\u998f\u5c063D\u4eba\u4f53\u6a21\u677f\u4e2d\u7684\u9aa8\u9abc\u63a7\u5236\u6574\u5408\u52302D\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u589e\u5f3a\u4e86SDS\u76d1\u7763\u5728\u89c6\u89d2\u548c\u4eba\u4f53\u59ff\u6001\u65b9\u9762\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u6709\u52a9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5934\u50cf\uff0c\u7f13\u89e3\u4e86\u591a\u9762\u3001\u591a\u4f59\u80a2\u4f53\u548c\u6a21\u7cca\u7b49\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u6df7\u54083D\u9ad8\u65af\u5934\u50cf\u8868\u793a\u57fa\u4e8e\u9ad8\u6548\u76843D\u9ad8\u65af\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u9690\u5f0f\u573a\u548c\u53c2\u6570\u53163D\u7f51\u683c\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3001\u7a33\u5b9a\u7684SDS\u4f18\u5316\u548c\u8868\u60c5\u52a8\u753b\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDreamWaltz-G\u5728\u751f\u6210\u548c\u52a8\u753b3D\u5934\u50cf\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u52a8\u753b\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u8fd8\u652f\u6301\u591a\u6837\u5316\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4eba\u7c7b\u89c6\u9891\u91cd\u6f14\u548c\u591a\u4e3b\u4f53\u573a\u666f\u5408\u6210\u3002 Yukun Huang PDF N/A DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition. \u5dee\u5206\u9690\u79c1\u6b63\u5219\u5316\uff1a\u901a\u8fc7\u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u5305\u542b\u654f\u611f\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u5e94\u6cc4\u9732\u6570\u636e\u96c6\u4e2d\u7684\u79c1\u4eba\u4fe1\u606f\u3002\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DP-SGD\uff09\u7b97\u6cd5\u9700\u8981\u5bf9\u6807\u51c6\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7b97\u6cd5\u8fdb\u884c\u4fee\u6539\uff0c\u4ee5\u4fbf\u8bad\u7ec3\u65b0\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4ee5\u66f4\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u76f8\u540c\u7684\u76ee\u6807\u3002 Francisco Aguilera-Mart\u00ednez PDF N/A Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization Training machine learning models based on neural networks requires large datasets, which may contain sensitive information. The models, however, should not expose private information from these datasets. Differentially private SGD [DP-SGD] requires the modification of the standard stochastic gradient descent [SGD] algorithm for training new models. In this short paper, a novel regularization strategy is proposed to achieve the same goal in a more efficient manner. FineZip\uff1a\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u65e0\u635f\u6587\u672c\u538b\u7f29\u4e2d\u7684\u6781\u9650 \u5c3d\u7ba1\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u5df2\u88ab\u8bc1\u660e\u4e0e\u538b\u7f29\u6709\u7740\u6df1\u523b\u7684\u8054\u7cfb\uff0c\u4f46\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u672a\u88ab\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\u4e2d\u3002\u672c\u6587\u5bf9\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548c\u53d8\u6362\u5668\u7684\u538b\u7f29\u6280\u672f\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u4ee5\u89e3\u7b54\u8fd9\u4e00\u95ee\u9898\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u4f20\u7edf\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\u4e0e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548cLLM\u7684\u6587\u672c\u538b\u7f29\u65b9\u6cd5\u3002\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6781\u4e3a\u4e0d\u5207\u5b9e\u9645\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6700\u8fd1\u7684\u4e00\u4e2a\u6587\u672c\u538b\u7f29\u7cfb\u7edfLLMZip\u4f7f\u7528\u4e86Llama3-8B\u6a21\u578b\uff0c\u5c3d\u7ba1\u5728\u538b\u7f29\u6bd4\u7387\u4e0a\u6709\u5de8\u5927\u63d0\u5347\uff0c\u4f46\u538b\u7f2910 MB\u7684\u6587\u672c\u9700\u89819.5\u5929\u7684\u65f6\u95f4\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FineZip\u2014\u2014\u4e00\u79cd\u7ed3\u5408\u4e86\u5728\u7ebf\u8bb0\u5fc6\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u6982\u5ff5\u7684\u65b0\u578b\u57fa\u4e8eLLM\u7684\u6587\u672c\u538b\u7f29\u7cfb\u7edf\uff0c\u6781\u5927\u5730\u51cf\u5c11\u4e86\u538b\u7f29\u65f6\u95f4\u3002\u4e0eLLMZip\u76f8\u6bd4\uff0cFineZip\u80fd\u5728\u7ea64\u5c0f\u65f6\u5185\u5b8c\u6210\u4e0a\u8ff0\u8bed\u6599\u7684\u538b\u7f29\uff0c\u6bd4LLMZip\u5feb\u4e8654\u500d\uff0c\u4e14\u6027\u80fd\u76f8\u5f53\u3002FineZip\u5728\u538b\u7f29\u6bd4\u7387\u4e0a\u5927\u5e45\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u7b97\u6cd5\u538b\u7f29\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u7ea650%\u3002\u901a\u8fc7\u8fd9\u9879\u5de5\u4f5c\uff0c\u6211\u4eec\u8fc8\u51fa\u4e86\u4f7f\u57fa\u4e8eLLM\u7684\u65e0\u635f\u6587\u672c\u538b\u7f29\u6210\u4e3a\u73b0\u5b9e\u7684\u7b2c\u4e00\u6b65\u3002\u5c3d\u7ba1FineZip\u5728\u8fd9\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4f46LLMs\u4ecd\u672a\u6210\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u538b\u7f29\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u521b\u65b0\u94fa\u5e73\u9053\u8def\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002 Fazal Mittu PDF N/A FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression While the language modeling objective has been shown to be deeply connected with compression, it is surprising that modern LLMs are not employed in practical text compression systems. In this paper, we provide an in-depth analysis of neural network and transformer-based compression techniques to answer this question. We compare traditional text compression systems with neural network and LLM-based text compression methods. Although LLM-based systems significantly outperform conventional compression methods, they are highly impractical. Specifically, LLMZip, a recent text compression system using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with huge improvements in compression ratios. To overcome this, we present FineZip - a novel LLM-based text compression system that combines ideas of online memorization and dynamic context to reduce the compression time immensely. FineZip can compress the above corpus in approximately 4 hours compared to 9.5 days, a 54 times improvement over LLMZip and comparable performance. FineZip outperforms traditional algorithmic compression methods with a large margin, improving compression ratios by approximately 50\\%. With this work, we take the first step towards making lossless text compression with LLMs a reality. While FineZip presents a significant step in that direction, LLMs are still not a viable solution for large-scale text compression. We hope our work paves the way for future research and innovation to solve this problem. \u52a8\u6001\u5b66\u4e60\uff1a\u57fa\u4e8e\u52a8\u6001\u65e0\u4eba\u673a\u56e2\u961f\u7684\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u81ea\u4e3b\u8c03\u8282 \u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u901a\u4fe1\u7f51\u7edc\uff08UCN\uff09\u662f\u672a\u6765\u79fb\u52a8\u7f51\u7edc\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u4e3a\u4e86\u5e94\u5bf9UCN\u4e2d\u52a8\u6001\u73af\u5883\u7684\u53d8\u5316\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u56e0\u5176\u5f3a\u5927\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u80fd\u529b\u800c\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u73af\u5883\u6a21\u578b\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8eRL\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u63a7\u5236\u7b56\u7565\u8bbe\u8ba1\u4e0a\uff0c\u5047\u8bbe\u65e0\u4eba\u673a\u6570\u91cf\u662f\u56fa\u5b9a\u7684\u3002\u5f88\u5c11\u6709\u7814\u7a76\u63a2\u8ba8\u5f53\u670d\u52a1\u65e0\u4eba\u673a\u52a8\u6001\u53d8\u5316\u65f6\uff0c\u5982\u4f55\u81ea\u9002\u5e94\u5730\u8c03\u8282UCN\u3002\u672c\u6587\u8ba8\u8bba\u4e86\u5728\u52a8\u6001\u65e0\u4eba\u673a\u96c6\u5408\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8eRL\u7684\u7b56\u7565\u8bbe\u8ba1\u4ee5\u5b9e\u73b0UCN\u7684\u81ea\u9002\u5e94\u8c03\u8282\uff0c\u6db5\u76d6\u4e86\u901a\u7528UCN\u4e2d\u7684\u53cd\u5e94\u6027\u7b56\u7565\u548c\u592a\u9633\u80fdUCN\u4e2d\u7684\u524d\u77bb\u6027\u7b56\u7565\u3002\u9996\u5148\u6982\u8ff0\u4e86UCN\u548cRL\u6846\u67b6\uff0c\u7136\u540e\u8be6\u7ec6\u9610\u8ff0\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u53ca\u5176\u5173\u952e\u6311\u6218\u548c\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6700\u540e\uff0c\u4ee5\u6211\u4eec\u6700\u8fd1\u7684\u4e00\u4e9b\u5de5\u4f5c\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u4e0d\u540c\u7684RL\u7b97\u6cd5\u5904\u7406\u52a8\u6001\u65e0\u4eba\u673a\u56e2\u961f\uff0c\u4ee5\u6fc0\u53d1\u521b\u65b0\u7684\u65b9\u6cd5\u3002 Ran Zhang PDF N/A Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key component in future mobile networking. To handle the dynamic environments in UCNs, reinforcement learning (RL) has been a promising solution attributed to its strong capability of adaptive decision-making free of the environment models. However, most existing RL-based research focus on control strategy design assuming a fixed set of UAVs. Few works have investigated how UCNs should be adaptively regulated when the serving UAVs change dynamically. This article discusses RL-based strategy design for adaptive UCN regulation given a dynamic UAV set, addressing both reactive strategies in general UCNs and proactive strategies in solar-powered UCNs. An overview of the UCN and the RL framework is first provided. Potential research directions with key challenges and possible solutions are then elaborated. Some of our recent works are presented as case studies to inspire innovative ways to handle dynamic UAV crew with different RL algorithms. \u6709\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u4e2d\u5177\u6709\u4e00\u822c\u72b6\u6001\u548c\u52a8\u4f5c\u7684\u653f\u7b56\u4f18\u5316\u666f\u89c2 \u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u7b56\u7565\u4f18\u5316\u7684\u975e\u51f8\u6027\u7ed9\u7406\u89e3\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u5168\u5c40\u6536\u655b\u6027\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u5bf9\u4e8e\u4e00\u7c7b\u5177\u6709\u4e00\u822c\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u7ec4\u6613\u4e8e\u9a8c\u8bc1\u7684\u5047\u8bbe\uff0c\u4ee5\u786e\u4fdd\u7b56\u7565\u4f18\u5316\u7684Kurdyka-Lojasiewicz\uff08KL\uff09\u6761\u4ef6\u3002\u5229\u7528KL\u6761\u4ef6\uff0c\u5c3d\u7ba1\u7b56\u7565\u4f18\u5316\u662f\u975e\u51f8\u7684\uff0c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4ecd\u80fd\u4ee5\u975e\u6e10\u8fd1\u901f\u7387\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u7b56\u7565\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5728\u5404\u79cd\u63a7\u5236\u548c\u8fd0\u8425\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u5e94\u7528\uff0c\u5305\u62ec\u71b5\u6b63\u5219\u5316\u7684\u8868\u683cMDPs\u3001\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u95ee\u9898\u3001\u968f\u673a\u5e93\u5b58\u6a21\u578b\u548c\u968f\u673a\u73b0\u91d1\u4f59\u989d\u95ee\u9898\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u901a\u8fc7\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728$\\tilde{\\mathcal{O}}(\\epsilon^{-1})$\u7684\u6837\u672c\u91cf\u548c\u5173\u4e8e\u89c4\u5212\u65f6\u57df\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u83b7\u5f97$\\epsilon$-\u6700\u4f18\u7b56\u7565\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5728\u6587\u732e\u4e2d\u9996\u6b21\u5efa\u7acb\u4e86\u5177\u6709\u9a6c\u5c14\u53ef\u592b\u8c03\u5236\u9700\u6c42\u7684\u591a\u5143\u5e93\u5b58\u7cfb\u7edf\u548c\u968f\u673a\u73b0\u91d1\u4f59\u989d\u95ee\u9898\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002 Xin Chen PDF N/A Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy optimization. Leveraging the KL condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an $\\epsilon$-optimal policy can be obtained using a sample size in $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature. PACE\uff1a\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u7684\u6cdb\u5316\u4e0e\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f8\u7ed3\u5408 \u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08Parameter-Efficient Fine-Tuning, PEFT\uff09\u6709\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668\u9002\u5e94\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u7684\u4f18\u5316\u5f80\u5f80\u4ee5\u5fae\u8c03\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u727a\u7272\u4e3a\u4ee3\u4ef7\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8f83\u5c0f\u7684\u6743\u91cd\u68af\u5ea6\u8303\u6570\u548c\u8f83\u5927\u7684\u6570\u636e\u96c6\u4e0e\u6539\u8fdb\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8054\u7cfb\u8d77\u6765\u3002\u53d7\u6b64\u8054\u7cfb\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u51cf\u5c0f\u68af\u5ea6\u8303\u6570\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4f7f\u5fae\u8c03\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u9f50\uff0c\u4ee5\u4fdd\u7559\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u7b80\u5355\u7684\u5bf9\u9f50\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u68af\u5ea6\u7684\u51cf\u5c0f\uff0c\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u7206\u70b8\uff0c\u589e\u52a0\u7ba1\u7406\u68af\u5ea6\u7684\u96be\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PACE\uff0c\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u901a\u8fc7\u4e58\u6027\u566a\u58f0\u6270\u52a8\u4ece\u9002\u914d\u5668\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff0c\u5e76\u786e\u4fdd\u5fae\u8c03\u6a21\u578b\u5728\u4e0d\u540c\u6270\u52a8\u4e0b\u5bf9\u540c\u4e00\u6837\u672c\u4fdd\u6301\u4e00\u81f4\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cPACE\u4e0d\u4ec5\u9690\u5f0f\u5730\u5bf9\u68af\u5ea6\u8fdb\u884c\u6b63\u5219\u5316\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u9690\u5f0f\u5730\u5bf9\u9f50\u4e86\u5fae\u8c03\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u4fdd\u7559\u77e5\u8bc6\u3002\u5b9e\u9a8c\u8bc1\u636e\u652f\u6301\u4e86\u6211\u4eec\u7684\u7406\u8bba\u3002\u5728\u56db\u4e2a\u89c6\u89c9\u9002\u5e94\u4efb\u52a1\u4e2d\uff0cPACE\u4f18\u4e8e\u73b0\u6709\u7684PEFT\u65b9\u6cd5\uff1aVTAB-1k\u3001FGVC\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u9886\u57df\u9002\u5e94\u3002\u4ee3\u7801\u5c06\u5728https://github.com/MaxwellYaoNi/PACE\u63d0\u4f9b\u3002 Yao Ni PDF N/A PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE outperforms existing PEFT methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and domain adaptation. Code will be available at https://github.com/MaxwellYaoNi/PACE \u8bc4\u4f30\u5b5f\u52a0\u62c9\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u4e2d\u5bf9\u4e0d\u540c\u7fa4\u4f53\u7684\u6bd2\u6027\u7a0b\u5ea6\uff1a\u4e00\u9879\u5168\u9762\u8c03\u67e5 \u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u73b0\u4ee3\u793e\u4f1a\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\uff0c\u5b83\u4eec\u662f\u6c9f\u901a\u3001\u601d\u60f3\u4ea4\u6d41\u548c\u5efa\u7acb\u7f51\u7edc\u7684\u6e20\u9053\u3002\u7136\u800c\uff0c\u901a\u8fc7\u6709\u6bd2\u8bc4\u8bba\uff08\u4ece\u5192\u72af\u6027\u8a00\u8bba\u5230\u4ec7\u6068\u8a00\u8bba\uff09\u6ee5\u7528\u8fd9\u4e9b\u5e73\u53f0\u662f\u4e00\u4e2a\u4ee4\u4eba\u62c5\u5fe7\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u8bc6\u522b\u9488\u5bf9\u4e09\u4e2a\u7279\u5b9a\u7fa4\u4f53\u2014\u2014\u8de8\u6027\u522b\u8005\u3001\u539f\u4f4f\u6c11\u548c\u79fb\u6c11\u2014\u2014\u7684\u5b5f\u52a0\u62c9\u8bed\u6709\u6bd2\u8bc4\u8bba\uff0c\u8fd9\u4e9b\u8bc4\u8bba\u6765\u81ea\u591a\u4e2a\u793e\u4ea4\u5a92\u4f53\u6765\u6e90\u3002\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u8bc6\u522b\u548c\u5206\u7c7b\u6709\u6bd2\u8bed\u8a00\u7684\u590d\u6742\u8fc7\u7a0b\uff0c\u540c\u65f6\u8003\u8651\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u6709\u6bd2\u6027\uff1a\u9ad8\u3001\u4e2d\u3001\u4f4e\u3002\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u521b\u5efa\u6570\u636e\u96c6\u3001\u624b\u52a8\u6ce8\u91ca\uff0c\u5e76\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8f6c\u6362\u5668\u6a21\u578b\u5982Bangla-BERT\u3001bangla-bert-base\u3001distil-BERT\u548cBert-base-multilingual-cased\u8fdb\u884c\u5206\u7c7b\u3002\u91c7\u7528\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5982\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\uff0c\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBangla-BERT\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u8fbe\u5230\u4e860.8903\u7684F1\u5206\u6570\u3002\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86\u5b5f\u52a0\u62c9\u793e\u4ea4\u5a92\u4f53\u5bf9\u8bdd\u4e2d\u6709\u6bd2\u6027\u7684\u590d\u6742\u6027\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u4e0d\u540c\u5f71\u54cd\u3002 Mukaffi Bin Moin PDF N/A Assessing the Level of Toxicity Against Distinct Groups in Bangla Social Media Comments: A Comprehensive Investigation Social media platforms have a vital role in the modern world, serving as conduits for communication, the exchange of ideas, and the establishment of networks. However, the misuse of these platforms through toxic comments, which can range from offensive remarks to hate speech, is a concerning issue. This study focuses on identifying toxic comments in the Bengali language targeting three specific groups: transgender people, indigenous people, and migrant people, from multiple social media sources. The study delves into the intricate process of identifying and categorizing toxic language while considering the varying degrees of toxicity: high, medium, and low. The methodology involves creating a dataset, manual annotation, and employing pre-trained transformer models like Bangla-BERT, bangla-bert-base, distil-BERT, and Bert-base-multilingual-cased for classification. Diverse assessment metrics such as accuracy, recall, precision, and F1-score are employed to evaluate the model's effectiveness. The experimental findings reveal that Bangla-BERT surpasses alternative models, achieving an F1-score of 0.8903. This research exposes the complexity of toxicity in Bangla social media dialogues, revealing its differing impacts on diverse demographic groups. Blox-Net\uff1a\u5229\u7528VLM\u76d1\u7763\u3001\u7269\u7406\u6a21\u62df\u548c\u5177\u5907\u91cd\u7f6e\u529f\u80fd\u7684\u673a\u5668\u4eba\u8fdb\u884c\u673a\u5668\u4eba\u7ec4\u88c5\u7684\u751f\u6210\u5f0f\u8bbe\u8ba1 \u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u751f\u6210\u6587\u672c\u3001\u4ee3\u7801\u548c\u56fe\u50cf\u65b9\u9762\u5c55\u73b0\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u80fd\u529b\u3002\u53d7\u5230\u5de5\u4e1a\u9886\u57df\u201c\u88c5\u914d\u8bbe\u8ba1\u201d\u7814\u7a76\u4e30\u5bcc\u5386\u53f2\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u65b0\u95ee\u9898\uff1a\u673a\u5668\u4eba\u88c5\u914d\u751f\u6210\u8bbe\u8ba1\uff08Generative Design-for-Robot-Assembly, GDfRA\uff09\u3002\u8be5\u4efb\u52a1\u8981\u6c42\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff08\u4f8b\u5982\u201c\u957f\u9888\u9e7f\u201d\uff09\u548c\u53ef\u7528\u7269\u7406\u7ec4\u4ef6\u7684\u56fe\u50cf\uff08\u59823D\u6253\u5370\u7684\u79ef\u6728\u5757\uff09\u751f\u6210\u4e00\u4e2a\u88c5\u914d\u4f53\u3002\u8f93\u51fa\u7ed3\u679c\u5305\u62ec\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u7a7a\u95f4\u6392\u5217\u4ee5\u53ca\u673a\u5668\u4eba\u6784\u5efa\u8be5\u88c5\u914d\u4f53\u7684\u6307\u4ee4\u3002\u8f93\u51fa\u5fc5\u987b\u6ee1\u8db3\u4e24\u4e2a\u6761\u4ef6\uff1a1) \u4e0e\u8bf7\u6c42\u7684\u5bf9\u8c61\u76f8\u4f3c\uff1b2) \u80fd\u591f\u88ab\u5e26\u6709\u5438\u76d8\u5939\u5177\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u81c2\u53ef\u9760\u5730\u88c5\u914d\u3002\u63a5\u7740\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Blox-Net\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u751f\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u4eff\u771f\u3001\u6270\u52a8\u5206\u6790\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u7269\u7406\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u6210\u719f\u65b9\u6cd5\u7684GDfRA\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6700\u5c0f\u4eba\u7c7b\u76d1\u7763\u4e0b\u89e3\u51b3\u4e00\u7c7bGDfRA\u95ee\u9898\u3002Blox-Net\u5728\u5176\u8bbe\u8ba1\u7684\u88c5\u914d\u4f53\u7684\u201c\u53ef\u8bc6\u522b\u6027\u201d\u65b9\u9762\u8fbe\u5230\u4e8663.5%\u7684Top-1\u51c6\u786e\u7387\uff08\u4f8b\u5982\uff0c\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u5176\u8bbe\u8ba1\u662f\u5426\u50cf\u957f\u9888\u9e7f\uff09\u3002\u8fd9\u4e9b\u8bbe\u8ba1\u5728\u7ecf\u8fc7\u81ea\u52a8\u6270\u52a8\u91cd\u65b0\u8bbe\u8ba1\u540e\uff0c\u80fd\u591f\u88ab\u673a\u5668\u4eba\u53ef\u9760\u5730\u88c5\u914d\uff0c\u572810\u6b21\u8fde\u7eed\u88c5\u914d\u8fed\u4ee3\u4e2d\u51e0\u4e4e\u5b8c\u7f8e\u6210\u529f\uff0c\u4ec5\u5728\u88c5\u914d\u524d\u91cd\u7f6e\u65f6\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u4ece\u6587\u672c\u8bcd\uff08\u201c\u957f\u9888\u9e7f\u201d\uff09\u5230\u53ef\u9760\u7684\u7269\u7406\u88c5\u914d\u7684\u6574\u4e2a\u8bbe\u8ba1\u8fc7\u7a0b\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002 Andrew Goldberg PDF N/A Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset Generative AI systems have shown impressive capabilities in creating text, code, and images. Inspired by the rich history of research in industrial ''Design for Assembly'', we introduce a novel problem: Generative Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on a natural language prompt (e.g., ''giraffe'') and an image of available physical components, such as 3D-printed blocks. The output is an assembly, a spatial arrangement of these components, and instructions for a robot to build this assembly. The output must 1) resemble the requested object and 2) be reliably assembled by a 6 DoF robot arm with a suction gripper. We then present Blox-Net, a GDfRA system that combines generative vision language models with well-established methods in computer vision, simulation, perturbation analysis, motion planning, and physical robot experimentation to solve a class of GDfRA problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of 63.5% in the ''recognizability'' of its designed assemblies (eg, resembling giraffe as judged by a VLM). These designs, after automated perturbation redesign, were reliably assembled by a robot, achieving near-perfect success across 10 consecutive assembly iterations with human intervention only during reset prior to assembly. Surprisingly, this entire design process from textual word (''giraffe'') to reliable physical assembly is performed with zero human intervention. \u6df1\u5ea6\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\uff0c\u63a8\u52a8\u5927\u6570\u636e\u5206\u6790\u4e0e\u7ba1\u7406\uff1a\u5b9e\u7528\u5165\u95e8 \u672c\u4e66\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u3001\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u5927\u6570\u636e\u5206\u6790\u548c\u7ba1\u7406\u4e2d\u7684\u63a8\u52a8\u4f5c\u7528\u3002\u672c\u4e66\u4e13\u6ce8\u4e8e\u7b80\u5316\u6df1\u5ea6\u5b66\u4e60\u80cc\u540e\u7684\u590d\u6742\u6570\u5b66\u6982\u5ff5\uff0c\u901a\u8fc7\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u5c55\u793a\u548c\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\uff0c\u5e2e\u52a9\u8bfb\u8005\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u4ee5\u53ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u7b49\u6280\u672f\u7684\u5de5\u4f5c\u539f\u7406\u3002\u4e66\u4e2d\u4ecb\u7ecd\u4e86Transformer\u3001GPT\u3001ResNet\u3001BERT\u548cYOLO\u7b49\u591a\u4e2a\u7ecf\u5178\u6a21\u578b\u548c\u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u56fe\u50cf\u8bc6\u522b\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u672c\u4e66\u8fd8\u5f3a\u8c03\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u5404\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u5e94\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u6307\u5bfc\u3002\u540c\u65f6\uff0c\u672c\u4e66\u6982\u8ff0\u4e86SQL\u548cNoSQL\u6570\u636e\u5e93\u7b49\u5173\u952e\u5927\u6570\u636e\u7ba1\u7406\u6280\u672f\uff0c\u4ee5\u53caApache Hadoop\u548cSpark\u7b49\u5206\u5e03\u5f0f\u8ba1\u7b97\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u5b83\u4eec\u5728\u7ba1\u7406\u548c\u5904\u7406\u6d77\u91cf\u6570\u636e\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6700\u7ec8\uff0c\u672c\u4e66\u5f3a\u8c03\u4e86\u638c\u63e1\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u6570\u636e\u7ba1\u7406\u6280\u80fd\u4f5c\u4e3a\u672a\u6765\u52b3\u52a8\u529b\u5173\u952e\u5de5\u5177\u7684\u4ef7\u503c\uff0c\u4f7f\u5176\u6210\u4e3a\u521d\u5b66\u8005\u548c\u6709\u7ecf\u9a8c\u4e13\u4e1a\u4eba\u58eb\u7684\u5fc5\u5907\u8d44\u6e90\u3002 Benji Peng PDF N/A Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer This book explores the role of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in driving the progress of big data analytics and management. The book focuses on simplifying the complex mathematical concepts behind deep learning, offering intuitive visualizations and practical case studies to help readers understand how neural networks and technologies like Convolutional Neural Networks (CNNs) work. It introduces several classic models and technologies such as Transformers, GPT, ResNet, BERT, and YOLO, highlighting their applications in fields like natural language processing, image recognition, and autonomous driving. The book also emphasizes the importance of pre-trained models and how they can enhance model performance and accuracy, with instructions on how to apply these models in various real-world scenarios. Additionally, it provides an overview of key big data management technologies like SQL and NoSQL databases, as well as distributed computing frameworks such as Apache Hadoop and Spark, explaining their importance in managing and processing vast amounts of data. Ultimately, the book underscores the value of mastering deep learning and big data management skills as critical tools for the future workforce, making it an essential resource for both beginners and experienced professionals. \u7f16\u7a0b\u6bcf\u4e2a\u793a\u4f8b\uff1a\u5927\u89c4\u6a21\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u5982\u540c\u4e13\u5bb6\u822c\u7cbe\u51c6 \u4f20\u7edf\u4e0a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u63d0\u5347\u8bed\u6599\u5e93\u8d28\u91cf\uff0c\u7531\u6b64\u4ea7\u751f\u4e86\u4f17\u591a\u89c4\u5219\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89c4\u5219\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u6bcf\u4e2a\u793a\u4f8b\u7684\u72ec\u7279\u7279\u5f81\u3002\u540c\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u793a\u4f8b\u5e94\u7528\u5b9a\u5236\u89c4\u5219\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u6765\u8bf4\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u672c\u6587\u5c55\u793a\uff0c\u5373\u4f7f\u662f\u4ec5\u67090.3\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e5f\u80fd\u5c55\u73b0\u51fa\u53ef\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5ab2\u7f8e\u7684\u663e\u8457\u6570\u636e\u7cbe\u70bc\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u7f16\u7a0b\u6bcf\u4e2a\u793a\u4f8b\u201d\uff08ProX\uff09\u8fd9\u4e00\u521b\u65b0\u6846\u67b6\uff0c\u5c06\u6570\u636e\u7cbe\u70bc\u89c6\u4e3a\u7f16\u7a0b\u4efb\u52a1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u751f\u6210\u548c\u6267\u884c\u7ec6\u7c92\u5ea6\u64cd\u4f5c\uff08\u5982\u5b57\u7b26\u4e32\u5f52\u4e00\u5316\uff09\u6765\u5927\u89c4\u6a21\u7cbe\u70bc\u6bcf\u4e2a\u5355\u72ec\u793a\u4f8b\u7684\u8bed\u6599\u5e93\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728ProX\u7cbe\u70bc\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5404\u79cd\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176\u8868\u73b0\u5747\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u6216\u5176\u4ed6\u7b5b\u9009\u65b9\u6cd5\u5904\u7406\u7684\u6570\u636e\uff0c\u63d0\u5347\u5e45\u5ea6\u8d85\u8fc72%\u3002ProX\u7684\u6709\u6548\u6027\u6db5\u76d6\u4e86\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5305\u62ecC4\u3001RedPajama-V2\u548cFineWeb\u3002\u6b64\u5916\uff0cProX\u5728\u7279\u5b9a\u9886\u57df\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff1a\u5728\u6ca1\u6709\u7279\u5b9a\u9886\u57df\u8bbe\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0cProX\u7cbe\u70bc\u7684OpenWebMath\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u57fa\u4e8e\u4eba\u7c7b\u624b\u5de5\u89c4\u5219\u7684\u65b9\u6cd5\uff0cMistral-7B\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6%\uff0cLlama-2-7B\u63d0\u9ad8\u4e8614.6%\uff0cCodeLlama-7B\u63d0\u9ad8\u4e8620.3%\uff0c\u8fd9\u4e9b\u63d0\u5347\u5747\u5728100\u4ebf\u6807\u8bb0\u5185\uff0c\u53ef\u4e0e\u57282000\u4ebf\u6807\u8bb0\u4e0a\u8bad\u7ec3\u7684Llemma-7B\u7b49\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0cProX\u663e\u8457\u8282\u7701\u4e86\u8bad\u7ec3\u7684\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\uff08FLOPs\uff09\uff0c\u4e3a\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002\u6211\u4eec\u6b63\u5728\u5f00\u6e90ProX\uff0c\u5305\u62ec\u8d85\u8fc71000\u4ebf\u6807\u8bb0\u7684\u8bed\u6599\u5e93\u3001\u6a21\u578b\uff0c\u5e76\u5206\u4eab\u6240\u6709\u8bad\u7ec3\u548c\u5b9e\u73b0\u7ec6\u8282\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u672a\u6765\u521b\u65b0\u3002\u4ee3\u7801\u94fe\u63a5\uff1ahttps://github.com/GAIR-NLP/ProX Fan Zhou PDF N/A Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training.We are open-sourcing ProX with &gt;100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX \u63cf\u8ff0LLMs\u6b8b\u5dee\u6d41\u4e2d\u7684\u7a33\u5b9a\u533a\u57df \u6211\u4eec\u5728Transformer\u7684\u6b8b\u5dee\u6d41\u4e2d\u8bc6\u522b\u51fa\u201c\u7a33\u5b9a\u533a\u57df\u201d\uff0c\u5728\u8fd9\u4e9b\u533a\u57df\u5185\uff0c\u6a21\u578b\u7684\u8f93\u51fa\u5bf9\u5c0f\u7684\u6fc0\u6d3b\u53d8\u5316\u4e0d\u654f\u611f\uff0c\u4f46\u5728\u533a\u57df\u8fb9\u754c\u5904\u8868\u73b0\u51fa\u9ad8\u654f\u611f\u6027\u3002\u8fd9\u4e9b\u533a\u57df\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\uff0c\u5e76\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u6216\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u800c\u53d8\u5f97\u66f4\u52a0\u660e\u786e\u3002\u8fd9\u4e9b\u533a\u57df\u4f3c\u4e4e\u6bd4\u5148\u524d\u7814\u7a76\u7684\u51f8\u591a\u9762\u4f53\u5927\u5f97\u591a\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u8fd9\u4e9b\u7a33\u5b9a\u533a\u57df\u4e0e\u8bed\u4e49\u533a\u5206\u76f8\u543b\u5408\uff0c\u5176\u4e2d\u76f8\u4f3c\u7684\u63d0\u793a\u5728\u533a\u57df\u5185\u805a\u96c6\uff0c\u800c\u6765\u81ea\u540c\u4e00\u533a\u57df\u7684\u6fc0\u6d3b\u4f1a\u5bfc\u81f4\u76f8\u4f3c\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u3002 Jett Janiak PDF N/A Characterizing stable regions in the residual stream of LLMs We identify \"stable regions\" in the residual stream of Transformers, where the model's output remains insensitive to small activation changes, but exhibits high sensitivity at region boundaries. These regions emerge during training and become more defined as training progresses or model size increases. The regions appear to be much larger than previously studied polytopes. Our analysis suggests that these stable regions align with semantic distinctions, where similar prompts cluster within regions, and activations from the same region lead to similar next token predictions. \u975e\u6e10\u8fd1\u6536\u655b\u6027\u5206\u6790\u7684\u968f\u673a\u68af\u5ea6\u54c8\u5bc6\u987f\u8499\u7279\u5361\u7f57\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u5177\u6709\u4e0d\u8fde\u7eed\u968f\u673a\u68af\u5ea6\u7684ReLU\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3 \u672c\u6587\u5bf9\u968f\u673a\u68af\u5ea6\u54c8\u5bc6\u987f\u8499\u7279\u5361\u7f57\uff08SGHMC\uff09\u7b97\u6cd5\u5728Wasserstein-1\u548cWasserstein-2\u8ddd\u79bb\u4e0b\u5411\u76ee\u6807\u6d4b\u5ea6\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u975e\u6e10\u8fd1\u5206\u6790\u3002\u4e0e\u73b0\u6709\u5173\u4e8eSGHMC\u7684\u6587\u732e\u76f8\u6bd4\uff0c\u6211\u4eec\u5141\u8bb8\u5176\u968f\u673a\u68af\u5ea6\u5177\u6709\u4e0d\u8fde\u7eed\u6027\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u4e3a\u5177\u6709\u4e0d\u8fde\u7eed\u968f\u673a\u68af\u5ea6\u7684\u975e\u51f8\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u9884\u671f\u8d85\u989d\u98ce\u9669\u63d0\u4f9b\u663e\u5f0f\u4e0a\u754c\uff0c\u8fd9\u4e9b\u4e0a\u754c\u53ef\u4ee5\u88ab\u63a7\u5236\u5f97\u4efb\u610f\u5c0f\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u4f7f\u7528ReLU\u6fc0\u6d3b\u51fd\u6570\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002\u4e3a\u4e86\u8bf4\u660e\u6211\u4eec\u4e3b\u8981\u7ed3\u679c\u7684\u9002\u7528\u6027\uff0c\u6211\u4eec\u8003\u8651\u4e86\u5206\u4f4d\u6570\u4f30\u8ba1\u548c\u6d89\u53ca\u91d1\u878d\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u76f8\u5173\u7684ReLU\u795e\u7ecf\u7f51\u7edc\u7684\u51e0\u4e2a\u4f18\u5316\u95ee\u9898\u7684\u6570\u503c\u5b9e\u9a8c\u3002 Luxu Liang PDF N/A Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks In this paper, we provide a non-asymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the existing literature on SGHMC, we allow its stochastic gradient to be discontinuous. This allows us to provide explicit upper bounds, which can be controlled to be arbitrarily small, for the expected excess risk of non-convex stochastic optimization problems with discontinuous stochastic gradients, including, among others, the training of neural networks with ReLU activation function. To illustrate the applicability of our main results, we consider numerical experiments on quantile estimation and on several optimization problems involving ReLU neural networks relevant in finance and artificial intelligence. \u7d2f\u52a0\u5668\u611f\u77e5\u7684\u540e\u8bad\u7ec3\u91cf\u5316 \u6700\u8fd1\u7684\u51e0\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u4f4e\u7cbe\u5ea6\u7d2f\u52a0\uff0c\u62a5\u544a\u4e86\u5728\u5404\u79cd\u5e73\u53f0\u4e0a\u541e\u5410\u91cf\u3001\u529f\u8017\u548c\u9762\u79ef\u7684\u6539\u8fdb\u3002\u7136\u800c\uff0c\u4f34\u968f\u7684\u63d0\u6848\u4ec5\u8003\u8651\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u8303\u5f0f\uff0c\u5176\u4e2d\u6a21\u578b\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5fae\u8c03\u6216\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0cQAT\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u6602\u8d35\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u8fd1\u671f\u5bf9\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u7814\u7a76\u7684\u6fc0\u589e\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u7684\u7814\u7a76\u6807\u5fd7\u7740\u9996\u6b21\u5728PTQ\u8bbe\u7f6e\u4e0b\u5bf9\u7d2f\u52a0\u5668\u611f\u77e5\u91cf\u5316\u8fdb\u884c\u6b63\u5f0f\u7814\u7a76\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u5f15\u5165\u4e86AXE\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u7d2f\u52a0\u5668\u611f\u77e5\u6269\u5c55\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u73b0\u6709\u7684\u9010\u5c42PTQ\u7b97\u6cd5\u8d4b\u4e88\u6ea2\u51fa\u907f\u514d\u4fdd\u8bc1\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u63a8\u52a8AXE\u7684\u53d1\u5c55\uff0c\u5e76\u901a\u8fc7\u5728\u4e24\u79cd\u6700\u5148\u8fdb\u7684PTQ\u7b97\u6cd5\uff08GPFQ\u548cOPTQ\uff09\u4e4b\u4e0a\u5b9e\u73b0\u5b83\u6765\u5c55\u793a\u5176\u7075\u6d3b\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06AXE\u63a8\u5e7f\u4ee5\u9996\u6b21\u652f\u6301\u591a\u9636\u6bb5\u7d2f\u52a0\uff0c\u4e3a\u5168\u9762\u6570\u636e\u8def\u5f84\u4f18\u5316\u548c\u6269\u5c55\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6253\u5f00\u4e86\u5927\u95e8\u3002\u6211\u4eec\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u751f\u6210\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86AXE\uff0c\u5e76\u89c2\u5bdf\u5230\u5728\u7d2f\u52a0\u5668\u4f4d\u5bbd\u4e0e\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u65b9\u9762\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002 Ian Colbert PDF N/A Accumulator-Aware Post-Training Quantization Several recent studies have investigated low-precision accumulation, reporting improvements in throughput, power, and area across various platforms. However, the accompanying proposals have only considered the quantization-aware training (QAT) paradigm, in which models are fine-tuned or trained from scratch with quantization in the loop. As models continue to grow in size, QAT techniques become increasingly more expensive, which has motivated the recent surge in post-training quantization (PTQ) research. To the best of our knowledge, ours marks the first formal study of accumulator-aware quantization in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework of accumulator-aware extensions designed to endow overflow avoidance guarantees to existing layer-wise PTQ algorithms. We theoretically motivate AXE and demonstrate its flexibility by implementing it on top of two state-of-the-art PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage accumulation for the first time, opening the door for full datapath optimization and scaling to large language models (LLMs). We evaluate AXE across image classification and language generation models, and observe significant improvements in the trade-off between accumulator bit width and model accuracy over baseline methods. Ctrl-GenAug\uff1a\u9762\u5411\u533b\u5b66\u5e8f\u5217\u5206\u7c7b\u7684\u53ef\u63a7\u751f\u6210\u589e\u5f3a \u5728\u533b\u7597\u9886\u57df\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u548c\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u6807\u6ce8\u8fc7\u7a0b\u9650\u5236\u4e86\u6df1\u5ea6\u6a21\u578b\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u589e\u5f3a\u65b9\u6cd5\u4e3a\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u88ab\u8bc1\u660e\u5728\u63a8\u8fdb\u4e0b\u6e38\u533b\u7597\u8bc6\u522b\u4efb\u52a1\u4e2d\u6709\u6548\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u9891/3D\u5e8f\u5217\u751f\u6210\u65f6\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u8bed\u4e49\u548c\u987a\u5e8f\u53ef\u63a7\u6027\uff0c\u5e76\u4e14\u5ffd\u89c6\u4e86\u5bf9\u5408\u6210\u6837\u672c\u566a\u58f0\u8d28\u91cf\u7684\u63a7\u5236\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u5e93\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5728\u6b64\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Ctrl-GenAug\uff0c\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u751f\u6210\u589e\u5f3a\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u5ea6\u8bed\u4e49\u548c\u987a\u5e8f\u5b9a\u5236\u7684\u5e8f\u5217\u5408\u6210\uff0c\u5e76\u6291\u5236\u9519\u8bef\u5408\u6210\u7684\u6837\u672c\uff0c\u4ee5\u8f85\u52a9\u533b\u7597\u5e8f\u5217\u5206\u7c7b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6761\u4ef6\u5f15\u5bfc\u7684\u5e8f\u5217\u751f\u6210\u5668\uff0c\u7528\u4e8e\u53ef\u63a7\u5730\u5408\u6210\u4fc3\u8fdb\u8bca\u65ad\u7684\u6837\u672c\u3002\u63a5\u7740\uff0c\u6211\u4eec\u96c6\u6210\u4e00\u4e2a\u987a\u5e8f\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u6837\u672c\u7684\u65f6\u95f4/\u7acb\u4f53\u4e00\u81f4\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u566a\u58f0\u5408\u6210\u6570\u636e\u8fc7\u6ee4\u5668\uff0c\u4ee5\u5728\u8bed\u4e49\u548c\u987a\u5e8f\u5c42\u9762\u4e0a\u6291\u5236\u4e0d\u53ef\u9760\u7684\u6848\u4f8b\u3002\u5728\u4e09\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e09\u79cd\u8303\u5f0f\u8bad\u7ec3\u768411\u4e2a\u7f51\u7edc\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5168\u9762\u5206\u6790\u4e86Ctrl-GenAug\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9ad8\u98ce\u9669\u4eba\u7fa4\u548c\u57df\u5916\u6761\u4ef6\u4e0b\u3002 Xinrui Zhou PDF N/A Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions. \u901a\u8fc7\u5feb\u901f\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u5c40\u90e8\u6b63\u5219\u5316\u7684\u7a00\u758f\u56fe \u901a\u8fc7\u7a00\u758f\u8868\u793a\u6784\u5efa\u7684\u7a00\u758f\u56fe\u5df2\u88ab\u8bc1\u660e\u5728\u805a\u7c7b\u9ad8\u7ef4\u6570\u636e\u65b9\u9762\u662f\u6709\u6548\u7684\u3002\u5c3d\u7ba1\u5176\u5177\u6709\u4ee4\u4eba\u4fe1\u670d\u7684\u5b9e\u8bc1\u8868\u73b0\uff0c\u4f46\u4f20\u7edf\u7684\u7a00\u758f\u56fe\u5728\u5355\u72ec\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u8fdb\u884c\u7a00\u758f\u8868\u793a\u65f6\u5ffd\u7565\u4e86\u6570\u636e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u83b7\u5f97\u4e0e\u6570\u636e\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u76f8\u4e00\u81f4\u7684\u7a00\u758f\u56fe\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u652f\u6301\u6b63\u5219\u5316\u7a00\u758f\u56fe\uff08Support Regularized Sparse Graph\uff0c\u7b80\u79f0SRSG\uff09\u7528\u4e8e\u6570\u636e\u805a\u7c7b\u3002SRSG\u901a\u8fc7\u4e00\u4e2a\u5b9a\u4e49\u826f\u597d\u7684\u652f\u6301\u6b63\u5219\u5316\u9879\uff0c\u9f13\u52b1\u76f8\u90bb\u6570\u636e\u70b9\u90bb\u57df\u7684\u5c40\u90e8\u5e73\u6ed1\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u6765\u89e3\u51b3SRSG\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5176\u6536\u655b\u901f\u5ea6\u4e0eNesterov\u5728\u5149\u6ed1\u4e14\u5177\u6709Lipschitz\u8fde\u7eed\u68af\u5ea6\u7684\u51f8\u76ee\u6807\u51fd\u6570\u4e0a\u7684\u6700\u4f18\u4e00\u9636\u65b9\u6cd5\u6536\u655b\u7387\u76f8\u5339\u914d\u3002\u5728\u5404\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSRSG\u4f18\u4e8e\u5176\u4ed6\u7ade\u4e89\u6027\u805a\u7c7b\u65b9\u6cd5\u3002 Dongfang Sun PDF N/A Locally Regularized Sparse Graph by Fast Proximal Gradient Descent Sparse graphs built by sparse representation has been demonstrated to be effective in clustering high-dimensional data. Albeit the compelling empirical performance, the vanilla sparse graph ignores the geometric information of the data by performing sparse representation for each datum separately. In order to obtain a sparse graph aligned with the local geometric structure of data, we propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data clustering. SRSG encourages local smoothness on the neighborhoods of nearby data points by a well-defined support regularization term. We propose a fast proximal gradient descent method to solve the non-convex optimization problem of SRSG with the convergence matching the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Extensive experimental results on various real data sets demonstrate the superiority of SRSG over other competing clustering methods. SEN12-WATER\uff1a\u4e00\u4e2a\u65b0\u7684\u6c34\u6587\u5e94\u7528\u6570\u636e\u96c6\u53ca\u5176\u57fa\u51c6\u6d4b\u8bd5 \u6c14\u5019\u53d8\u5316\u548c\u65e5\u76ca\u52a0\u5267\u7684\u5e72\u65f1\u7ed9\u5168\u7403\u6c34\u8d44\u6e90\u7ba1\u7406\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u4e25\u91cd\u7684\u6c34\u8d44\u6e90\u77ed\u7f3a\uff0c\u5a01\u80c1\u751f\u6001\u7cfb\u7edf\u3001\u519c\u4e1a\u548c\u4eba\u7c7b\u793e\u533a\u3002\u4e3a\u4e86\u63a8\u8fdb\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u6597\u4e89\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\uff0cSEN12-WATER\uff0c\u5e76\u7ed3\u5408\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4e3b\u52a8\u5e72\u65f1\u76f8\u5173\u5206\u6790\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u6570\u636e\u96c6\u88ab\u8bc6\u522b\u4e3a\u65f6\u7a7a\u6570\u636e\u7acb\u65b9\u4f53\uff0c\u6574\u5408\u4e86SAR\u6781\u5316\u3001\u9ad8\u7a0b\u3001\u5761\u5ea6\u548c\u591a\u5149\u8c31\u5149\u5b66\u6ce2\u6bb5\u3002\u6211\u4eec\u7684DL\u6846\u67b6\u80fd\u591f\u5206\u6790\u548c\u4f30\u7b97\u611f\u5174\u8da3\u6c34\u5e93\u4e2d\u7684\u6c34\u8d44\u6e90\u635f\u5931\u968f\u65f6\u95f4\u7684\u53d8\u5316\uff0c\u901a\u8fc7\u68c0\u67e5\u6c34\u4f53\u79ef\u7b49\u7269\u7406\u91cf\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u5e72\u65f1\u5206\u6790\u4e2d\u6c34\u52a8\u6001\u7684\u91cd\u8981\u89c1\u89e3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u6240\u63d0\u51fa\u6570\u636e\u96c6\u7684\u591a\u65f6\u95f4\u548c\u591a\u6a21\u6001\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63a8\u8fdb\u4e86\u5bf9\u5e72\u65f1\u7684\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u6c14\u5019\u53d8\u5316\u9002\u5e94\u6027\u548c\u53ef\u6301\u7eed\u6c34\u8d44\u6e90\u7ba1\u7406\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5305\u62ec\u591a\u4e2a\u7ec4\u4ef6\uff0c\u5982\u4eceSAR\u6570\u636e\u4e2d\u53bb\u9664\u6591\u70b9\u566a\u58f0\u3001\u901a\u8fc7U-Net\u67b6\u6784\u8fdb\u884c\u6c34\u4f53\u5206\u5272\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4ee5\u53ca\u65f6\u95f4\u5206\u5e03\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08TD-CNN\uff09\u7684\u9884\u6d4b\u80fd\u529b\u3002\u7ed3\u679c\u901a\u8fc7\u5730\u9762\u5b9e\u51b5\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u6570\u636e\u901a\u8fc7\u4e13\u7528\u4f20\u611f\u5668\u5728\u5730\u9762\u4e0a\u83b7\u53d6\uff0c\u5e76\u4f7f\u7528\uff08\u5b9a\u5236\u7684\uff09\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5982\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001\u4ea4\u5e76\u6bd4\u3001\u5747\u65b9\u8bef\u5dee\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u548c\u5cf0\u503c\u4fe1\u566a\u6bd4\u3002 Luigi Russo PDF N/A SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking Climate change and increasing droughts pose significant challenges to water resource management around the world. These problems lead to severe water shortages that threaten ecosystems, agriculture, and human communities. To advance the fight against these challenges, we present a new dataset, SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL) framework for proactive drought-related analysis. The dataset, identified as a spatiotemporal datacube, integrates SAR polarization, elevation, slope, and multispectral optical bands. Our DL framework enables the analysis and estimation of water losses over time in reservoirs of interest, revealing significant insights into water dynamics for drought analysis by examining temporal changes in physical quantities such as water volume. Our methodology takes advantage of the multitemporal and multimodal characteristics of the proposed dataset, enabling robust generalization and advancing understanding of drought, contributing to climate change resilience and sustainable water resource management. The proposed framework involves, among the several components, speckle noise removal from SAR data, a water body segmentation through a U-Net architecture, the time series analysis, and the predictive capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results are validated through ground truth data acquired on-ground via dedicated sensors and (tailored) metrics, such as Precision, Recall, Intersection over Union, Mean Squared Error, Structural Similarity Index Measure and Peak Signal-to-Noise Ratio. \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u4ece\u6a21\u7cca\u7a7a\u95f4\u63a8\u7406\u7684\u89c6\u89c9\u6f14\u793a\u4e2d\u5b66\u4e60\uff1f \u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5df2\u6210\u4e3a\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u5176\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u662f\u4e00\u79cd\u6d41\u884c\u7684\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u7b56\u7565\u3002\u4f46\u662f\uff0cVLMs\u80fd\u5426\u4ec5\u4ece\u89c6\u89c9\u6f14\u793a\u4e2d\u5b66\u4e60\u65b0\u6982\u5ff5\uff0c\u8fd8\u662f\u5b83\u4eec\u4ec5\u9650\u4e8e\u9002\u5e94ICL\u793a\u4f8b\u7684\u8f93\u51fa\u683c\u5f0f\uff1f\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u7a7a\u95f4\u89c6\u89c9\u6b67\u4e49\u4efb\u52a1\uff08SVAT\uff09\uff0c\u5b83\u6311\u6218\u6700\u5148\u8fdb\u7684VLMs\u5728\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u65b0\u7684\u89c6\u89c9\u7a7a\u95f4\u4efb\u52a1\u3002\u6211\u4eec\u53d1\u73b0\uff0cVLMs\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u65e0\u6cd5\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\uff0c\u6709\u65f6\u5728\u5fae\u8c03\u540e\u4ecd\u7136\u5931\u8d25\u3002\u7136\u800c\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5411\u8bad\u7ec3\u4e2d\u6dfb\u52a0\u66f4\u7b80\u5355\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u63d0\u9ad8ICL\u7684\u8868\u73b0\u3002 Bowen Zhao PDF N/A Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning? Large vision-language models (VLMs) have become state-of-the-art for many computer vision tasks, with in-context learning (ICL) as a popular adaptation strategy for new ones. But can VLMs learn novel concepts purely from visual demonstrations, or are they limited to adapting to the output format of ICL examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks (SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks in-context. We find that VLMs fail to do this zero-shot, and sometimes continue to fail after finetuning. However, adding simpler data to the training by curriculum learning leads to improved ICL performance. \u5229\u7528Transformer\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u4ea4\u4e92\uff1a\u63d0\u5347\u6e38\u620f\u7528\u6237\u6d88\u8d39\u503e\u5411\u9884\u6d4b Dream11\u662f\u4e00\u4e2a\u68a6\u5e7b\u4f53\u80b2\u5e73\u53f0\uff0c\u5141\u8bb8\u7528\u6237\u4e3a\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u4f53\u80b2\u8d5b\u4e8b\u521b\u5efa\u81ea\u5df1\u7684\u865a\u62df\u56e2\u961f\u3002\u6211\u4eec\u4e3a\u8d85\u8fc72\u4ebf\u7684\u7528\u6237\u7fa4\u4f53\u4e3e\u529e\u591a\u79cd\u4f53\u80b2\u8d5b\u4e8b\u3002\u5728\u8fd9\u79cd\u771f\u5b9e\u8d27\u5e01\u6e38\u620f\uff08RMG\uff09\u73af\u5883\u4e2d\uff0c\u7528\u6237\u652f\u4ed8\u53c2\u4e0e\u8d39\u7528\u6765\u53c2\u52a0\u6211\u4eec\u4e3a\u7528\u6237\u63d0\u4f9b\u7684\u5404\u79cd\u7ade\u8d5b\u4ea7\u54c1\u3002\u5728\u6211\u4eec\u7684\u5f53\u524d\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u9884\u6d4b\u7528\u6237\u5728\u6e38\u620f\u56de\u5408\u4e2d\u7684\u6d88\u8d39\u503e\u5411\u7684\u95ee\u9898\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5c06\u5176\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\uff0c\u4f8b\u5982\u6839\u636e\u7528\u6237\u7684\u6d88\u8d39\u503e\u5411\u5fae\u8c03\u6fc0\u52b1\u63aa\u65bd\u6765\u63d0\u5347\u7528\u6237\u6d88\u8d39\uff0c\u6216\u6839\u636e\u7528\u6237\u7684\u6d88\u8d39\u503e\u5411\u4e2a\u6027\u5316\u4ea7\u54c1\u5217\u8868\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u57fa\u4e8e\u8fc7\u53bb\u7684\u4ea4\u6613\u6570\u636e\u4e3a\u6bcf\u4e2a\u7528\u6237\u5efa\u6a21\u6d88\u8d39\u503e\u5411\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u57fa\u51c6\u6d4b\u8bd5\u4e86\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u7684\u57fa\u4e8e\u6811\u7684\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u53d8\u5316\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6355\u6349\u8f93\u5165\u7279\u5f81\u4e4b\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u67b6\u6784\u5728\u9884\u6d4b\u7528\u6237\u5728\u6e38\u620f\u56de\u5408\u4e2d\u7684\u6d88\u8d39\u503e\u5411\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6211\u4eec\u7684\u65b0Transformer\u6a21\u578b\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684FT-Transformer\uff0c\u5c06MAE\u63d0\u9ad8\u4e862.5%\uff0cMSE\u63d0\u9ad8\u4e8621.8%\u3002 Ved Prakash PDF N/A Efficient Feature Interactions with Transformers: Improving User Spending Propensity Predictions in Gaming Dream11 is a fantasy sports platform that allows users to create their own virtual teams for real-life sports events. We host multiple sports and matches for our 200M+ user base. In this RMG (real money gaming) setting, users pay an entry amount to participate in various contest products that we provide to users. In our current work, we discuss the problem of predicting the user's propensity to spend in a gaming round, so it can be utilized for various downstream applications. e.g. Upselling users by incentivizing them marginally as per their spending propensity, or personalizing the product listing based on the user's propensity to spend.   We aim to model the spending propensity of each user based on past transaction data. In this paper, we benchmark tree-based and deep-learning models that show good results on structured data, and we propose a new architecture change that is specifically designed to capture the rich interactions among the input features. We show that our proposed architecture outperforms the existing models on the task of predicting the user's propensity to spend in a gaming round. Our new transformer model surpasses the state-of-the-art FT-Transformer, improving MAE by 2.5\\% and MSE by 21.8\\%. \u901a\u8fc7\u7c97\u7c92\u5ea6\u7b54\u6848\u5206\u89e3\u589e\u5f3a\u957f\u6587\u6863\u7406\u89e3\u4e2d\u7684\u4e8b\u540e\u5f52\u56e0 \u51c6\u786e\u5730\u5c06\u56de\u7b54\u6587\u672c\u5f52\u56e0\u4e8e\u5176\u6765\u6e90\u6587\u6863\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u7684\u95ee\u7b54\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u957f\u6587\u6863\u7684\u5f52\u56e0\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e8b\u540e\u5f52\u56e0\u7cfb\u7edf\u65e8\u5728\u5c06\u56de\u7b54\u6587\u672c\u6620\u5c04\u56de\u6e90\u6587\u6863\uff0c\u4f46\u8fd9\u79cd\u6620\u5c04\u7684\u7c92\u5ea6\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u89e3\u51b3\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u6d6e\u73b0\uff1a\u7a76\u7adf\u5e94\u8be5\u5f52\u56e0\u4ec0\u4e48\uff0c\u91cd\u70b9\u5728\u4e8e\u8bc6\u522b\u56de\u7b54\u4e2d\u9700\u8981\u4f9d\u636e\u7684\u4fe1\u606f\u5355\u5143\uff1f\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u56de\u7b54\u4e8b\u5b9e\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f52\u56e0\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u677f\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5229\u7528\u95ee\u9898\uff0c\u5e76\u5728\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7ed3\u5408\u8d1f\u91c7\u6837\u8fdb\u884c\u5206\u89e3\u3002\u8fd9\u79cd\u65b9\u6cd5\u589e\u5f3a\u4e86\u62bd\u8c61\u548c\u62bd\u53d6\u56de\u7b54\u7684\u8bed\u4e49\u7406\u89e3\u3002\u6211\u4eec\u901a\u8fc7\u5168\u9762\u8003\u5bdf\u5404\u79cd\u5f52\u56e0\u65b9\u6cd5\uff0c\u4ece\u57fa\u4e8e\u68c0\u7d22\u7684\u6280\u672f\u5230\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f52\u56e0\u5668\uff0c\u6765\u68c0\u9a8c\u56de\u7b54\u5206\u89e3\u7684\u5f71\u54cd\u3002 Pritika Ramu PDF N/A Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition Accurately attributing answer text to its source document is crucial for developing a reliable question-answering system. However, attribution for long documents remains largely unexplored. Post-hoc attribution systems are designed to map answer text back to the source document, yet the granularity of this mapping has not been addressed. Furthermore, a critical question arises: What precisely should be attributed, with an emphasis on identifying the information units within an answer that necessitate grounding? In this paper, we propose and investigate a novel approach to the factual decomposition of generated answers for attribution, employing template-based in-context learning. To accomplish this, we utilize the question and integrate negative sampling during few-shot in-context learning for decomposition. This approach enhances the semantic understanding of both abstractive and extractive answers. We examine the impact of answer decomposition by providing a thorough examination of various attribution approaches, ranging from retrieval-based techniques to LLM-based attributors. \u611f\u77e5\u5ea6\u91cf\u5bf9\u97f3\u4e50\u6d41\u6d3e\u5206\u7c7b\u4e2d\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd \u81ea\u7136\u4fe1\u53f7\u7684\u4e3b\u89c2\u8d28\u91cf\u53ef\u4ee5\u901a\u8fc7\u5ba2\u89c2\u7684\u611f\u77e5\u5ea6\u91cf\u6765\u8fd1\u4f3c\u3002\u8bbe\u8ba1\u7528\u6765\u8fd1\u4f3c\u4eba\u7c7b\u89c2\u5bdf\u8005\u7684\u611f\u77e5\u884c\u4e3a\uff0c\u611f\u77e5\u5ea6\u91cf\u901a\u5e38\u53cd\u6620\u4e86\u81ea\u7136\u4fe1\u53f7\u548c\u795e\u7ecf\u901a\u8def\u4e2d\u53d1\u73b0\u7684\u7ed3\u6784\u3002\u4f7f\u7528\u611f\u77e5\u5ea6\u91cf\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u7684\u6a21\u578b\u53ef\u4ee5\u6355\u6349\u5230\u8fd9\u4e9b\u5ea6\u91cf\u4e2d\u5305\u542b\u7684\u7ed3\u6784\u6240\u5177\u6709\u7684\u611f\u77e5\u4e0a\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002\u6211\u4eec\u5c55\u793a\u4e86\uff0c\u4f7f\u7528\u4ece\u7ecf\u8fc7\u611f\u77e5\u635f\u5931\u8bad\u7ec3\u7684\u81ea\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u53ef\u4ee5\u5728\u97f3\u4e50\u7406\u89e3\u4efb\u52a1\uff08\u5982\u98ce\u683c\u5206\u7c7b\uff09\u4e2d\u63d0\u9ad8\u6027\u80fd\uff0c\u76f8\u6bd4\u4e8e\u76f4\u63a5\u5c06\u8fd9\u4e9b\u5ea6\u91cf\u4f5c\u4e3a\u8ddd\u79bb\u6765\u8bad\u7ec3\u5206\u7c7b\u5668\u3002\u8fd9\u4e00\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u4f7f\u7528\u611f\u77e5\u5ea6\u91cf\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5bf9\u65b0\u4fe1\u53f7\u7684\u6cdb\u5316\u80fd\u529b\u6709\u6240\u63d0\u9ad8\u3002 Tashi Namgyal PDF N/A The Effect of Perceptual Metrics on Music Representation Learning for Genre Classification The subjective quality of natural signals can be approximated with objective perceptual metrics. Designed to approximate the perceptual behaviour of human observers, perceptual metrics often reflect structures found in natural signals and neurological pathways. Models trained with perceptual metrics as loss functions can capture perceptually meaningful features from the structures held within these metrics. We demonstrate that using features extracted from autoencoders trained with perceptual losses can improve performance on music understanding tasks, i.e. genre classification, over using these metrics directly as distances when learning a classifier. This result suggests improved generalisation to novel signals when using perceptual metrics as loss functions for representation learning. \u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u57fa\u51c6\u6d4b\u8bd5\u9886\u57df\u6cdb\u5316\u7b97\u6cd5 \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\uff08CPath\uff09\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5f53\u5e94\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u9886\u57df\u504f\u79fb\uff0c\u5176\u6027\u80fd\u5e38\u5e38\u53d7\u5230\u5f71\u54cd\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u9700\u8981\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u5728CPath\u80cc\u666f\u4e0b\u5bf9DG\u7b97\u6cd5\u7684\u7cfb\u7edf\u8bc4\u4f30\u5c1a\u7f3a\u4e4f\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc77,560\u6b21\u4ea4\u53c9\u9a8c\u8bc1\u8fd0\u884c\uff0c\u5bf930\u79cdDG\u7b97\u6cd5\u57283\u4e2a\u96be\u5ea6\u4e0d\u540c\u7684CPath\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u7edf\u4e00\u4e14\u7a33\u5065\u7684\u5e73\u53f0\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u7279\u5b9a\u6a21\u6001\u6280\u672f\u53ca\u5982\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7b49\u6700\u65b0\u8fdb\u5c55\u3002\u5e7f\u6cdb\u7684\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u5404\u79cdDG\u7b56\u7565\u76f8\u5bf9\u6027\u80fd\u7684\u6df1\u5165\u89c1\u89e3\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u67d3\u8272\u589e\u5f3a\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6cdb\u764c\u80bf\u7624\u68c0\u6d4b\u6570\u636e\u96c6\uff08HISTOPANTUM\uff09\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002\u672c\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u5728\u9009\u62e9\u9002\u7528\u4e8eCPath\u4efb\u52a1\u7684\u9002\u5f53DG\u65b9\u6cd5\u65f6\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6307\u5bfc\u3002 Neda Zamanitajeddin PDF N/A Benchmarking Domain Generalization Algorithms in Computational Pathology Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks. DRIM\uff1a\u4ece\u4e0d\u5b8c\u6574\u7684\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u4e2d\u5b66\u4e60\u89e3\u8026\u8868\u793a \u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u533b\u7597\u6570\u636e\u901a\u5e38\u662f\u591a\u6a21\u6001\u4e14\u4e0d\u5b8c\u6574\u7684\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u5bf9\u80fd\u591f\u9ad8\u6548\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u7684\u9ad8\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002\u5229\u7528\u5305\u62ec\u7ec4\u7ec7\u75c5\u7406\u5b66\u5207\u7247\u3001\u6838\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u548c\u57fa\u56e0\u6570\u636e\u5728\u5185\u7684\u591a\u79cd\u6a21\u6001\uff0c\u4e3a\u6539\u5584\u9884\u540e\u9884\u6d4b\u548c\u63ed\u793a\u65b0\u7684\u6cbb\u7597\u9014\u5f84\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u673a\u4f1a\u3002\u5bf9\u6bd4\u5b66\u4e60\u5e7f\u6cdb\u7528\u4e8e\u4ece\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u914d\u5bf9\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u793a\uff0c\u5b83\u5047\u8bbe\u4e0d\u540c\u7684\u89c6\u56fe\u5305\u542b\u76f8\u540c\u7684\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5e76\u4ec5\u5229\u7528\u5171\u4eab\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u533b\u7597\u6570\u636e\u65f6\uff0c\u8fd9\u79cd\u5047\u8bbe\u53d8\u5f97\u5177\u6709\u9650\u5236\u6027\uff0c\u56e0\u4e3a\u6bcf\u79cd\u6a21\u6001\u8fd8\u5305\u542b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u76f8\u5173\u7684\u7279\u5b9a\u77e5\u8bc6\u3002\u6211\u4eec\u5f15\u5165\u4e86DRIM\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u8fd9\u4e9b\u5171\u4eab\u548c\u72ec\u7279\u7684\u8868\u793a\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u7ec4\u6a21\u6001\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7f16\u7801\u6bcf\u4e2a\u6a21\u6001\u7684\u8868\u793a\uff0c\u8be5\u8868\u793a\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u4e00\u90e8\u5206\u5c01\u88c5\u4e86\u8de8\u6a21\u6001\u7684\u4e0e\u60a3\u8005\u76f8\u5173\u7684\u5171\u540c\u4fe1\u606f\uff0c\u53e6\u4e00\u90e8\u5206\u5c01\u88c5\u4e86\u6a21\u6001\u7279\u5b9a\u7684\u7ec6\u8282\u3002\u901a\u8fc7\u589e\u52a0\u4e0d\u540c\u60a3\u8005\u6a21\u6001\u4e4b\u95f4\u7684\u5171\u4eab\u4fe1\u606f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6bcf\u4e2a\u6a21\u6001\u5185\u5171\u4eab\u548c\u72ec\u7279\u7ec4\u4ef6\u4e4b\u95f4\u7684\u91cd\u53e0\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u80f6\u8d28\u7624\u60a3\u8005\u7684\u751f\u5b58\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u5bf9\u7f3a\u5931\u6a21\u6001\u5177\u6709\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\uff0c\u4ee3\u7801\u5df2\u5728https://github.com/Lucas-rbnt/DRIM\u516c\u5f00\u53d1\u5e03\u3002 Lucas Robinet PDF N/A DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data Real-life medical data is often multimodal and incomplete, fueling the growing need for advanced deep learning models capable of integrating them efficiently. The use of diverse modalities, including histopathology slides, MRI, and genetic data, offers unprecedented opportunities to improve prognosis prediction and to unveil new treatment pathways. Contrastive learning, widely used for deriving representations from paired data in multimodal tasks, assumes that different views contain the same task-relevant information and leverages only shared information. This assumption becomes restrictive when handling medical data since each modality also harbors specific knowledge relevant to downstream tasks. We introduce DRIM, a new multimodal method for capturing these shared and unique representations, despite data sparsity. More specifically, given a set of modalities, we aim to encode a representation for each one that can be divided into two components: one encapsulating patient-related information common across modalities and the other, encapsulating modality-specific details. This is achieved by increasing the shared information among different patient modalities while minimizing the overlap between shared and unique components within each modality. Our method outperforms state-of-the-art algorithms on glioma patients survival prediction tasks, while being robust to missing modalities. To promote reproducibility, the code is made publicly available at https://github.com/Lucas-rbnt/DRIM \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u5370\u5ea6\u5c3c\u897f\u4e9aePuskesmas\u4e2d\u533b\u60a3\u4e92\u52a8\u8fdb\u884c\u5b9e\u65f6\u8f6c\u5f55\u548c\u603b\u7ed3 \u5bfc\u81f4\u536b\u751f\u4e2d\u5fc3(Puskesmas)\u6548\u7387\u4f4e\u4e0b\u7684\u5173\u952e\u95ee\u9898\u4e4b\u4e00\u662f\u533b\u751f\u4e0e\u60a3\u8005\u4e92\u52a8\u8017\u65f6\u8f83\u957f\u3002\u533b\u751f\u9700\u8981\u8fdb\u884c\u5168\u9762\u7684\u54a8\u8be2\uff0c\u5305\u62ec\u8bca\u65ad\u60a3\u8005\u7684\u75c5\u60c5\u3001\u63d0\u4f9b\u6cbb\u7597\u5efa\u8bae\u4ee5\u53ca\u5c06\u8be6\u7ec6\u8bb0\u5f55\u8f6c\u5f55\u5230\u75c5\u5386\u4e2d\u3002\u5728\u8bed\u8a00\u80cc\u666f\u591a\u6837\u7684\u5730\u533a\uff0c\u533b\u751f\u901a\u5e38\u9700\u8981\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u5ef6\u957f\u4e86\u8fd9\u4e00\u8fc7\u7a0b\u3002\u867d\u7136\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8f6c\u5f55\u548c\u603b\u7ed3\u901a\u5e38\u53ef\u4ee5\u5229\u7528\u4eba\u5de5\u667a\u80fd\u81ea\u52a8\u5316\uff0c\u4ee5\u63d0\u9ad8\u65f6\u95f4\u6548\u7387\uff0c\u5e76\u5e2e\u52a9\u533b\u751f\u63d0\u9ad8\u62a4\u7406\u8d28\u91cf\uff0c\u5b9e\u73b0\u65e9\u671f\u8bca\u65ad\u548c\u5e72\u9884\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528\u672c\u5730\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6765\u8f6c\u5f55\u3001\u7ffb\u8bd1\u548c\u603b\u7ed3\u533b\u751f\u4e0e\u60a3\u8005\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002\u6211\u4eec\u5229\u7528Whisper\u6a21\u578b\u8fdb\u884c\u8f6c\u5f55\uff0c\u5e76\u4f7f\u7528GPT-3\u5c06\u5176\u603b\u7ed3\u4e3aePuskemas\u533b\u7597\u8bb0\u5f55\u683c\u5f0f\u3002\u8be5\u7cfb\u7edf\u4f5c\u4e3a\u73b0\u6709\u7f51\u9875\u6d4f\u89c8\u5668\u6269\u5c55\u7684\u9644\u52a0\u7ec4\u4ef6\u5b9e\u73b0\uff0c\u5141\u8bb8\u533b\u751f\u5728\u4ea4\u8c08\u65f6\u586b\u5199\u60a3\u8005\u8868\u683c\u3002\u901a\u8fc7\u5229\u7528\u8fd9\u4e00\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u5b9e\u65f6\u8f6c\u5f55\u3001\u7ffb\u8bd1\u548c\u603b\u7ed3\uff0c\u533b\u751f\u53ef\u4ee5\u7f29\u77ed\u60a3\u8005\u62a4\u7406\u7684\u5468\u8f6c\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u8bb0\u5f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5728\u672a\u6765\u5c31\u8bca\u65f6\u66f4\u52a0\u8be6\u7ec6\u548c\u6709\u6d1e\u5bdf\u529b\u3002\u8fd9\u4e00\u521b\u65b0\u89e3\u51b3\u4e86\u5370\u5c3c\u533b\u7597\u673a\u6784\u62e5\u6324\u548c\u884c\u653f\u8d1f\u62c5\u91cd\u7b49\u6311\u6218\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u8fd9\u4e00\u89e3\u51b3\u65b9\u6848\u5c06\u5e2e\u52a9\u533b\u751f\u8282\u7701\u65f6\u95f4\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u62a4\u7406\uff0c\u5e76\u751f\u6210\u66f4\u51c6\u786e\u7684\u533b\u7597\u8bb0\u5f55\uff0c\u6807\u5fd7\u7740\u5411\u73b0\u4ee3\u5316\u533b\u7597\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u786e\u4fdd\u60a3\u8005\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u4e5f\u80fd\u53ca\u65f6\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u62a4\u7406\u3002 Azmul Asmar Irfan PDF N/A Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia One of the key issues contributing to inefficiency in Puskesmas is the time-consuming nature of doctor-patient interactions. Doctors need to conduct thorough consultations, which include diagnosing the patient's condition, providing treatment advice, and transcribing detailed notes into medical records. In regions with diverse linguistic backgrounds, doctors often have to ask clarifying questions, further prolonging the process. While diagnosing is essential, transcription and summarization can often be automated using AI to improve time efficiency and help doctors enhance care quality and enable early diagnosis and intervention. This paper proposes a solution using a localized large language model (LLM) to transcribe, translate, and summarize doctor-patient conversations. We utilize the Whisper model for transcription and GPT-3 to summarize them into the ePuskemas medical records format. This system is implemented as an add-on to an existing web browser extension, allowing doctors to fill out patient forms while talking. By leveraging this solution for real-time transcription, translation, and summarization, doctors can improve the turnaround time for patient care while enhancing the quality of records, which become more detailed and insightful for future visits. This innovation addresses challenges like overcrowded facilities and the administrative burden on healthcare providers in Indonesia. We believe this solution will help doctors save time, provide better care, and produce more accurate medical records, representing a significant step toward modernizing healthcare and ensuring patients receive timely, high-quality care, even in resource-constrained settings. \u4f7f\u7528\u56feKoopman\u81ea\u7f16\u7801\u5668\u5bf9\u6297\u591a\u65e0\u4eba\u673a\u76d1\u89c6\u7684\u9884\u6d4b\u9690\u853d\u901a\u4fe1 \u4f4e\u68c0\u6d4b\u6982\u7387\uff08LPD\uff09\u901a\u4fe1\u65e8\u5728\u63a9\u76d6\u65e0\u7ebf\u7535\u9891\u7387\uff08RF\uff09\u4fe1\u53f7\u7684\u5b58\u5728\uff0c\u4ee5\u9003\u907f\u76d1\u63a7\u3002\u5728\u4f7f\u7528\u65e0\u4eba\u673a\uff08UAV\uff09\u8fdb\u884c\u79fb\u52a8\u76d1\u63a7\u7684\u80cc\u666f\u4e0b\uff0c\u7531\u4e8e\u65e0\u4eba\u673a\u7684\u5feb\u901f\u8fde\u7eed\u79fb\u52a8\u5177\u6709\u672a\u77e5\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5b9e\u73b0LPD\u901a\u4fe1\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u56e0\u6b64\uff0c\u51c6\u786e\u9884\u6d4b\u65e0\u4eba\u673a\u7684\u672a\u6765\u4f4d\u7f6e\u5bf9\u4e8e\u5b9e\u73b0\u5b9e\u65f6LPD\u901a\u4fe1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u9884\u6d4b\u9690\u853d\u901a\u4fe1\uff0c\u65e8\u5728\u5728\u591a\u65e0\u4eba\u673a\u76d1\u63a7\u4e0b\u7684\u5730\u9762\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u6700\u5c0f\u5316\u68c0\u6d4b\u6982\u7387\u3002\u6211\u4eec\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4e0eKoopman\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u4ee5\u5efa\u6a21\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5316\u52a8\u529b\u5b66\u5b9e\u73b0\u957f\u671f\u9884\u6d4b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u5386\u53f2\u6570\u636e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u3002\u5e7f\u6cdb\u7684\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u6211\u4eec\u7684\u65b9\u6cd5\u9884\u6d4b\u7684\u8f68\u8ff9\u5bfc\u81f4\u68c0\u6d4b\u6982\u7387\u81f3\u5c11\u964d\u4f4e\u4e8663%-75%\uff0c\u663e\u793a\u51fa\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9690\u853d\u64cd\u4f5c\u7684\u6f5c\u529b\u3002 Sivaram Krishnan PDF N/A Predictive Covert Communication Against Multi-UAV Surveillance Using Graph Koopman Autoencoder Low Probability of Detection (LPD) communication aims to obscure the presence of radio frequency (RF) signals to evade surveillance. In the context of mobile surveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD communication presents significant challenges due to the UAVs' rapid and continuous movements, which are characterized by unknown nonlinear dynamics. Therefore, accurately predicting future locations of UAVs is essential for enabling real-time LPD communication. In this paper, we introduce a novel framework termed predictive covert communication, aimed at minimizing detectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our data-driven method synergistically integrates graph neural networks (GNN) with Koopman theory to model the complex interactions within a multi-UAV network and facilitating long-term predictions by linearizing the dynamics, even with limited historical data. Extensive simulation results substantiate that the predicted trajectories using our method result in at least 63%-75% lower probability of detection when compared to well-known state-of-the-art baseline approaches, showing promise in enabling low-latency covert operations in practical scenarios. \u68c0\u6d4b\u95ee\u9898\u4e2d\u7684\u65f6\u95f4\u6a21\u7cca\u6027 \u68c0\u6d4b\u548c\u56de\u7b54\u6a21\u7cca\u95ee\u9898\u662f\u5f00\u653e\u9886\u57df\u95ee\u7b54\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u3002\u6a21\u7cca\u95ee\u9898\u6839\u636e\u5176\u89e3\u91ca\u7684\u4e0d\u540c\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u7b54\u6848\uff0c\u5e76\u4e14\u53ef\u4ee5\u91c7\u53d6\u591a\u79cd\u5f62\u5f0f\u3002\u65f6\u95f4\u6a21\u7cca\u95ee\u9898\u662f\u8fd9\u7c7b\u95ee\u9898\u4e2d\u6700\u5e38\u89c1\u7684\u4e00\u79cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86TEMPAMBIQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u65f6\u95f4\u6a21\u7cca\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ece\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u76848,162\u4e2a\u5f00\u653e\u9886\u57df\u95ee\u9898\u3002\u6211\u4eec\u7684\u6807\u6ce8\u91cd\u70b9\u5728\u4e8e\u6355\u6349\u65f6\u95f4\u6a21\u7cca\u6027\uff0c\u4ee5\u7814\u7a76\u68c0\u6d4b\u65f6\u95f4\u6a21\u7cca\u95ee\u9898\u7684\u4efb\u52a1\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u95ee\u9898\u6d88\u6b67\u7248\u672c\u7684\u591a\u6837\u5316\u641c\u7d22\u7b56\u7565\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u975e\u641c\u7d22\u7684\u7ade\u4e89\u6027\u57fa\u7ebf\uff0c\u5e76\u4f7f\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\u6d4b\u8bd5\u4e86\u68c0\u6d4b\u65f6\u95f4\u6a21\u7cca\u6027\u7684\u80fd\u529b\u3002 Bhawna Piryani PDF N/A Detecting Temporal Ambiguity in Questions Detecting and answering ambiguous questions has been a challenging task in open-domain question answering. Ambiguous questions have different answers depending on their interpretation and can take diverse forms. Temporally ambiguous questions are one of the most common types of such questions. In this paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA dataset consisting of 8,162 open-domain questions derived from existing datasets. Our annotations focus on capturing temporal ambiguity to study the task of detecting temporally ambiguous questions. We propose a novel approach by using diverse search strategies based on disambiguated versions of the questions. We also introduce and test non-search, competitive baselines for detecting temporal ambiguity using zero-shot and few-shot approaches. \u5982\u4f55\u5c06\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u8d77\u6765\uff1f\u54ea\u4e9b\u56e0\u7d20\u91cd\u8981\uff0c\u54ea\u4e9b\u4e0d\u91cd\u8981\uff1f \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u663e\u8457\u8868\u73b0\u63a8\u52a8\u4e86\u7814\u7a76\u5de5\u4f5c\uff0c\u4ee5\u5229\u7528\u5b83\u4eec\u5904\u7406\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u8f93\u5165\u6a21\u5f0f\u3002\u5728\u8bed\u97f3\u8f6c\u6587\u672c\uff08S2T\uff09\u4efb\u52a1\u4e2d\uff0c\u65b0\u5174\u7684\u89e3\u51b3\u65b9\u6848\u5305\u62ec\u901a\u8fc7\u9002\u914d\u5668\u6a21\u5757\u5c06\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFM\uff09\u7684\u7f16\u7801\u5668\u8f93\u51fa\u6295\u5f71\u5230LLM\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u65e0\u7814\u7a76\u63a2\u8ba8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6bcf\u4e2a\u7ec4\u4ef6\uff08SFM\u3001\u9002\u914d\u5668\u3001LLM\uff09\uff0c\u4e5f\u6ca1\u6709\u7814\u7a76\u6700\u4f73\u9002\u914d\u5668\u8bbe\u8ba1\u662f\u5426\u53d6\u51b3\u4e8e\u6240\u9009\u7684SFM\u548cLLM\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e865\u79cd\u9002\u914d\u5668\u6a21\u5757\u30012\u79cdLLM\uff08Mistral\u548cLlama\uff09\u4ee5\u53ca2\u79cdSFM\uff08Whisper\u548cSeamlessM4T\uff09\u5728\u4e24\u79cd\u5e7f\u6cdb\u5e94\u7528\u7684S2T\u4efb\u52a1\uff08\u5373\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\uff09\u4e2d\u7684\u7ec4\u5408\u6548\u679c\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSFM\u5728\u4e0b\u6e38\u6027\u80fd\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u800c\u9002\u914d\u5668\u7684\u9009\u62e9\u5f71\u54cd\u9002\u4e2d\uff0c\u5e76\u4f9d\u8d56\u4e8eSFM\u548cLLM\u3002 Francesco Verdini PDF N/A How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM. \u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53cd\u4e8b\u5b9e\u4ee4\u724c\u751f\u6210 \u5f53\u7136\uff0c\u6211\u5f88\u4e50\u610f\u4e3a\u60a8\u751f\u6210\u4e00\u4e2a\u6545\u4e8b\uff1a\u83b1\u62c9\u8239\u957f\u7ad9\u5728\u5979\u503c\u5f97\u4fe1\u8d56\u7684\u8239\u53ea\u201c\u65cb\u98ce\u4e4b\u6012\u201d\u7684\u8235\u8f6e\u524d\uff0c\u51dd\u89c6\u7740\u65e0\u8fb9\u65e0\u9645\u7684\u5927\u6d77\u3002[...] \u83b1\u62c9\u610f\u8bc6\u5230\u6b8b\u9177\u7684\u771f\u76f8\uff0c\u773c\u4e2d\u6d8c\u51fa\u6cea\u6c34\u2014\u2014\u5979\u4e3a\u4e86\u77ed\u6682\u7684\u8d22\u5bcc\u727a\u7272\u4e86\u4e00\u5207\uff0c\u5931\u53bb\u4e86\u8239\u5458\u7684\u7231\u3001\u5bb6\u4eba\u7684\u7231\uff0c\u751a\u81f3\u5931\u53bb\u4e86\u81ea\u6211\u3002\u5c3d\u7ba1\u8fd9\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6545\u4e8b\u5f15\u4eba\u5165\u80dc\uff0c\u4f46\u4eba\u4eec\u53ef\u80fd\u4f1a\u597d\u5947\u2014\u2014\u5982\u679c\u6a21\u578b\u9009\u62e9\u201c\u6885\u8299\u8239\u957f\u201d\u4f5c\u4e3a\u4e3b\u89d2\uff0c\u6545\u4e8b\u4f1a\u5982\u4f55\u5c55\u5f00\uff1f\u6211\u4eec\u65e0\u4ece\u5f97\u77e5\u3002\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u65e0\u72b6\u6001\u7684\u2014\u2014\u5b83\u4eec\u4e0d\u4fdd\u7559\u4efb\u4f55\u5185\u90e8\u8bb0\u5fc6\u6216\u72b6\u6001\u3002\u7ed9\u5b9a\u4e00\u4e2a\u63d0\u793a\uff0c\u5b83\u4eec\u4f7f\u7528\u81ea\u56de\u5f52\u8fc7\u7a0b\u751f\u6210\u4e00\u7cfb\u5217\u6807\u8bb0\u4f5c\u4e3a\u8f93\u51fa\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u65e0\u6cd5\u5bf9\u8fc7\u53bb\u751f\u6210\u7684\u6807\u8bb0\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8d4b\u4e88\u5b83\u4eec\u8fd9\u79cd\u529f\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eGumbel-Max\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u6807\u8bb0\u751f\u6210\u56e0\u679c\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u5141\u8bb8\u4efb\u4f55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u51e0\u4e4e\u4e0d\u589e\u52a0\u6210\u672c\u7684\u65b9\u5f0f\u8fdb\u884c\u53cd\u4e8b\u5b9e\u6807\u8bb0\u751f\u6210\uff0c\u5b9e\u73b0\u8d77\u6765\u975e\u5e38\u7b80\u5355\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u5fae\u8c03\u6216\u63d0\u793a\u5de5\u7a0b\u3002\u6211\u4eec\u5728Llama 3 8B-instruct\u4e0a\u5b9e\u73b0\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5e76\u5bf9\u53cd\u4e8b\u5b9e\u751f\u6210\u7684\u6587\u672c\u8fdb\u884c\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u53cd\u4e8b\u5b9e\u6807\u8bb0\u751f\u6210\u7684\u793a\u8303\u5e94\u7528\u6765\u68c0\u6d4b\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u6709\u8da3\u89c1\u89e3\u3002 Ivi Chatzi PDF N/A Counterfactual Token Generation in Large Language Models \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...] Lyra's eyes welled up with tears as she realized the bitter truth - she had sacrificed everything for fleeting riches, and lost the love of her crew, her family, and herself.\" Although this story, generated by a large language model, is captivating, one may wonder -- how would the story have unfolded if the model had chosen \"Captain Maeve\" as the protagonist instead? We cannot know. State-of-the-art large language models are stateless -- they maintain no internal memory or state. Given a prompt, they generate a sequence of tokens as an output using an autoregressive process. As a consequence, they cannot reason about counterfactual alternatives to tokens they have generated in the past. In this work, our goal is to enhance them with this functionality. To this end, we develop a causal model of token generation that builds upon the Gumbel-Max structural causal model. Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation, it is embarrassingly simple to implement, and it does not require any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-instruct and conduct both qualitative and quantitative analyses of counterfactually generated text. We conclude with a demonstrative application of counterfactual token generation for bias detection, unveiling interesting insights about the model of the world constructed by large language models. CombU\uff1a\u4e00\u79cd\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u62df\u5408\u6570\u5b66\u8868\u8fbe\u5f0f\u7684\u7ec4\u5408\u5355\u5143\u6fc0\u6d3b\u65b9\u6cd5 \u6fc0\u6d3b\u51fd\u6570\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840\uff0c\u56e0\u4e3a\u5b83\u4eec\u5f15\u5165\u4e86\u6570\u636e\u5173\u7cfb\u4e2d\u7684\u975e\u7ebf\u6027\uff0c\u4ece\u800c\u4f7f\u6df1\u5ea6\u7f51\u7edc\u80fd\u591f\u8fd1\u4f3c\u590d\u6742\u7684\u6570\u636e\u5173\u7cfb\u3002\u73b0\u6709\u7684\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u5f00\u53d1\u65b0\u7684\u6570\u5b66\u51fd\u6570\u4e0a\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5de7\u5999\u5730\u7ec4\u5408\u73b0\u6709\u7684\u6fc0\u6d3b\u51fd\u6570\u4e5f\u80fd\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u7ec4\u5408\u5355\u5143\u6fc0\u6d3b\uff08CombU\uff09\uff0c\u5b83\u5728\u4e0d\u540c\u5c42\u7ea7\u7684\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u6fc0\u6d3b\u51fd\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u53ef\u4ee5\u8bc1\u660e\u80fd\u591f\u51c6\u786e\u62df\u5408\u5927\u591a\u6570\u6570\u5b66\u8868\u8fbe\u5f0f\u3002\u5728\u56db\u4e2a\u6570\u5b66\u8868\u8fbe\u5f0f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u4e0e\u516d\u79cd\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6fc0\u6d3b\u51fd\u6570\u7b97\u6cd5\u76f8\u6bd4\uff0c\u7ed3\u679c\u8868\u660eCombU\u572816\u9879\u6307\u6807\u4e2d\u768410\u9879\u4e0a\u4f18\u4e8e\u6240\u6709SOTA\u7b97\u6cd5\uff0c\u5e76\u5728\u5176\u4f596\u9879\u6307\u6807\u4e2d\u6392\u540d\u524d\u4e09\u3002 Jiayu Li PDF N/A CombU: A Combined Unit Activation for Fitting Mathematical Expressions with Neural Networks The activation functions are fundamental to neural networks as they introduce non-linearity into data relationships, thereby enabling deep networks to approximate complex data relations. Existing efforts to enhance neural network performance have predominantly focused on developing new mathematical functions. However, we find that a well-designed combination of existing activation functions within a neural network can also achieve this objective. In this paper, we introduce the Combined Units activation (CombU), which employs different activation functions at various dimensions across different layers. This approach can be theoretically proven to fit most mathematical expressions accurately. The experiments conducted on four mathematical expression datasets, compared against six State-Of-The-Art (SOTA) activation function algorithms, demonstrate that CombU outperforms all SOTA algorithms in 10 out of 16 metrics and ranks in the top three for the remaining six metrics. CNN\u6df1\u5ea6\u6df7\u5408 \u6211\u4eec\u5f15\u5165\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u7684\u6df1\u5ea6\u6df7\u5408\uff08Mixture-of-Depths, MoD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u901a\u9053\u4e0e\u5f53\u524d\u9884\u6d4b\u7684\u76f8\u5173\u6027\u6765\u9009\u62e9\u6027\u5904\u7406\u901a\u9053\uff0c\u4ece\u800c\u63d0\u9ad8CNNs\u7684\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5377\u79ef\u5757\uff08Conv-Blocks\uff09\u5185\u52a8\u6001\u9009\u62e9\u5173\u952e\u901a\u9053\u8fdb\u884c\u96c6\u4e2d\u5904\u7406\uff0c\u540c\u65f6\u8df3\u8fc7\u4e0d\u592a\u76f8\u5173\u7684\u901a\u9053\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u3002\u4e0e\u9700\u8981\u52a8\u6001\u8ba1\u7b97\u56fe\u7684\u6761\u4ef6\u8ba1\u7b97\u65b9\u6cd5\u4e0d\u540c\uff0cCNN MoD\u4f7f\u7528\u5177\u6709\u56fa\u5b9a\u5f20\u91cf\u5927\u5c0f\u7684\u9759\u6001\u8ba1\u7b97\u56fe\uff0c\u4ece\u800c\u63d0\u9ad8\u786c\u4ef6\u6548\u7387\u3002\u5b83\u52a0\u901f\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u9700\u5b9a\u5236\u7684CUDA\u5185\u6838\u3001\u72ec\u7279\u7684\u635f\u5931\u51fd\u6570\u6216\u5fae\u8c03\u3002CNN MoD\u8981\u4e48\u5728\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3001GMACs\u548c\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u4f20\u7edfCNN\u7684\u6027\u80fd\uff0c\u8981\u4e48\u5728\u4fdd\u6301\u76f8\u4f3c\u63a8\u7406\u65f6\u95f4\u3001GMACs\u548c\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u5176\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728ImageNet\u4e0a\uff0cResNet86-MoD\u5728CPU\u4e0a\u52a0\u901f6%\u3001GPU\u4e0a\u52a0\u901f5%\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8fc7\u4e86\u6807\u51c6\u7684ResNet50 0.45%\u3002\u6b64\u5916\uff0cResNet75-MoD\u5728CPU\u4e0a\u52a0\u901f25%\u3001GPU\u4e0a\u52a0\u901f15%\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0eResNet50\u76f8\u540c\u7684\u6027\u80fd\u3002 Rinor Cakaj PDF N/A CNN Mixture-of-Depths We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks (CNNs), a novel approach that enhances the computational efficiency of CNNs by selectively processing channels based on their relevance to the current prediction. This method optimizes computational resources by dynamically selecting key channels in feature maps for focused processing within the convolutional blocks (Conv-Blocks), while skipping less relevant channels. Unlike conditional computation methods that require dynamic computation graphs, CNN MoD uses a static computation graph with fixed tensor sizes which improve hardware efficiency. It speeds up the training and inference processes without the need for customized CUDA kernels, unique loss functions, or finetuning. CNN MoD either matches the performance of traditional CNNs with reduced inference times, GMACs, and parameters, or exceeds their performance while maintaining similar inference times, GMACs, and parameters. For example, on ImageNet, ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a 6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same performance as ResNet50 with a 25% speedup on CPU and 15% on GPU. LLM-CARD\uff1a\u8fc8\u5411\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u4e0e\u5168\u666f\u56fe \u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u8fc5\u901f\u53d1\u5c55\uff0c\u5927\u91cf\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u65ad\u6d8c\u73b0\uff0c\u7528\u4e8e\u5404\u79cdNLP\u4efb\u52a1\u3002\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u8bba\u6587\u88ab\u53d1\u8868\uff0c\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u9762\u4e34\u7740\u4fe1\u606f\u8fc7\u8f7d\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u63d0\u53d6\u548c\u7ec4\u7ec7\u5173\u4e8eLLMs\u7684\u5173\u952e\u4fe1\u606f\u7684\u7cfb\u7edf\uff08\u5373\\textbf{LLM\u6a21\u578b\u5361\u7247}\uff09\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08\\textbf{NER}\uff09\u548c\u5173\u7cfb\u63d0\u53d6\uff08\\textbf{RE}\uff09\u65b9\u6cd5\uff0c\u5f00\u53d1\u8fd9\u6837\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u7cfb\u7edf\uff0c\u81ea\u52a8\u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9ad8\u6548\u83b7\u53d6\u5173\u4e8eLLMs\u7684\u4fe1\u606f\u3002\u8fd9\u4e9b\u7279\u5f81\u5305\u62ec\u6a21\u578b\u7684\\textit{\u8bb8\u53ef\u8bc1}\u3001\u6a21\u578b\u7684\\textit{\u540d\u79f0}\u4ee5\u53ca\u6a21\u578b\u7684\\textit{\u5e94\u7528}\u3002\u901a\u8fc7\u8fd9\u4e9b\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u4e3a\u6bcf\u7bc7\u8bba\u6587\u751f\u6210\u4e00\u4e2a\u6a21\u578b\u5361\u7247\u3002\u5728\\textbf{\u6570\u636e\u8d21\u732e}\u65b9\u9762\uff0c\u901a\u8fc7\u5bf9106\u7bc7\u5b66\u672f\u8bba\u6587\u8fdb\u884c\u5904\u7406\uff0c\u5b9a\u4e49\u4e86\u4e09\u4e2a\u5b57\u5178\u2014\u2014LLMs\u540d\u79f0\u3001\u8bb8\u53ef\u8bc1\u548c\u5e94\u7528\u3002\u901a\u8fc7\u5b57\u5178\u67e5\u627e\u63d0\u53d6\u4e8611,051\u4e2a\u53e5\u5b50\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u5ba1\u67e5\u6700\u7ec8\u9009\u5b9a\u7684129\u4e2a\u53e5\u5b50\uff08\u8fd9\u4e9b\u53e5\u5b50\u4e2d\u6a21\u578b\u540d\u79f0\u4e0e\u8bb8\u53ef\u8bc1\u4e4b\u95f4\u5b58\u5728\u5173\u8054\uff09\u548c106\u4e2a\u53e5\u5b50\uff08\u8fd9\u4e9b\u53e5\u5b50\u4e2d\u6a21\u578b\u540d\u79f0\u4e0e\u5e94\u7528\u4e4b\u95f4\u5b58\u5728\u5173\u8054\uff09\u6784\u5efa\u4e86\u6570\u636e\u96c6\u3002 Shengwei Tian PDF N/A LLM-CARD: Towards a Description and Landscape of Large Language Models With the rapid growth of the Natural Language Processing (NLP) field, a vast variety of Large Language Models (LLMs) continue to emerge for diverse NLP tasks. As an increasing number of papers are presented, researchers and developers face the challenge of information overload. Thus, it is particularly important to develop a system that can automatically extract and organise key information about LLMs from academic papers (\\textbf{LLM model card}). This work is to develop such a pioneer system by using Named Entity Recognition (\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically extract key information about large language models from the papers, helping researchers to efficiently access information about LLMs. These features include model \\textit{licence}, model \\textit{name}, and model \\textit{application}. With these features, we can form a model card for each paper. \\textbf{Data-contribution} wise, 106 academic papers were processed by defining three dictionaries - LLMs name, licence, and application. 11,051 sentences were extracted through dictionary lookup, and the dataset was constructed through manual review of the final selection of 129 sentences that have a link between the name and the licence, and 106 sentences that have a link between the model name and the application. \u6a21\u578b\u80fd\u591f\u5e76\u4e14\u5e94\u8be5\u63a5\u7eb3\u4eba\u7c7b\u751f\u6210\u6570\u5b66\u7684\u4ea4\u6d41\u6027\u8d28 \u6570\u5b66\u662f\u7531\u4eba\u7c7b\u4e3a\u4eba\u7c7b\u6784\u5efa\u7684\uff1a\u6b63\u5982\u81ea\u7136\u8bed\u6599\u5e93\u4e0d\u4ec5\u53cd\u6620\u547d\u9898\uff0c\u8fd8\u53cd\u6620\u8bed\u8a00\u4f7f\u7528\u8005\u7684\u4ea4\u6d41\u76ee\u6807\u4e00\u6837\uff0c\u6a21\u578b\u6240\u8bad\u7ec3\u7684\u6570\u5b66\u6570\u636e\u4e0d\u4ec5\u53cd\u6620\u7406\u60f3\u5316\u7684\u6570\u5b66\u5b9e\u4f53\uff0c\u8fd8\u53cd\u6620\u4e30\u5bcc\u7684\u4ea4\u6d41\u610f\u56fe\u3002\u5c3d\u7ba1\u4ee5\u7eaf\u7cb9\u7b26\u53f7\u7684\u65b9\u5f0f\u5904\u7406\u6570\u5b66\u6709\u5176\u91cd\u8981\u4f18\u52bf\uff0c\u4f46\u6211\u4eec\u5728\u6b64\u5047\u8bbe\uff0c\u5c06\u6570\u5b66\u89c6\u4e3a\u60c5\u5883\u5316\u7684\u8bed\u8a00\u4ea4\u6d41\u5177\u6709\u597d\u5904\uff0c\u5e76\u4e14\u8bed\u8a00\u6a21\u578b\u975e\u5e38\u9002\u5408\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5c3d\u7ba1\u8fd9\u4e00\u70b9\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8ba4\u8bc6\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u6765\u8bf4\u660e\u8fd9\u4e9b\u89c2\u70b9\u3002\u9996\u5148\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u5b9e\u9a8c\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4ee5\u7c7b\u4eba\u7684\u65b9\u5f0f\u89e3\u91ca\u7b49\u53f7\u2014\u2014\u4e3a\u540c\u4e00\u57fa\u7840\u65b9\u7a0b\u4ee5\u4e0d\u540c\u65b9\u5f0f\u6392\u5217\u751f\u6210\u7cfb\u7edf\u4e0a\u4e0d\u540c\u7684\u6587\u5b57\u9898\u3002\u5176\u6b21\uff0c\u6211\u4eec\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u8bc1\u660e\u4ee5\u81ea\u7136\u7684\u65b9\u5f0f\u6392\u5e8f\uff0c\u5373\u4f7f\u5176\u4ed6\u987a\u5e8f\u5728\u903b\u8f91\u4e0a\u662f\u7b49\u4ef7\u7684\u3002\u6211\u4eec\u4e3b\u5f20\u5f00\u53d1\u4ece\u4eba\u7c7b\u751f\u6210\u7684\u6570\u5b66\u4e2d\u5b66\u4e60\u5e76\u4f53\u73b0\u5176\u4e2d\u6f5c\u5728\u4ea4\u6d41\u610f\u56fe\u7684AI\u7cfb\u7edf\u3002 Sasha Boguraev PDF N/A Models Can and Should Embrace the Communicative Nature of Human-Generated Math Math is constructed by people for people: just as natural language corpora reflect not just propositions but the communicative goals of language users, the math data that models are trained on reflects not just idealized mathematical entities but rich communicative intentions. While there are important advantages to treating math in a purely symbolic manner, we here hypothesize that there are benefits to treating math as situated linguistic communication and that language models are well suited for this goal, in ways that are not fully appreciated. We illustrate these points with two case studies. First, we ran an experiment in which we found that language models interpret the equals sign in a humanlike way -- generating systematically different word problems for the same underlying equation arranged in different ways. Second, we found that language models prefer proofs to be ordered in naturalistic ways, even though other orders would be logically equivalent. We advocate for AI systems that learn from and represent the communicative intentions latent in human-generated math. PitRSDNet\uff1a\u9884\u6d4b\u5185\u955c\u4e0b\u5782\u4f53\u624b\u672f\u4e2d\u624b\u672f\u5269\u4f59\u65f6\u95f4 \u51c6\u786e\u7684\u672f\u4e2d\u5269\u4f59\u624b\u672f\u65f6\u95f4\uff08RSD\uff09\u9884\u6d4b\u4f7f\u9ebb\u9189\u5e08\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u51b3\u5b9a\u4f55\u65f6\u7ed9\u4e88\u9ebb\u9189\u5242\u548c\u836f\u7269\uff0c\u5e76\u901a\u77e5\u533b\u9662\u5de5\u4f5c\u4eba\u5458\u4f20\u9001\u4e0b\u4e00\u4f4d\u60a3\u8005\u3002\u56e0\u6b64\uff0cRSD\u5728\u901a\u8fc7\u9ad8\u6548\u6392\u7a0b\u63d0\u5347\u60a3\u8005\u62a4\u7406\u548c\u964d\u4f4e\u624b\u672f\u5ba4\u6210\u672c\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u5728\u5185\u7aa5\u955c\u5782\u4f53\u624b\u672f\u4e2d\uff0c\u7531\u4e8e\u5b58\u5728\u591a\u79cd\u53ef\u9009\u6b65\u9aa4\u5bfc\u81f4\u7684\u5de5\u4f5c\u6d41\u7a0b\u5e8f\u5217\u53d8\u5316\uff0c\u4f7f\u5f97\u624b\u672f\u65f6\u95f4\u7684\u9884\u6d4b\u5c24\u4e3a\u56f0\u96be\u3002\u672c\u6587\u63d0\u51fa\u4e86PitRSDNet\uff0c\u7528\u4e8e\u9884\u6d4b\u5782\u4f53\u624b\u672f\u4e2d\u7684RSD\uff0c\u8fd9\u662f\u4e00\u79cd\u65f6\u7a7a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u5e8f\u5217\u3002PitRSDNet\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5c06\u5de5\u4f5c\u6d41\u7a0b\u77e5\u8bc6\u878d\u5165RSD\u9884\u6d4b\uff1a1\uff09\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u9884\u6d4b\u6b65\u9aa4\u548cRSD\uff1b2\uff09\u5728\u65f6\u95f4\u5b66\u4e60\u548c\u63a8\u7406\u4e2d\u5c06\u5148\u524d\u6b65\u9aa4\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7eb3\u5165\u3002PitRSDNet\u5728\u4e00\u4e2a\u5305\u542b88\u4e2a\u89c6\u9891\u7684\u65b0\u5185\u7aa5\u955c\u5782\u4f53\u624b\u672f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u76f8\u8f83\u4e8e\u4ee5\u5f80\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u7ade\u4e89\u6027\u6027\u80fd\u63d0\u5347\u3002\u7814\u7a76\u7ed3\u679c\u8fd8\u7a81\u663e\u4e86PitRSDNet\u5982\u4f55\u5229\u7528\u5148\u524d\u6b65\u9aa4\u7684\u77e5\u8bc6\u63d0\u9ad8\u5f02\u5e38\u503c\u6848\u4f8b\u7684RSD\u7cbe\u5ea6\u3002 Anjana Wijekoon PDF N/A PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in Endoscopic Pituitary Surgery Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow for anaesthetists to more accurately decide when to administer anaesthetic agents and drugs, as well as to notify hospital staff to send in the next patient. Therefore RSD plays an important role in improving patient care and minimising surgical theatre costs via efficient scheduling. In endoscopic pituitary surgery, it is uniquely challenging due to variable workflow sequences with a selection of optional steps contributing to high variability in surgery duration. This paper presents PitRSDNet for predicting RSD during pituitary surgery, a spatio-temporal neural network model that learns from historical data focusing on workflow sequences. PitRSDNet integrates workflow knowledge into RSD prediction in two forms: 1) multi-task learning for concurrently predicting step and RSD; and 2) incorporating prior steps as context in temporal learning and inference. PitRSDNet is trained and evaluated on a new endoscopic pituitary surgery dataset with 88 videos to show competitive performance improvements over previous statistical and machine learning methods. The findings also highlight how PitRSDNet improve RSD precision on outlier cases utilising the knowledge of prior steps. INT-FlashAttention\uff1a\u4e3aINT8\u91cf\u5316\u542f\u7528Flash Attention \u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u57fa\u7840\uff0c\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u9762\u4e34\u7740\u4e0e\u5e8f\u5217\u957f\u5ea6\u76f8\u5173\u7684\u4e8c\u6b21\u65f6\u95f4\u4e0e\u5185\u5b58\u590d\u6742\u5ea6\u7684\u6311\u6218\u3002FlashAttention\u901a\u8fc7\u5229\u7528GPU\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u52a0\u901f\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u5e76\u51cf\u5c11\u4e86\u5176\u5185\u5b58\u4f7f\u7528\u3002\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u662f\u5c06FlashAttention\u4e0e\u91cf\u5316\u65b9\u6cd5\u7ed3\u5408\u3002\u672c\u6587\u4ecb\u7ecd\u4e86INT-FlashAttention\uff0c\u8fd9\u662f\u9996\u4e2a\u4e0eFlashAttention\u524d\u5411\u5de5\u4f5c\u6d41\u7a0b\u517c\u5bb9\u7684INT8\u91cf\u5316\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86FlashAttention\u5728Ampere GPU\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u3002\u6211\u4eec\u901a\u8fc7\u5b8c\u5168INT8\u6fc0\u6d3b\u548c\u901a\u7528\u77e9\u9635\u4e58\u6cd5\uff08GEMM\uff09\u5185\u6838\u5b9e\u73b0\u4e86INT-FlashAttention\u539f\u578b\uff0c\u4f7f\u5176\u6210\u4e3a\u9996\u4e2a\u5b8c\u5168INT8\u8f93\u5165\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u7b26\u3002\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684token\u7ea7\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0cINT-FlashAttention\u8fd8\u517c\u5bb9\u5176\u4ed6\u6570\u636e\u683c\u5f0f\uff0c\u5982INT4\u7b49\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f7f\u7528FP16\u548cFP8\u6570\u636e\u683c\u5f0f\u7684\u6807\u51c6FlashAttention\u76f8\u6bd4\uff0cINT-FlashAttention\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e8672%\uff0c\u91cf\u5316\u8bef\u5dee\u51cf\u5c11\u4e8682%\u3002 Shimao Chen PDF N/A INT-FlashAttention: Enabling Flash Attention for INT8 Quantization As the foundation of large language models (LLMs), self-attention module faces the challenge of quadratic time and memory complexity with respect to sequence length. FlashAttention accelerates attention computation and reduces its memory usage by leveraging the GPU memory hierarchy. A promising research direction is to integrate FlashAttention with quantization methods. This paper introduces INT-FlashAttention, the first INT8 quantization architecture compatible with the forward workflow of FlashAttention, which significantly improves the inference speed of FlashAttention on Ampere GPUs. We implement our INT-FlashAttention prototype with fully INT8 activations and general matrix-multiplication (GEMM) kernels, making it the first attention operator with fully INT8 input. As a general token-level post-training quantization framework, INT-FlashAttention is also compatible with other data formats like INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster inference speed and 82% smaller quantization error compared to standard FlashAttention with FP16 and FP8 data format. \u6162\u7279\u5f81\u5206\u6790\uff08Slow Feature Analysis\uff09\u4e0e\u540e\u7ee7\u8868\u793a\uff08Successor Representation\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u4ec0\u4e48\uff1f \u5bf9\u6162\u7279\u5f81\u5206\u6790\uff08SFA\uff09\u548c\u540e\u7ee7\u8868\u793a\uff08SR\uff09\u8fdb\u884c\u4e86\u5206\u6790\u6bd4\u8f83\u3002\u5c3d\u7ba1SFA\u548cSR\u6e90\u81ea\u673a\u5668\u5b66\u4e60\u7684\u4e0d\u540c\u9886\u57df\uff0c\u4f46\u5b83\u4eec\u5728\u6570\u5b66\u6027\u8d28\u548c\u654f\u611f\u4fe1\u606f\u7c7b\u578b\u65b9\u9762\u5177\u6709\u91cd\u8981\u7684\u5171\u6027\u3002\u672c\u7814\u7a76\u6cbf\u7740\u8fd9\u4e24\u4e2a\u8f74\u7ebf\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u8054\u7cfb\u3002\u7279\u522b\u662f\uff0c\u5206\u6790\u4e86SFA\u7b97\u6cd5\u7684\u591a\u4e2a\u53d8\u4f53\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eMDP\uff08\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff09\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u5f15\u51fa\u4e86\u4e00\u7cfb\u5217\u6d89\u53caSR\u53ca\u5176\u4ed6\u76f8\u5173\u91cf\u7684\u7279\u5f81\u503c\u95ee\u9898\u3002\u968f\u540e\uff0c\u5728\u7f51\u683c\u4e16\u754c\u7684\u73a9\u5177\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u8fd9\u4e9b\u7279\u5f81\u503c\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u901a\u5e38\u4e0eSR\u76f8\u5173\u7684\u4f4d\u7f6e\u548c\u7f51\u683c\u72b6\u573a\u57df\u540c\u6837\u53ef\u4ee5\u901a\u8fc7SFA\u751f\u6210\u3002 Eddie Seabrook PDF N/A What is the relationship between Slow Feature Analysis and the Successor Representation? (This is a work in progress. Feedback is welcome) An analytical comparison is made between slow feature analysis (SFA) and the successor representation (SR). While SFA and the SR stem from distinct areas of machine learning, they share important properties, both in terms of their mathematics and the types of information they are sensitive to. This work studies their connection along these two axes. In particular, multiple variants of the SFA algorithm are explored analytically and then applied to the setting of an MDP, leading to a family of eigenvalue problems involving the SR and other related quantities. These resulting eigenvalue problems are then illustrated in the toy setting of a gridworld, where it is demonstrated that the place- and grid-like fields often associated to the SR can equally be generated using SFA. AXCEL\uff1a\u4f7f\u7528LLMs\u8fdb\u884c\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u4e00\u81f4\u6027\u8bc4\u4f30 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\uff0c\u7136\u800c\u8bc4\u4f30\u751f\u6210\u6587\u672c\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4f20\u7edf\u7684\u8bc4\u4f30\u6307\u6807\u5982ROUGE\u548cBLEU\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u8f83\u5f31\u3002\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u7684\u66f4\u590d\u6742\u6307\u6807\u867d\u7136\u663e\u793a\u51fa\u6539\u8fdb\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5176\u5b9e\u73b0\u590d\u6742\uff0c\u7531\u4e8e\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u5dee\u800c\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u8bad\u7ec3\uff0c\u5e76\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u6700\u8fd1\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u8bc4\u4f30\u6307\u6807\u4f7f\u7528LLMs\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5df2\u7ecf\u51fa\u73b0\uff1b\u5c3d\u7ba1\u5b83\u4eec\u66f4\u5bb9\u6613\u5b9e\u73b0\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7279\u5b9a\u7684\u63d0\u793a\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u901a\u7528\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4f7f\u7528LLMs\u7684\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u4e00\u81f4\u6027\u8bc4\u4f30\uff08AXCEL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u63d0\u4f9b\u8be6\u7ec6\u7684\u63a8\u7406\u548c\u6307\u51fa\u4e0d\u4e00\u81f4\u7684\u6587\u672c\u7247\u6bb5\u6765\u89e3\u91ca\u4e00\u81f4\u6027\u8bc4\u5206\u3002AXCEL\u8fd8\u662f\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u6307\u6807\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u591a\u4e2a\u4efb\u52a1\u800c\u65e0\u9700\u66f4\u6539\u63d0\u793a\u3002\u5728\u68c0\u6d4b\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\uff0cAXCEL\u5728\u603b\u7ed3\u4efb\u52a1\u4e2d\u6bd4\u975e\u63d0\u793a\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6307\u6807\u9ad8\u51fa8.7%\uff0c\u5728\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u9ad8\u51fa6.2%\uff0c\u5728\u6570\u636e\u5230\u6587\u672c\u8f6c\u6362\u4efb\u52a1\u4e2d\u9ad8\u51fa29.4%\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u5e95\u5c42LLMs\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u8bc4\u4f30\u6307\u6807\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u6700\u65b0\u7684LLMs\u5bf9SOTA\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u6807\u8fdb\u884c\u91cd\u65b0\u6821\u51c6\uff0c\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86AXCEL\u5728\u4f7f\u7528\u5f00\u6e90LLMs\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002 P Aditya Sreekar PDF N/A AXCEL: Automated eXplainable Consistency Evaluation using LLMs Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated eXplainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs. \u9762\u5411\u7528\u6237\u7684\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\u7814\u7a76\uff1a\u4ee5\u4eba\u4e3a\u672c\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd \u5c3d\u7ba1\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65e8\u5728\u4f7f\u4eba\u5de5\u667a\u80fd\u5bf9\u4eba\u7c7b\u6765\u8bf4\u65e2\u53ef\u7406\u89e3\u53c8\u6709\u7528\uff0c\u4f46\u5b83\u56e0\u8fc7\u4e8e\u4f9d\u8d56\u5f62\u5f0f\u4e3b\u4e49\u548c\u89e3\u51b3\u65b9\u6848\u4e3b\u4e49\u800c\u53d7\u5230\u6279\u8bc4\uff0c\u66f4\u5173\u6ce8\u6570\u5b66\u4e0a\u7684\u5408\u7406\u6027\u800c\u975e\u7528\u6237\u9700\u6c42\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u8bbe\u8ba1\u601d\u7ef4\u542f\u53d1\u7684\u66ff\u4ee3\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff1aXAI\u7814\u7a76\u793e\u533a\u5e94\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89d2\uff0c\u4ee5\u786e\u4fdd\u7528\u6237\u76f8\u5173\u6027\u3002\u6211\u4eec\u901a\u8fc7XAI\u4e2d\u4e00\u4e2a\u76f8\u5bf9\u5e74\u8f7b\u7684\u5b50\u9886\u57df\u2014\u2014\u8bad\u7ec3\u6570\u636e\u5f52\u5c5e\uff08TDA\uff09\u6765\u9610\u91ca\u8fd9\u4e00\u70b9\u3002\u968f\u7740TDA\u7814\u7a76\u7684\u6fc0\u589e\u548c\u7ade\u4e89\u7684\u52a0\u5267\uff0c\u8be5\u9886\u57df\u5b58\u5728\u91cd\u590d\u89e3\u51b3\u65b9\u6848\u4e3b\u4e49\u6a21\u5f0f\u7684\u98ce\u9669\u3002\u6211\u4eec\u4e0e\u4e00\u7fa4\u591a\u6837\u5316\u7684AI\u4ece\u4e1a\u8005\u8fdb\u884c\u4e86\u9700\u6c42\u53d1\u73b0\u7814\u7a76\uff0c\u4ee5\u8bc6\u522b\u4e0eTDA\u76f8\u5173\u7684\u6f5c\u5728\u7528\u6237\u9700\u6c42\u3002\u901a\u8fc7\u8bbf\u8c08\uff08N=10\uff09\u548c\u7cfb\u7edf\u7684\u8c03\u67e5\uff08N=31\uff09\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u76ee\u524d\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u89c6\u7684\u65b0TDA\u4efb\u52a1\u3002\u6211\u4eec\u9080\u8bf7TDA\u548cXAI\u793e\u533a\u8003\u8651\u8fd9\u4e9b\u65b0\u9896\u4efb\u52a1\uff0c\u5e76\u63d0\u5347\u5176\u7814\u7a76\u6210\u679c\u7684\u7528\u6237\u76f8\u5173\u6027\u3002 Elisa Nguyen PDF N/A Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI While Explainable AI (XAI) aims to make AI understandable and useful to humans, it has been criticised for relying too much on formalism and solutionism, focusing more on mathematical soundness than user needs. We propose an alternative to this bottom-up approach inspired by design thinking: the XAI research community should adopt a top-down, user-focused perspective to ensure user relevance. We illustrate this with a relatively young subfield of XAI, Training Data Attribution (TDA). With the surge in TDA research and growing competition, the field risks repeating the same patterns of solutionism. We conducted a needfinding study with a diverse group of AI practitioners to identify potential user needs related to TDA. Through interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks that are currently largely overlooked. We invite the TDA and XAI communities to consider these novel tasks and improve the user relevance of their research outcomes. \u89e3\u7801\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff1a\u793e\u4f1a\u6280\u672f\u5f71\u54cd\u3001\u7ea6\u675f\u53ca\u65b0\u5174\u95ee\u9898\u7684\u7cfb\u7edf\u6982\u8ff0 \u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u6781\u5927\u5730\u9769\u65b0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u9886\u57df\uff0c\u4f7f\u5176\u80fd\u591f\u7406\u89e3\u548c\u4e0e\u4eba\u7c7b\u8bed\u8a00\u4e92\u52a8\u3002\u56e0\u6b64\uff0c\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5bf9\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8c03\u67e5\uff0c\u4ee5\u8bc6\u522bLLM\u53d1\u5c55\u3001\u5f71\u54cd\u548c\u5c40\u9650\u6027\u7684\u4e3b\u8981\u4e3b\u9898\u548c\u65b9\u5411\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86LLM\u7814\u7a76\u7684\u76ee Zeyneb N. Kaya PDF N/A Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions There have been rapid advancements in the capabilities of large language models (LLMs) in recent years, greatly revolutionizing the field of natural language processing (NLP) and artificial intelligence (AI) to understand and interact with human language. Therefore, in this work, we conduct a systematic investigation of the literature to identify the prominent themes and directions of LLM developments, impacts, and limitations. Our findings illustrate the aims, methodologies, limitations, and future directions of LLM research. It includes responsible development considerations, algorithmic improvements, ethical challenges, and societal implications of LLM development. Overall, this paper provides a rigorous and comprehensive overview of current research in LLM and identifies potential directions for future development. The article highlights the application areas that could have a positive impact on society along with the ethical considerations. \u81ea\u9002\u5e94\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u7528\u4e8e\u52a8\u6001\u8bbe\u5907\u4e0a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u6211\u4eec\u4e0e\u6280\u672f\u7684\u4e92\u52a8\u65b9\u5f0f\uff0c\u4f46\u5b83\u4eec\u5bf9\u4e2a\u4eba\u7528\u6237\u504f\u597d\u7684\u4e2a\u6027\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u4e2d\u3002\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u4e25\u91cd\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff08ASLS\uff09\uff0c\u8be5\u7b56\u7565\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\u6765\u52a8\u6001\u4e2a\u6027\u5316LLMs\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u7528\u6237\u753b\u50cf\u5c42\uff0c\u7528\u4e8e\u6536\u96c6\u4e92\u52a8\u6570\u636e\uff0c\u4ee5\u53ca\u4e00\u4e2a\u795e\u7ecf\u9002\u5e94\u5c42\uff0c\u7528\u4e8e\u5b9e\u65f6\u6a21\u578b\u5fae\u8c03\u3002\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u4ece\u7528\u6237\u53cd\u9988\u4e2d\u6301\u7eed\u5b66\u4e60\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u7528\u6237\u7279\u5b9a\u60c5\u5883\u9ad8\u5ea6\u4e00\u81f4\u7684\u54cd\u5e94\u3002ASLS\u7684\u81ea\u9002\u5e94\u673a\u5236\u6700\u5c0f\u5316\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e2a\u6027\u5316\u6548\u7387\u3002\u5728\u5404\u79cd\u7528\u6237\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASLS\u5728\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6ee1\u610f\u5ea6\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u7a81\u663e\u4e86\u5176\u5728\u8bbe\u5907\u4e0a\u91cd\u65b0\u5b9a\u4e49LLMs\u4e3a\u9ad8\u5ea6\u54cd\u5e94\u548c\u60c5\u5883\u611f\u77e5\u7cfb\u7edf\u7684\u6f5c\u529b\u3002 Rafael Mendoza PDF N/A Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization Large language models (LLMs) have revolutionized how we interact with technology, but their personalization to individual user preferences remains a significant challenge, particularly in on-device applications. Traditional methods often depend heavily on labeled datasets and can be resource-intensive. To address these issues, we present Adaptive Self-Supervised Learning Strategies (ASLS), which utilizes self-supervised learning techniques to personalize LLMs dynamically. The framework comprises a user profiling layer for collecting interaction data and a neural adaptation layer for real-time model fine-tuning. This innovative approach enables continuous learning from user feedback, allowing the model to generate responses that align closely with user-specific contexts. The adaptive mechanisms of ASLS minimize computational demands and enhance personalization efficiency. Experimental results across various user scenarios illustrate the superior performance of ASLS in boosting user engagement and satisfaction, highlighting its potential to redefine LLMs as highly responsive and context-aware systems on-device. \u65e0\u7ebf\u4eba\u5de5\u667a\u80fd\u8303\u5f0f\u7684\u786c\u4ef6\u5728\u73af\u771f\u5b9e\u73af\u5883\u6865\u6881 \u5982\u4eca\uff0c\u8bb8\u591a\u7528\u4e8e\u6539\u8fdb\u8f66\u8f7d\u81ea\u7ec4\u7ec7\u7f51\u7edc\uff08VANET\uff09\u4e2d\u65e0\u7ebf\u6807\u51c6IEEE802.11p\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4e0e\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u76f8\u6bd4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u66f4\u5177\u6210\u672c\u6548\u76ca\uff0c\u56e0\u4e3a\u8f66\u8f86\u7684\u6210\u672c\u8f83\u9ad8\u3002\u5f53\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b9e\u65bd\u65f6\uff0c\u5b58\u5728\u610f\u5916\u7ed3\u679c\u7684\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u786c\u4ef6\u5728\u73af\uff08Hardware-in-the-Loop\uff09\u662f\u524d\u8fdb\u7684\u65b9\u5411\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u4e16\u754c\u4e2d\u5171\u540c\u6d4b\u8bd5\u7684\u673a\u4f1a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u6211\u4eec\u8ba4\u4e3a\u662f\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u6d4b\u8bd5\u4eba\u5de5\u667a\u80fd\u3001\u591a\u670d\u52a1\u548c\u9ad8\u6e05\u5730\u56fe\u6570\u636e\uff08LiDAR\uff09\u7684\u5f00\u521b\u6027\u786c\u4ef6\u5728\u73af\u7cfb\u7edf\u3002 Jeffrey Redondo PDF N/A Bridge to Real Environment with Hardware-in-the-loop for Wireless Artificial Intelligence Paradigms Nowadays, many machine learning (ML) solutions to improve the wireless standard IEEE802.11p for Vehicular Adhoc Network (VANET) are commonly evaluated in the simulated world. At the same time, this approach could be cost-effective compared to real-world testing due to the high cost of vehicles. There is a risk of unexpected outcomes when these solutions are implemented in the real world, potentially leading to wasted resources. To mitigate this challenge, the hardware-in-the-loop is the way to move forward as it enables the opportunity to test in the real world and simulated worlds together. Therefore, we have developed what we believe is the pioneering hardware-in-the-loop for testing artificial intelligence, multiple services, and HD map data (LiDAR), in both simulated and real-world settings. ABCFair\uff1a\u4e00\u79cd\u7528\u4e8e\u6bd4\u8f83\u516c\u5e73\u65b9\u6cd5\u7684\u81ea\u9002\u5e94\u57fa\u51c6\u65b9\u6cd5 \u5df2\u7ecf\u5b9e\u73b0\u4e86\u8bb8\u591a\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u8f7b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u504f\u89c1\u6765\u8ffd\u6c42\u5bf9\u654f\u611f\u7279\u5f81\u7684\u516c\u5e73\u6027\u3002\u7136\u800c\uff0c\u6bcf\u79cd\u65b9\u6cd5\u6240\u89e3\u51b3\u7684\u95ee\u9898\u8bbe\u7f6e\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5305\u62ec\u5e72\u9884\u9636\u6bb5\u3001\u654f\u611f\u7279\u5f81\u7684\u6784\u6210\u3001\u516c\u5e73\u6027\u6982\u5ff5\u4ee5\u53ca\u8f93\u51fa\u7684\u5206\u5e03\u3002\u5373\u4f7f\u5728\u4e8c\u5206\u7c7b\u4e2d\uff0c\u8fd9\u4e9b\u7ec6\u5fae\u7684\u5dee\u5f02\u4e5f\u4f7f\u5f97\u57fa\u51c6\u6d4b\u8bd5\u516c\u5e73\u65b9\u6cd5\u53d8\u5f97\u975e\u5e38\u590d\u6742\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8868\u73b0\u53ef\u80fd\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u504f\u89c1\u7f13\u89e3\u95ee\u9898\u6700\u521d\u662f\u5982\u4f55\u6784\u5efa\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86ABCFair\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u8bbe\u7f6e\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u5728\u4efb\u4f55\u7528\u4f8b\u4e2d\u65b9\u6cd5\u4e4b\u95f4\u7684\u6bd4\u8f83\u66f4\u52a0\u5408\u7406\u3002\u6211\u4eec\u5c06ABCFair\u5e94\u7528\u4e8e\u4e00\u7cfb\u5217\u9884\u5904\u7406\u3001\u5904\u7406\u4e2d\u548c\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u65e2\u5305\u62ec\u5927\u89c4\u6a21\u7684\u4f20\u7edf\u6570\u636e\u96c6\uff0c\u4e5f\u5305\u62ec\u53cc\u6807\u7b7e\uff08\u6709\u504f\u548c\u65e0\u504f\uff09\u6570\u636e\u96c6\uff0c\u4ee5\u89c4\u907f\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002 MaryBeth Defrance PDF N/A ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.   Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. We apply ABCFair to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off. \u6c42\u89e3\u65b9\u7a0b\u7ec4\u7684\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5 \u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u5728\u6c42\u89e3\u7ebf\u6027\u53ca\u975e\u7ebf\u6027\u65b9\u7a0b\u7ec4\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5176\u6027\u80fd\u4e0e\u4f20\u7edf\u7684\u89e3\u6cd5\u5982\u9ad8\u65af\u6d88\u5143\u6cd5\u3001\u725b\u987f\u6cd5\u53ca\u5217\u6587\u4f2f\u683c-\u9a6c\u5938\u5c14\u7279\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u9057\u4f20\u7b97\u6cd5\u5728\u5404\u79cd\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5747\u80fd\u63d0\u4f9b\u7cbe\u786e\u7684\u89e3\uff0c\u5c55\u73b0\u4e86\u5176\u7a33\u5065\u6027\u548c\u7075\u6d3b\u6027\u3002\u9057\u4f20\u7b97\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u5176\u80fd\u591f\u5e7f\u6cdb\u5730\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u63ed\u793a\u591a\u7ec4\u89e3\u2014\u2014\u8fd9\u662f\u4f20\u7edf\u65b9\u6cd5\uff08\u901a\u5e38\u4ec5\u6536\u655b\u4e8e\u5355\u4e00\u89e3\uff09\u6240\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002\u8fd9\u4e00\u7279\u6027\u5728\u590d\u6742\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5c24\u4e3a\u6709\u76ca\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7cfb\u7edf\u5f80\u5f80\u5b58\u5728\u591a\u4e2a\u6709\u6548\u89e3\uff0c\u4ece\u800c\u51f8\u663e\u4e86\u9057\u4f20\u7b97\u6cd5\u5728\u5e94\u5bf9\u590d\u6742\u89e3\u7a7a\u95f4\u65f6\u7684\u4f18\u8d8a\u6027\u3002 Samson Odan PDF N/A Metaheuristic Method for Solving Systems of Equations This study investigates the effectiveness of Genetic Algorithms (GAs) in solving both linear and nonlinear systems of equations, comparing their performance to traditional methods such as Gaussian Elimination, Newton's Method, and Levenberg-Marquardt. The GA consistently delivered accurate solutions across various test cases, demonstrating its robustness and flexibility. A key advantage of the GA is its ability to explore the solution space broadly, uncovering multiple sets of solutions -- a feat that traditional methods, which typically converge to a single solution, cannot achieve. This feature proved especially beneficial in complex nonlinear systems, where multiple valid solutions exist, highlighting the GA's superiority in navigating intricate solution landscapes. \u4fe1\u606f\u5316\u7684\u6df1\u5ea6\u5206\u5c42\u5206\u7c7b\uff1a\u4e00\u79cd\u53d7\u975e\u6807\u51c6\u5206\u6790\u542f\u53d1\u7684\u975e\u4f20\u7edf\u65b9\u6cd5 \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5c42\u6b21\u5206\u7c7b\u4efb\u52a1\u65b9\u6cd5\uff0c\u5373\u6839\u636e\u591a\u4e2a\u6807\u7b7e\u5728\u4e25\u683c\u7684\u7236\u5b50\u7ed3\u6784\u4e2d\u7ec4\u7ec7\u7684\u6570\u636e\u5206\u7c7b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u591a\u8f93\u51fa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5728\u6bcf\u4e2a\u8f93\u51fa\u5c42\u524d\u914d\u5907\u4e86\u7279\u5b9a\u7684\u6295\u5f71\u7b97\u5b50\u3002\u8fd9\u79cd\u88ab\u79f0\u4e3a\u8bcd\u5178\u6df7\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08LH-DNN\uff09\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u662f\u901a\u8fc7\u7ed3\u5408\u6765\u81ea\u4e0d\u540c\u4e14\u76f8\u5f53\u9065\u8fdc\u7814\u7a76\u9886\u57df\u7684\u5de5\u5177\u5b9e\u73b0\u7684\uff1a\u8bcd\u5178\u591a\u76ee\u6807\u4f18\u5316\u3001\u975e\u6807\u51c6\u5206\u6790\u548c\u6df1\u5ea6\u5b66\u4e60\u3002\u4e3a\u4e86\u8bc4\u4f30\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5c06\u7531\u6b64\u4ea7\u751f\u7684\u7f51\u7edc\u4e0e\u4e13\u95e8\u4e3a\u5c42\u6b21\u5206\u7c7b\u4efb\u52a1\u8bbe\u8ba1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08B-CNN\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u6d4b\u8bd5\u57fa\u51c6\u5305\u62ecCIFAR10\u3001CIFAR100\uff08\u8be5\u7f51\u7edc\u6700\u521d\u5728\u6b64\u63d0\u51fa\uff0c\u5e76\u6700\u8fd1\u88ab\u5e94\u7528\u4e8e\u591a\u4e2a\u5b9e\u9645\u573a\u666f\u5e76\u8fdb\u884c\u4e86\u8c03\u6574\uff09\u548cFashion-MNIST\u3002\u8bc1\u636e\u8868\u660e\uff0cLH-DNN\u5728\u5927\u5e45\u51cf\u5c11\u5b66\u4e60\u53c2\u6570\u3001\u8bad\u7ec3\u5468\u671f\u548c\u8ba1\u7b97\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u5b9e\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b66\u4e60\u5c42\u6b21\u5173\u7cfb\u65b9\u9762\uff0c\u4e14\u65e0\u9700\u4e13\u95e8\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u52a0\u6743\u503c\u3002 Lorenzo Fiaschi PDF N/A Informed deep hierarchical classification: a non-standard analysis inspired approach This work proposes a novel approach to the deep hierarchical classification task, i.e., the problem of classifying data according to multiple labels organized in a rigid parent-child structure. It consists in a multi-output deep neural network equipped with specific projection operators placed before each output layer. The design of such an architecture, called lexicographic hybrid deep neural network (LH-DNN), has been possible by combining tools from different and quite distant research fields: lexicographic multi-objective optimization, non-standard analysis, and deep learning. To assess the efficacy of the approach, the resulting network is compared against the B-CNN, a convolutional neural network tailored for hierarchical classification tasks, on the CIFAR10, CIFAR100 (where it has been originally and recently proposed before being adopted and tuned for multiple real-world applications) and Fashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable if not superior performance, especially in the learning of the hierarchical relations, in the face of a drastic reduction of the learning parameters, training epochs, and computational time, without the need for ad-hoc loss functions weighting values. \u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u52a0\u6743\u4ea4\u53c9\u71b5 \u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u4f4e\u8d44\u6e90\u8bed\u8a00\u6574\u5408\u5230\u591a\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e94\u7528\uff0c\u5373\u52a0\u6743\u4ea4\u53c9\u71b5\uff08\u901a\u5e38\u7528\u4e8e\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff09\uff0c\u4ee5\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u6301\u7eed\u591a\u8bed\u8a00\u5b66\u4e60\u80cc\u666f\u4e0b\u878d\u5165\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00ASR\u6a21\u578b\u3002\u6211\u4eec\u5bf9Whisper\u591a\u8bed\u8a00ASR\u6a21\u578b\u5728\u4e94\u79cd\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u91c7\u7528\u4e86\u8bed\u8a00\u52a0\u6743\u7684\u52a8\u6001\u4ea4\u53c9\u71b5\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u672a\u5e94\u7528\u6211\u4eec\u65b9\u6cd5\u7684\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u663e\u8457\u964d\u4f4e\u4e866.69%\uff0c\u4e0e\u539f\u59cbWhisper\u6a21\u578b\u76f8\u6bd4\u964d\u4f4e\u4e8648.86%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u516d\u79cd\u8bed\u8a00\u4e2d\u5e73\u5747\u964d\u4f4e\u4e863.29%\u7684WER\uff0c\u4e14\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u6ca1\u6709\u9020\u6210\u6027\u80fd\u4e0b\u964d\u3002 Andr\u00e9s Pi\u00f1eiro-Mart\u00edn PDF N/A Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition This paper addresses the challenge of integrating low-resource languages into multilingual automatic speech recognition (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to facilitate the integration of low-resource languages into pre-trained multilingual ASR models within the context of continual multilingual learning. We fine-tune the Whisper multilingual ASR model on five high-resource languages and one low-resource language, employing language-weighted dynamic cross-entropy and data augmentation. The results show a remarkable 6.69% word error rate (WER) reduction for the low-resource language compared to the fine-tuned model without applying our approach, and a 48.86% WER reduction compared to the original Whisper model. In addition, our approach yields an average WER reduction of 3.29% across the six languages, showing no degradation for the high-resource languages. \u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u9002\u5e94\u89c4\u5212\u4e0e\u6269\u6563\u7684\u52a8\u6001\u969c\u788d\u7269\u89c4\u907f \u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e3a\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u4f7f\u5f97\u751f\u6210\u6a21\u578b\uff0c\u5982\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u7528\u4e8e\u89c4\u5212\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u9884\u6d4b\u957f\u65f6\u7a0b\u72b6\u6001\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u4f46\u5b83\u4eec\u5728\u5b58\u5728\u79fb\u52a8\u969c\u788d\u7269\u7684\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002\u6709\u6548\u7684\u78b0\u649e\u907f\u514d\u9700\u8981\u6301\u7eed\u76d1\u63a7\u548c\u9002\u5e94\u6027\u51b3\u7b56\u3002\u5c3d\u7ba1\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u91cd\u65b0\u89c4\u5212\u53ef\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u7531\u4e8e\u91cd\u590d\u9884\u6d4b\u91cd\u53e0\u72b6\u6001\u5e8f\u5217\uff0c\u8fd9\u5f15\u5165\u4e86\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\u2014\u2014\u5c24\u5176\u662f\u5bf9\u4e8e\u4ee5\u5bc6\u96c6\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u8457\u79f0\u7684\u6269\u6563\u6a21\u578b\u6765\u8bf4\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u5c24\u4e3a\u6602\u8d35\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u751f\u6210\u89c4\u5212\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u52a8\u4f5c\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u91cd\u65b0\u89c4\u5212\u7684\u9891\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u5f3a\u5927\u7684\u78b0\u649e\u907f\u514d\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6700\u5c0f\u5316\u4e86\u9891\u7e41\u3001\u8ba1\u7b97\u6602\u8d35\u4e14\u5197\u4f59\u7684\u91cd\u65b0\u89c4\u5212\u9700\u6c42\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5e73\u5747\u8f68\u8ff9\u957f\u5ea6\u589e\u52a013.5%\u548c\u957f\u65f6\u7a0b\u89c4\u5212\u4e2d\u5e73\u5747\u5956\u52b1\u589e\u52a012.7%\u7684\u6548\u679c\uff0c\u8fd9\u8868\u660e\u78b0\u649e\u7387\u964d\u4f4e\uff0c\u5e76\u4e14\u5728\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u7684\u80fd\u529b\u5f97\u5230\u63d0\u5347\u3002 Vineet Punyamoorty PDF N/A Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning with Diffusion By framing reinforcement learning as a sequence modeling problem, recent work has enabled the use of generative models, such as diffusion models, for planning. While these models are effective in predicting long-horizon state trajectories in deterministic environments, they face challenges in dynamic settings with moving obstacles. Effective collision avoidance demands continuous monitoring and adaptive decision-making. While replanning at every timestep could ensure safety, it introduces substantial computational overhead due to the repetitive prediction of overlapping state sequences -- a process that is particularly costly with diffusion models, known for their intensive iterative sampling procedure. We propose an adaptive generative planning approach that dynamically adjusts replanning frequency based on the uncertainty of action predictions. Our method minimizes the need for frequent, computationally expensive, and redundant replanning while maintaining robust collision avoidance performance. In experiments, we obtain a 13.5% increase in the mean trajectory length and a 12.7% increase in mean reward over long-horizon planning, indicating a reduction in collision rates and an improved ability to navigate the environment safely. \u57fa\u4e8e\u591a\u89c6\u89d2\u4f2a\u6807\u7b7e\u7684\u8bed\u97f3\u534a\u76d1\u7763\u8ba4\u77e5\u72b6\u6001\u5206\u7c7b \u5728\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u662f\u4e00\u4e2a\u5e38\u89c1\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5927\u91cf\u4e3b\u89c2\u8bc4\u4f30\u7684\u4efb\u52a1\u4e2d\uff0c\u5982\u8ba4\u77e5\u72b6\u6001\u5206\u7c7b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\uff08Semi-Supervised Learning, SSL\uff09\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u89d2\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\u6765\u9009\u62e9\u6700\u53ef\u4fe1\u7684\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u5206\u7c7b\u6a21\u578b\u3002\u5728\u58f0\u5b66\u65b9\u9762\uff0c\u4f7f\u7528\u591a\u4e2a\u97f3\u9891\u7f16\u7801\u5668\u751f\u6210\u7684\u5d4c\u5165\u8ba1\u7b97\u5f17\u96f7\u6b47\u97f3\u9891\u8ddd\u79bb\uff0c\u5c06\u672a\u6807\u6ce8\u6570\u636e\u4e0e\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u8bed\u8a00\u65b9\u9762\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u63d0\u793a\u4fee\u6b63\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u5e76\u6839\u636e\u6211\u4eec\u63d0\u51fa\u7684\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u9884\u6d4b\u6807\u7b7e\u3002\u5f53\u6765\u81ea\u4e24\u4e2a\u6765\u6e90\u7684\u4f2a\u6807\u7b7e\u4e00\u81f4\u65f6\uff0c\u8bc6\u522b\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u6570\u636e\uff0c\u800c\u5f53\u4f2a\u6807\u7b7e\u4e0d\u4e00\u81f4\u65f6\uff0c\u5219\u89c6\u4e3a\u4f4e\u7f6e\u4fe1\u5ea6\u6570\u636e\u3002\u7136\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2a\u53cc\u6a21\u6001\u5206\u7c7b\u5668\uff0c\u8fed\u4ee3\u5730\u6807\u6ce8\u4f4e\u7f6e\u4fe1\u5ea6\u6570\u636e\uff0c\u76f4\u5230\u8fbe\u5230\u9884\u5b9a\u4e49\u7684\u6807\u51c6\u3002\u6211\u4eec\u5728\u60c5\u7eea\u8bc6\u522b\u548c\u75f4\u5446\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684SSL\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u752830%\u6807\u6ce8\u6570\u636e\u7684\u5168\u76d1\u7763\u5b66\u4e60\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u4e24\u4e2a\u9009\u5b9a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002 Yuanchao Li PDF N/A Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines. \u63a2\u7a76OCR\u654f\u611f\u795e\u7ecf\u5143\u4ee5\u63d0\u5347\u5386\u53f2\u6587\u6863\u4e2d\u7684\u5b9e\u4f53\u8bc6\u522b \u672c\u6587\u63a2\u8ba8\u4e86Transformer\u67b6\u6784\u4e2d\u5b58\u5728\u7684OCR\u654f\u611f\u795e\u7ecf\u5143\u53ca\u5176\u5bf9\u5386\u53f2\u6587\u6863\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6027\u80fd\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5206\u6790\u5728\u5e72\u51c0\u548c\u566a\u58f0\u6587\u672c\u8f93\u5165\u4e0b\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u6211\u4eec\u8bc6\u522b\u5e76\u4e2d\u548c\u4e86OCR\u654f\u611f\u795e\u7ecf\u5143\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u57fa\u4e8e\u4e24\u4e2a\u5f00\u653e\u8bbf\u95ee\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama2\u548cMistral\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86OCR\u654f\u611f\u533a\u57df\u7684\u5b58\u5728\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5386\u53f2\u62a5\u7eb8\u548c\u53e4\u5178\u8bc4\u8bba\u4e0a\u7684NER\u6027\u80fd\u63d0\u5347\uff0c\u7a81\u663e\u4e86\u9488\u5bf9\u795e\u7ecf\u5143\u8fdb\u884c\u8c03\u5236\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u566a\u58f0\u6587\u672c\u4e0a\u6027\u80fd\u7684\u6f5c\u529b\u3002 Emanuela Boros PDF N/A Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents This paper investigates the presence of OCR-sensitive neurons within the Transformer architecture and their influence on named entity recognition (NER) performance on historical documents. By analysing neuron activation patterns in response to clean and noisy text inputs, we identify and then neutralise OCR-sensitive neurons to improve model performance. Based on two open access large language models (Llama2 and Mistral), experiments demonstrate the existence of OCR-sensitive regions and show improvements in NER performance on historical newspapers and classical commentaries, highlighting the potential of targeted neuron modulation to improve models' performance on noisy text. \u5229\u7528\u4e0d\u53d8\u6620\u5c04\u5206\u89e3\u7b49\u53d8\u6620\u5c04\uff1a\u5bf9\u79f0\u6027\u4e0b\u901a\u7528\u903c\u8fd1\u7684\u5e94\u7528 \u672c\u6587\u4e2d\uff0c\u6211\u4eec\u53d1\u5c55\u4e86\u4e00\u4e2a\u5173\u4e8e\u7fa4 ( G ) \u7684\u4e0d\u53d8\u6620\u5c04\u4e0e\u7b49\u53d8\u6620\u5c04\u4e4b\u95f4\u5173\u7cfb\u7684\u7406\u8bba\u3002\u968f\u540e\uff0c\u6211\u4eec\u5728\u5177\u6709\u7fa4\u5bf9\u79f0\u6027\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u80cc\u666f\u4e0b\u5229\u7528\u8fd9\u4e00\u7406\u8bba\uff0c\u4ee5\u83b7\u5f97\u5bf9\u5176\u673a\u5236\u7684\u65b0\u89c1\u89e3\u3002\u66f4\u786e\u5207\u5730\u8bf4\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u7b49\u53d8\u6620\u5c04\u4e0e\u67d0\u4e9b\u4e0d\u53d8\u6620\u5c04\u4e4b\u95f4\u7684\u4e00\u4e00\u5bf9\u5e94\u5173\u7cfb\u3002\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u5c06\u7b49\u53d8\u6620\u5c04\u7684\u8bba\u8bc1\u7b80\u5316\u4e3a\u4e0d\u53d8\u6620\u5c04\u7684\u8bba\u8bc1\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u4f5c\u4e3a\u5e94\u7528\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u4e0d\u53d8\u7f51\u7edc\u6784\u5efa\u901a\u7528\u7b49\u53d8\u67b6\u6784\u7684\u65b9\u6cd5\u3002\u8fdb\u800c\uff0c\u6211\u4eec\u89e3\u91ca\u4e86\u7531\u6211\u4eec\u7684\u6784\u9020\u4ea7\u751f\u7684\u901a\u7528\u67b6\u6784\u4e0e\u5df2\u77e5\u7684\u901a\u7528\u6807\u51c6\u7b49\u53d8\u67b6\u6784\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u5728\u81ea\u7531\u53c2\u6570\u6570\u91cf\u65b9\u9762\u7684\u6a21\u578b\u590d\u6742\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0d\u53d8\u7f51\u7edc\u4e0e\u7b49\u53d8\u7f51\u7edc\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fd8\u7ed9\u51fa\u4e86\u6709\u9650\u7fa4 ( G ) \u4e0b\u5e26\u6709 ReLU \u6fc0\u6d3b\u51fd\u6570\u7684 ( G )-\u7b49\u53d8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u903c\u8fd1\u7387\u3002 Akiyoshi Sannai PDF N/A Decomposition of Equivariant Maps via Invariant Maps: Application to Universal Approximation under Symmetry In this paper, we develop a theory about the relationship between invariant and equivariant maps with regard to a group $G$. We then leverage this theory in the context of deep neural networks with group symmetries in order to obtain novel insight into their mechanisms. More precisely, we establish a one-to-one relationship between equivariant maps and certain invariant maps. This allows us to reduce arguments for equivariant maps to those for invariant maps and vice versa. As an application, we propose a construction of universal equivariant architectures built from universal invariant networks. We, in turn, explain how the universal architectures arising from our construction differ from standard equivariant architectures known to be universal. Furthermore, we explore the complexity, in terms of the number of free parameters, of our models, and discuss the relation between invariant and equivariant networks' complexity. Finally, we also give an approximation rate for G-equivariant deep neural networks with ReLU activation functions for finite group G. \u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff1a\u4eba\u7c7b vs. \u81ea\u76d1\u7763\u6a21\u578b \u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u8fdb\u884c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u4f46\u9488\u5bf9\u8de8\u8bed\u8a00\u573a\u666f\u7684\u7814\u7a76\u4ecd\u8f83\u4e3a\u6709\u9650\u3002\u672c\u7814\u7a76\u5bf9\u4eba\u7c7b\u8868\u73b0\u4e0eSSL\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u9996\u5148\u4ece\u9010\u5c42\u5206\u6790\u5165\u624b\uff0c\u63a2\u8ba8\u4e86\u5728\u5355\u8bed\u8a00\u3001\u8de8\u8bed\u8a00\u548c\u8fc1\u79fb\u5b66\u4e60\u60c5\u5883\u4e0b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u6bd4\u8f83\u4e86\u6a21\u578b\u548c\u4eba\u7c7b\u5728\u8bdd\u8bed\u7ea7\u548c\u5206\u6bb5\u7ea7\u4e0a\u7684SER\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u7814\u7a76\u4e86\u65b9\u8a00\u5bf9\u8de8\u8bed\u8a00SER\u7684\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9002\u5f53\u7684\u77e5\u8bc6\u8fc1\u79fb\u4e0b\uff0c\u6a21\u578b\u80fd\u591f\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u5e76\u8fbe\u5230\u4e0e\u6bcd\u8bed\u8005\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u65b9\u8a00\u5bf9\u6ca1\u6709\u5148\u524d\u8bed\u8a00\u548c\u526f\u8bed\u8a00\u80cc\u666f\u7684\u4e2a\u4f53\u7684SER\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u548c\u6a21\u578b\u5728\u4e0d\u540c\u60c5\u611f\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u7279\u5f81\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3aSSL\u6a21\u578b\u7684\u8de8\u8bed\u8a00SER\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u7a81\u663e\u4e86\u5b83\u4eec\u5728\u60c5\u611f\u611f\u77e5\u4e0a\u4e0e\u4eba\u7c7b\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u3002 Zhichen Han PDF N/A Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception. \u4f7f\u7528\u4ee4\u724c\u5185\u805a\u6027\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\u7684LLM\u751f\u6210\u6587\u672c \u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u4e0d\u65ad\u63d0\u5347\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u81ea\u52a8\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\u7684\u9700\u6c42\u65e5\u76ca\u51f8\u663e\u3002\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u56e0\u5176\u65e0\u9700\u8bad\u7ec3\u7684\u7279\u6027\uff0c\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u2014\u2014\u8bcd\u5143\u5185\u805a\u6027\uff0c\u8fd9\u5bf9\u96f6\u6837\u672c\u68c0\u6d4b\u975e\u5e38\u6709\u7528\uff0c\u5e76\u8bc1\u660eLLM\u751f\u6210\u7684\u6587\u672c\u5f80\u5f80\u6bd4\u4eba\u7c7b\u4e66\u5199\u7684\u6587\u672c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bcd\u5143\u5185\u805a\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86TOCSIN\uff0c\u4e00\u79cd\u901a\u7528\u7684\u53cc\u901a\u9053\u68c0\u6d4b\u8303\u5f0f\uff0c\u5229\u7528\u8bcd\u5143\u5185\u805a\u6027\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u6765\u6539\u8fdb\u73b0\u6709\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u3002\u4e3a\u4e86\u8ba1\u7b97\u8bcd\u5143\u5185\u805a\u6027\uff0cTOCSIN\u4ec5\u9700\u8981\u8fdb\u884c\u51e0\u8f6e\u968f\u673a\u8bcd\u5143\u5220\u9664\u548c\u8bed\u4e49\u5dee\u5f02\u6d4b\u91cf\uff0c\u8fd9\u4f7f\u5176\u7279\u522b\u9002\u7528\u4e8e\u751f\u6210\u6a21\u578b\u4e0d\u53ef\u8bbf\u95ee\u7684\u5b9e\u9645\u9ed1\u7bb1\u573a\u666f\u3002\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u6e90\u6a21\u578b\u548c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u68c0\u6d4b\u5668\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1a\\url{https://github.com/Shixuan-Ma/TOCSIN}\u3002 Shixuan Ma PDF N/A Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of LLM-generated text. Zero-shot detectors, due to their training-free nature, have received considerable attention and notable success. In this paper, we identify a new feature, token cohesiveness, that is useful for zero-shot detection, and we demonstrate that LLM-generated text tends to exhibit higher token cohesiveness than human-written text. Based on this observation, we devise TOCSIN, a generic dual-channel detection paradigm that uses token cohesiveness as a plug-and-play module to improve existing zero-shot detectors. To calculate token cohesiveness, TOCSIN only requires a few rounds of random token deletion and semantic difference measurement, making it particularly suitable for a practical black-box setting where the source model used for generation is not accessible. Extensive experiments with four state-of-the-art base detectors on various datasets, source models, and evaluation settings demonstrate the effectiveness and generality of the proposed approach. Code available at: \\url{https://github.com/Shixuan-Ma/TOCSIN}."},{"location":"biorxiv_papers/","title":"BioRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"},{"location":"medrxiv_papers/","title":"MedRxiv Papers","text":"\u6807\u9898 \u6458\u8981 \u4f5c\u8005 PDF\u94fe\u63a5 \u4ee3\u7801\u4ed3\u5e93 Title Abstract"}]}