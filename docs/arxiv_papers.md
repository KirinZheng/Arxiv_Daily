# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| 连接剧集与语义：一种理解长篇视频的新框架 | 尽管现有研究常常将长视频视为扩展的短视频，我们提出了一种新颖的方法，更准确地反映了人类的认知。本文介绍了BREASE：用于长视频理解的桥接情节与语义模型，该模型模拟情节记忆的积累以捕捉动作序列，并通过分散在视频中的语义知识对其进行强化。我们的工作有两个关键贡献：首先，我们开发了一种情节压缩器（ECO），能够从微观到半宏观层面高效地聚合关键表征。其次，我们提出了一种语义检索器（SeTR），通过聚焦于更广泛的上下文，用语义信息增强这些聚合表征，显著降低特征维度同时保留相关的宏观层面信息。大量实验表明，BREASE在零样本和完全监督设置下，在多个长视频理解基准测试中达到了最先进的性能。项目页面和代码位于：https://joslefaure.github.io/assets/html/hermes.html。 | Gueter Josmy Faure | [PDF](http://arxiv.org/pdf/2408.17443v1) | N/A | Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding | While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html. |
| SelectTTS：通过基于离散单元的帧选择合成任何人的声音 | 合成未见发言者的声音是多说话人文本到语音（TTS）中持续存在的挑战。大多数多说话人TTS模型依赖于在训练期间通过说话人调节来建模说话人特征。通过这种方法对未见说话人属性进行建模需要增加模型复杂度，这使得重现结果和改进结果变得具有挑战性。我们设计了一个简单的替代方案。我们提出了SelectTTS，一种新颖的方法，从目标说话人中选择适当的帧，并使用帧级自监督学习（SSL）特征进行解码。我们展示了这种方法可以有效地捕捉未见说话人的特征，并在客观和主观指标上与其它多说话人TTS框架取得可比的结果。通过SelectTTS，我们展示了从目标说话人的语音中选择帧是一种直接的方式，可以在低模型复杂度下实现对未见说话人的泛化。我们在说话人相似性性能上优于SOTA基线XTTS-v2和VALL-E，模型参数减少了8倍以上，训练数据减少了270倍。 | Ismail Rasim Ulgen | [PDF](http://arxiv.org/pdf/2408.17432v1) | N/A | SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection | Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data |
