# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| 桥接剧集与语义：一种理解长篇视频的新型框架 | 尽管现有研究常将长视频视为扩展的短视频，我们提出了一种更准确反映人类认知的新方法。本文介绍了BREASE：用于长视频理解的桥接剧集与语义（BRidging Episodes And SEmantics）模型，该模型模拟情景记忆积累以捕捉动作序列，并通过视频中分散的语义知识对其进行强化。我们的工作做出了两个关键贡献：首先，我们开发了一个情景压缩器（Episodic COmpressor，简称ECO），它能高效地从微观到半宏观层面聚合关键表征。其次，我们提出了一个语义检索器（Semantics reTRiever，简称SeTR），通过关注更广泛的上下文，用语义信息增强这些聚合表征，显著降低特征维度同时保留相关的宏观级别信息。广泛的实验表明，BREASE在多个长视频理解基准测试中，无论是零样本学习还是完全监督学习设置下，均达到了最先进的性能。项目页面和代码位于：https://joslefaure.github.io/assets/html/hermes.html。 | Gueter Josmy Faure | [PDF](http://arxiv.org/pdf/2408.17443v1) | N/A | Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding | While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html. |
| SelectTTS：通过基于离散单元的帧选择合成任何人的声音 | 合成未见说话者的声音是多说话者文本到语音（TTS）领域中一个持续的挑战。大多数多说话者TTS模型依赖于在训练过程中通过说话者调节来建模说话者特征。通过这种方法对未见说话者属性进行建模需要增加模型复杂度，这使得复现结果和改进结果变得困难。我们设计了一个简单的替代方案。我们提出了SelectTTS，一种新颖的方法，从目标说话者中选择适当的帧，并使用帧级自监督学习（SSL）特征进行解码。我们展示了这种方法可以有效地捕捉未见说话者的特征，并在客观和主观指标上与其他多说话者TTS框架取得可比的结果。通过SelectTTS，我们证明了从目标说话者的语音中选择帧是一种直接的方式，可以在模型复杂度较低的情况下实现对未见说话者的泛化。我们在说话者相似性性能上超过了SOTA基线XTTS-v2和VALL-E，模型参数减少了8倍以上，训练数据减少了270倍。 | Ismail Rasim Ulgen | [PDF](http://arxiv.org/pdf/2408.17432v1) | N/A | SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection | Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data |
