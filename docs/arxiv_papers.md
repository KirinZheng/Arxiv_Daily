# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| RoboTwin：配备生成式数字孪生的双臂机器人基准测试（早期版本） | 双臂机器人及其工具使用能力的有效协作在机器人技术的进步中变得越来越重要。这些技能在扩展机器人在多样化现实世界环境中的操作能力方面发挥着重要作用。然而，进展受到专门训练数据稀缺的阻碍。本文介绍了RoboTwin，这是一个新颖的基准数据集，结合了现实世界中的远程操作数据和数字孪生中的合成数据，专为双臂机器人场景设计。利用COBOT Magic平台，我们收集了关于工具使用和人与机器人互动的多样化数据。我们提出了一种创新的方法，使用AI生成内容来创建数字孪生，将2D图像转化为详细的3D模型。此外，我们利用大型语言模型生成专家级别的训练数据和面向功能的任务特定姿态序列。我们的主要贡献包括：1) RoboTwin基准数据集，2) 高效的现实到模拟管道，以及3) 使用语言模型进行自动专家级数据生成。这些进展旨在解决机器人训练数据的短缺问题，有望加速开发更强大和多功能的机器人系统，以广泛应用于现实世界。项目页面可在https://robotwin-benchmark.github.io/early-version/ 找到。 | Yao Mu | [PDF](http://arxiv.org/pdf/2409.02920v1) | N/A | RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) | Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications. The project page is available at https://robotwin-benchmark.github.io/early-version/ |
| 掩码扩散模型实际上是时间无关的掩码模型，并且利用了不准确的分类采样 | 掩码扩散模型（MDMs）由于其在离散数据生成建模中优于其他离散扩散模型的性能，已成为一个热门的研究课题，并在语言建模任务中与自回归模型（ARMs）展开竞争。最近在简化掩码扩散框架方面的努力进一步使其与连续空间扩散模型对齐，并提供了更原则化的训练和采样方法。然而，本文揭示了MDMs的训练和采样在理论上都不依赖于时间变量，这可以说是扩散模型的关键特征，而是等同于掩码模型。我们在采样方面的联系是通过我们提出的首次命中采样器（FHS）实现的。具体来说，我们证明了FHS在理论上等同于MDMs的原始生成过程，同时显著减轻了耗时的类别采样，实现了20倍的加速。此外，我们的研究挑战了之前关于MDMs在生成困惑度上能超越ARMs的说法。我们首次发现了一个潜在的数值问题，即使在32位浮点精度下，也会导致不准确的类别采样。我们表明，数值问题在理论和实证上都降低了有效温度，导致先前文献中对MDMs生成结果的不公平评估。 | Kaiwen Zheng | [PDF](http://arxiv.org/pdf/2409.02908v1) | N/A | Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling | Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\times$ speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature. |
| 机器学习中的拓扑方法：面向实践者的教程 | 拓扑机器学习（TML）是一个新兴领域，它利用代数拓扑学的技术来分析传统机器学习方法可能无法捕捉的复杂数据结构。本教程全面介绍了两种关键的TML技术：持久同调和Mapper算法，重点在于实际应用。持久同调捕捉多尺度的拓扑特征，如聚类、环和空洞，而Mapper算法则创建一个可解释的图，总结高维数据。为了提高可访问性，我们采用以数据为中心的方法，使读者能够亲身体验将这些技术应用于相关任务。我们提供了逐步解释、实现、动手示例和案例研究，以展示这些工具如何应用于现实世界的问题。目标是让研究人员和从业者掌握知识和资源，将TML融入他们的工作中，揭示传统机器学习方法常常隐藏的洞察。教程代码可在https://github.com/cakcora/TopologyForML获取。 | Baris Coskunuzer | [PDF](http://arxiv.org/pdf/2409.02901v1) | N/A | Topological Methods in Machine Learning: A Tutorial for Practitioners | Topological Machine Learning (TML) is an emerging field that leverages techniques from algebraic topology to analyze complex data structures in ways that traditional machine learning methods may not capture. This tutorial provides a comprehensive introduction to two key TML techniques, persistent homology and the Mapper algorithm, with an emphasis on practical applications. Persistent homology captures multi-scale topological features such as clusters, loops, and voids, while the Mapper algorithm creates an interpretable graph summarizing high-dimensional data. To enhance accessibility, we adopt a data-centric approach, enabling readers to gain hands-on experience applying these techniques to relevant tasks. We provide step-by-step explanations, implementations, hands-on examples, and case studies to demonstrate how these tools can be applied to real-world problems. The goal is to equip researchers and practitioners with the knowledge and resources to incorporate TML into their work, revealing insights often hidden from conventional machine learning methods. The tutorial code is available at https://github.com/cakcora/TopologyForML |
| LongCite：使大型语言模型能够在长上下文问答中生成细粒度的引用 | 尽管当前的长上下文大型语言模型（LLMs）在基于大量文本回答用户问题方面展示了令人印象深刻的能力，但其回答中缺乏引用使得用户难以验证，从而引发了对其可信度的担忧，因为它们可能产生幻觉。在这项工作中，我们的目标是使长上下文LLMs能够生成带有细粒度句子级引用的回答，从而提高其忠实度和可验证性。我们首先介绍了LongBench-Cite，这是一个用于评估当前LLMs在带有引用的长上下文问答（LQAC）中性能的自动化基准，揭示了显著的改进空间。为此，我们提出了CoF（从粗到细），一种新颖的流水线，利用现成的LLMs自动生成带有精确句子级引用的长上下文QA实例，并利用这一流水线构建了LongCite-45k，一个大规模的LQAC SFT数据集。最后，我们使用LongCite-45k数据集训练了LongCite-8B和LongCite-9B，成功地使它们能够在单次输出中生成准确的回答和细粒度的句子级引用。在LongBench-Cite上的评估结果显示，我们训练的模型在引用质量方面达到了最先进的水平，超过了包括GPT-4o在内的先进专有模型。 | jiajie Zhang | [PDF](http://arxiv.org/pdf/2409.02897v1) | N/A | LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA | Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o. |
| 区域数据驱动的天气模拟与全球拉伸网格 | 本文介绍了一种适用于区域天气预报应用的数据驱动模型（DDM）。该模型通过引入一种拉伸网格架构扩展了人工智能预报系统，该架构在感兴趣的区域提供更高分辨率，而在全球其他地方保持较低分辨率。该模型基于图神经网络，自然支持任意多分辨率网格配置。模型应用于北欧地区的短期天气预测，生成空间分辨率为2.5公里、时间分辨率为6小时的预报。模型先在31公里分辨率的43年全球ERA5数据上进行预训练，然后使用来自MetCoOp集合预报系统（MEPS）的3.3年2.5公里分辨率业务分析数据进行进一步细化。模型的性能通过挪威各地测量站的地面观测数据进行评估，并与MEPS的短期天气预报进行比较。DDM在2米温度预报方面优于MEPS的控制运行和集合平均值。模型还生成了具有竞争力的降水和风速预报，但显示出低估极端事件的趋势。 | Thomas Nils Nipen | [PDF](http://arxiv.org/pdf/2409.02891v1) | N/A | Regional data-driven weather modeling with a global stretched-grid | A data-driven model (DDM) suitable for regional weather forecasting applications is presented. The model extends the Artificial Intelligence Forecasting System by introducing a stretched-grid architecture that dedicates higher resolution over a regional area of interest and maintains a lower resolution elsewhere on the globe. The model is based on graph neural networks, which naturally affords arbitrary multi-resolution grid configurations.   The model is applied to short-range weather prediction for the Nordics, producing forecasts at 2.5 km spatial and 6 h temporal resolution. The model is pre-trained on 43 years of global ERA5 data at 31 km resolution and is further refined using 3.3 years of 2.5 km resolution operational analyses from the MetCoOp Ensemble Prediction System (MEPS). The performance of the model is evaluated using surface observations from measurement stations across Norway and is compared to short-range weather forecasts from MEPS. The DDM outperforms both the control run and the ensemble mean of MEPS for 2 m temperature. The model also produces competitive precipitation and wind speed forecasts, but is shown to underestimate extreme events. |
| LongLLaVA：通过混合架构高效地将多模态大语言模型扩展到1000张图像 | 扩展多模态大型语言模型（MLLMs）的长上下文能力对于视频理解、高分辨率图像理解和多模态代理至关重要。这涉及一系列系统优化，包括模型架构、数据构建和训练策略，特别是解决诸如\textit{图像增多时性能下降}和\textit{高计算成本}等挑战。在本文中，我们将模型架构调整为Mamba和Transformer块的混合体，采用多图像间时间与空间依赖性的数据构建方法，并采用渐进式训练策略。发布的模型\textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant)是首个混合MLLM，它在效率和效果之间实现了更好的平衡。LongLLaVA不仅在各种基准测试中取得了有竞争力的结果，而且保持了高吞吐量和低内存消耗。特别是，它可以在单个A100 80GB GPU上处理近千张图像，显示出在广泛任务中的应用前景。 | Xidong Wang | [PDF](http://arxiv.org/pdf/2409.02889v1) | N/A | LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture | Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \textit{degraded performance with more images} and \textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks. |
| 基准测试少样本图像分类器中的虚假偏差 | 少样本图像分类器旨在以最少的监督和有限的数据来识别和分类新数据，但通常表现出对类间和虚假属性之间的虚假关联的依赖，这种依赖被称为虚假偏差。虚假关联通常存在于某些样本中，少样本分类器可能会受到这些样本引起的虚假偏差的影响。目前缺乏一个自动化的基准测试系统来评估少样本分类器对虚假偏差的鲁棒性。在本文中，我们提出了一种系统且严格的基准测试框架，称为FewSTAB，用于公平地展示和量化少样本分类器对虚假偏差的多种鲁棒性程度。FewSTAB创建了带有偏差属性的少样本评估任务，以便使用这些任务进行预测可能会表现出较差的表现。为了构建这些任务，我们提出了一种基于预训练视觉语言模型的属性样本选择策略，从而消除了手动数据集整理的需求。这使得FewSTAB能够使用任何现有的测试数据自动进行虚假偏差的基准测试。FewSTAB不仅提供了新的评估维度，还为构建鲁棒分类器提供了新的设计指南。此外，它能够对不同程度的虚假偏差进行基准测试，并支持设计不同鲁棒性程度的需求。其有效性通过在三个数据集上对十种少样本学习方法的实验得到了验证。我们希望我们的框架能够激发对鲁棒少样本分类器的新设计。我们的代码可在https://github.com/gtzheng/FewSTAB获取。 | Guangtao Zheng | [PDF](http://arxiv.org/pdf/2409.02882v1) | N/A | Benchmarking Spurious Bias in Few-Shot Image Classifiers | Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB. |
| 可配置的基础模型：从模块化角度构建大型语言模型 | 大型语言模型（LLMs）的进步近期揭示了与其巨大参数需求相关的计算效率和持续可扩展性方面的挑战，使得这些模型在计算资源有限且需要多种能力的场景中的应用和演进日益复杂。受人类大脑模块化结构的启发，一种趋势是将LLMs分解为众多功能模块，允许部分模块进行推理，并通过动态组装模块来应对复杂任务，如专家混合模型（mixture-of-experts）。为了突出模块化方法的固有效率和可组合性，我们提出了“砖块”（brick）这一术语来代表每个功能模块，并将这种模块化结构称为可配置基础模型。本文全面概述和探讨了可配置基础模型的构建、应用及其局限性。我们首先将模块形式化为预训练阶段出现的“涌现砖块”——功能性神经分区，以及通过额外后训练构建的“定制砖块”——以提升LLMs的能力和知识。基于多样化的功能砖块，我们进一步提出了四种面向砖块的操作：检索与路由、合并、更新和扩展。这些操作使得LLMs能够根据指令动态配置以处理复杂任务。为验证我们的观点，我们对广泛使用的LLMs进行了实证分析，发现前馈神经网络（FFN）层遵循模块化模式，具有神经元的功能特化和功能性神经分区。最后，我们指出了几个开放问题和未来研究方向。总体而言，本文旨在为现有LLM研究提供一种新颖的模块化视角，并激发未来创建更高效、可扩展的基础模型的创新。 | Chaojun Xiao | [PDF](http://arxiv.org/pdf/2409.02877v1) | N/A | Configurable Foundation Models: Building LLMs from a Modular Perspective | Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models. |
| 城市驾驶混合模仿学习运动规划器 | 随着诸如nuPlan和Argoverse等开源数据集的发布，基于学习的规划器研究在过去几年中得到了广泛传播。现有系统在模仿人类驾驶员行为方面表现出色，但它们在保证安全闭环驾驶方面存在困难。相反，基于优化的规划器在短期规划场景中提供了更高的安全性。为了应对这一挑战，本文提出了一种新颖的混合运动规划器，它结合了基于学习和基于优化的技术。首先，一个多层感知器（MLP）生成类似于人类的轨迹，然后由一个基于优化的组件进行细化。该组件不仅最小化了跟踪误差，还计算出一条在运动学上可行且与障碍物和道路边界无碰撞的轨迹。我们的模型有效地平衡了安全性和人类相似性，缓解了这些目标之间的固有权衡。我们通过模拟实验验证了我们的方法，并通过在真实世界的自动驾驶车辆上部署进一步展示了其有效性。 | Cristian Gariboldi | [PDF](http://arxiv.org/pdf/2409.02871v1) | N/A | Hybrid Imitation-Learning Motion Planner for Urban Driving | With the release of open source datasets such as nuPlan and Argoverse, the research around learning-based planners has spread a lot in the last years. Existing systems have shown excellent capabilities in imitating the human driver behaviour, but they struggle to guarantee safe closed-loop driving. Conversely, optimization-based planners offer greater security in short-term planning scenarios. To confront this challenge, in this paper we propose a novel hybrid motion planner that integrates both learning-based and optimization-based techniques. Initially, a multilayer perceptron (MLP) generates a human-like trajectory, which is then refined by an optimization-based component. This component not only minimizes tracking errors but also computes a trajectory that is both kinematically feasible and collision-free with obstacles and road boundaries. Our model effectively balances safety and human-likeness, mitigating the trade-off inherent in these objectives. We validate our approach through simulation experiments and further demonstrate its efficacy by deploying it in real-world self-driving vehicles. |
| 深入了解用于时间序列分类的LITE深度学习方法 | 深度学习模型已被证明是时间序列分类（TSC）的强大解决方案。尽管最先进的架构在UCR和UEA档案上取得了有希望的结果，但它们具有大量的可训练参数。这可能导致训练时间长，二氧化碳排放量高，功耗大，并可能增加每秒浮点运算次数（FLOPS）。在本文中，我们提出了一种新的TSC架构，即带有增强技术的轻量级Inception（LITE），其参数数量仅为最先进的InceptionTime模型的2.34%，同时保持了性能。由于使用了深度可分离卷积（DWSC），该架构仅有9,814个可训练参数，并通过三种技术得到增强：多路复用、自定义滤波器和扩张卷积。在UCR上训练的LITE架构比InceptionTime快2.78倍，二氧化碳和功耗减少2.79倍。为了评估所提出架构在多元时间序列数据上的性能，我们调整了LITE以处理多元时间序列，我们称之为LITEMV。为了将理论应用于实际，我们还使用LITEMV对代表人类康复运动的多元时间序列进行了实验，结果表明LITEMV不仅是最有效的模型，而且在Kimore数据集上也是表现最佳的模型，该数据集是一个基于骨架的人类康复运动数据集。此外，为了解决LITEMV的可解释性问题，我们使用类激活图进行了一项研究，以理解模型在评估过程中做出的分类决策。 | Ali Ismail-Fawaz | [PDF](http://arxiv.org/pdf/2409.02869v1) | N/A | Look Into the LITE in Deep Learning for Time Series Classification | Deep learning models have been shown to be a powerful solution for Time Series Classification (TSC). State-of-the-art architectures, while producing promising results on the UCR and the UEA archives , present a high number of trainable parameters. This can lead to long training with high CO2 emission, power consumption and possible increase in the number of FLoating-point Operation Per Second (FLOPS). In this paper, we present a new architecture for TSC, the Light Inception with boosTing tEchnique (LITE) with only 2.34% of the number of parameters of the state-of-the-art InceptionTime model, while preserving performance. This architecture, with only 9, 814 trainable parameters due to the usage of DepthWise Separable Convolutions (DWSC), is boosted by three techniques: multiplexing, custom filters, and dilated convolution. The LITE architecture, trained on the UCR, is 2.78 times faster than InceptionTime and consumes 2.79 times less CO2 and power. To evaluate the performance of the proposed architecture on multivariate time series data, we adapt LITE to handle multivariate time series, we call this version LITEMV. To bring theory into application, we also conducted experiments using LITEMV on multivariate time series representing human rehabilitation movements, showing that LITEMV not only is the most efficient model but also the best performing for this application on the Kimore dataset, a skeleton based human rehabilitation exercises dataset. Moreover, to address the interpretability of LITEMV, we present a study using Class Activation Maps to understand the classification decision taken by the model during evaluation. |
| 构建一个可扩展、高效且可调控的搜索与排序平台 | 现代电子商务平台提供了海量的商品选择，使得顾客难以找到他们喜欢且与当前会话意图相关的商品。因此，电子商务平台拥有近乎实时的可扩展且适应性强的个性化排名和搜索系统至关重要。尽管科学文献中存在多种构建此类系统的方法，但由于复杂性和性能限制，许多方法并不适用于大规模工业应用。因此，工业排名系统通常采用计算效率高但简单的检索或候选生成方法，这些方法忽略了近乎实时的和异质的顾客信号，导致个性化和相关性体验较差。此外，相关的顾客体验由完全不同的系统提供，增加了复杂性、维护难度和不一致的体验。

本文介绍了一种个性化、适应性强的近乎实时排名平台，该平台可跨多种用例（如浏览和搜索）重复使用，并能够在高负载下（每秒数千次请求）为数百万商品和顾客提供服务。我们通过不同的排名层使用基于transformer的模型，这些模型能够直接从顾客行为序列中学习复杂的行为模式，同时能够整合时间（如会话内）和上下文信息。我们在大型在线电子商务平台上通过一系列全面的离线和在线真实世界实验验证了我们的系统，并展示了其在顾客体验和净收入方面相对于现有系统的优越性。最后，我们分享了从构建适用于大规模电子商务环境的全面、现代排名平台中获得的经验教训。 | Marjan Celikik | [PDF](http://arxiv.org/pdf/2409.02856v1) | N/A | Building a Scalable, Effective, and Steerable Search and Ranking Platform | Modern e-commerce platforms offer vast product selections, making it difficult for customers to find items that they like and that are relevant to their current session intent. This is why it is key for e-commerce platforms to have near real-time scalable and adaptable personalized ranking and search systems. While numerous methods exist in the scientific literature for building such systems, many are unsuitable for large-scale industrial use due to complexity and performance limitations. Consequently, industrial ranking systems often resort to computationally efficient yet simplistic retrieval or candidate generation approaches, which overlook near real-time and heterogeneous customer signals, which results in a less personalized and relevant experience. Moreover, related customer experiences are served by completely different systems, which increases complexity, maintenance, and inconsistent experiences.   In this paper, we present a personalized, adaptable near real-time ranking platform that is reusable across various use cases, such as browsing and search, and that is able to cater to millions of items and customers under heavy load (thousands of requests per second). We employ transformer-based models through different ranking layers which can learn complex behavior patterns directly from customer action sequences while being able to incorporate temporal (e.g. in-session) and contextual information. We validate our system through a series of comprehensive offline and online real-world experiments at a large online e-commerce platform, and we demonstrate its superiority when compared to existing systems, both in terms of customer experience as well as in net revenue. Finally, we share the lessons learned from building a comprehensive, modern ranking platform for use in a large-scale e-commerce environment. |
| 哎呀，我又采样了一次：重新解读少样本学习中的置信区间 | 在少样本学习（FSL）中计算置信区间（CI）的主要方法是通过有放回地采样任务，即允许相同的样本出现在多个任务中。这使得置信区间具有误导性，因为它考虑了采样器的随机性，但没有考虑数据本身的特性。为了量化这一问题的严重性，我们对有放回和无放回计算的置信区间进行了比较分析。结果显示，主流方法显著低估了置信区间。这一发现呼吁我们重新评估在FSL比较研究中如何解读置信区间及其得出的结论。我们的研究表明，使用配对检验可以在一定程度上解决这一问题。此外，我们还探索了通过策略性地采样特定大小的任务来进一步缩小（置信区间的）大小的方法。我们还引入了一个新的优化基准，可以在以下链接访问：https://github.com/RafLaf/FSL-benchmark-again。 | Raphael Lafargue | [PDF](http://arxiv.org/pdf/2409.02850v1) | N/A | Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning | The predominant method for computing confidence intervals (CI) in few-shot learning (FSL) is based on sampling the tasks with replacement, i.e.\ allowing the same samples to appear in multiple tasks. This makes the CI misleading in that it takes into account the randomness of the sampler but not the data itself. To quantify the extent of this problem, we conduct a comparative analysis between CIs computed with and without replacement. These reveal a notable underestimation by the predominant method. This observation calls for a reevaluation of how we interpret confidence intervals and the resulting conclusions in FSL comparative studies. Our research demonstrates that the use of paired tests can partially address this issue. Additionally, we explore methods to further reduce the (size of the) CI by strategically sampling tasks of a specific size. We also introduce a new optimized benchmark, which can be accessed at https://github.com/RafLaf/FSL-benchmark-again |
| SNNAX -- 在JAX中的脉冲神经网络 | 脉冲神经网络（SNN）模拟器是构建受生物启发的模型和神经形态硬件架构以及预测其性能的关键工具。对于这样的工具，易用性和灵活性至关重要，但模拟速度同样重要，尤其是在模拟SNN固有的复杂性时。在此，我们介绍SNNAX，这是一个基于JAX的框架，用于以类似于PyTorch的直观性和JAX的执行速度来模拟和训练此类模型。SNNAX模型易于扩展和定制，以适应所需的模型规格和目标神经形态硬件。此外，SNNAX提供了优化SNN训练和部署的关键功能，如灵活的自动微分和即时编译。我们评估并比较了SNNAX与其他常用的用于编程SNN的机器学习（ML）框架。我们提供了关键的性能指标、最佳实践、在SNNAX中模拟SNN的文档化示例，并实现了文献中使用的几个基准测试。 | Jamie Lohoff | [PDF](http://arxiv.org/pdf/2409.02842v1) | N/A | SNNAX -- Spiking Neural Networks in JAX | Spiking Neural Networks (SNNs) simulators are essential tools to prototype biologically inspired models and neuromorphic hardware architectures and predict their performance. For such a tool, ease of use and flexibility are critical, but so is simulation speed especially given the complexity inherent to simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating and training such models with PyTorch-like intuitiveness and JAX-like execution speed. SNNAX models are easily extended and customized to fit the desired model specifications and target neuromorphic hardware. Additionally, SNNAX offers key features for optimizing the training and deployment of SNNs such as flexible automatic differentiation and just-in-time compilation. We evaluate and compare SNNAX to other commonly used machine learning (ML) frameworks used for programming SNNs. We provide key performance metrics, best practices, documented examples for simulating SNNs in SNNAX, and implement several benchmarks used in the literature. |
| 使用类型和标记语言建模的历史德语文本规范化 | 历史拼写的变化对全文搜索或对历史数字化文本的自然语言处理构成了挑战。为了缩小历史正字法与现代拼写之间的差距，通常会追求对历史源材料进行自动正字法规范化。本报告提出了一种针对1700年至1900年间德语文学文本的规范化系统，该系统基于平行语料库进行训练。所提出的系统利用了基于Transformer语言模型的机器学习方法，结合了编码器-解码器模型来规范化单个词类，并使用预训练的因果语言模型在上下文中调整这些规范化。广泛的评估表明，所提出的系统提供了最先进的准确性，可与一个更大规模的端到端句子级规范化系统相媲美，该系统通过微调预训练的Transformer大型语言模型实现。然而，历史文本的规范化仍然是一个挑战，因为模型难以泛化，且缺乏大量高质量的平行数据。 | Anton Ehrmanntraut | [PDF](http://arxiv.org/pdf/2409.02841v1) | N/A | Historical German Text Normalization Using Type- and Token-Based Language Modeling | Historic variations of spelling poses a challenge for full-text search or natural language processing on historical digitized texts. To minimize the gap between the historic orthography and contemporary spelling, usually an automatic orthographic normalization of the historical source material is pursued. This report proposes a normalization system for German literary texts from c. 1700-1900, trained on a parallel corpus. The proposed system makes use of a machine learning approach using Transformer language models, combining an encoder-decoder model to normalize individual word types, and a pre-trained causal language model to adjust these normalizations within their context. An extensive evaluation shows that the proposed system provides state-of-the-art accuracy, comparable with a much larger fully end-to-end sentence-based normalization system, fine-tuning a pre-trained Transformer large language model. However, the normalization of historical text remains a challenge due to difficulties for models to generalize, and the lack of extensive high-quality parallel data. |
| R2GQA：检索器-阅读器-生成器问答系统，旨在帮助学生理解高等教育中的法律规章 | 在这篇文章中，我们提出了R2GQA系统，这是一个包含三个主要组件的检索器-阅读器-生成器问答系统：文档检索器、机器阅读器和答案生成器。检索器模块采用先进的信息检索技术，从法律法规文档数据集中提取文章的上下文。机器阅读器模块利用最先进的自然语言理解算法来理解检索到的文档并提取答案。最后，生成器模块将提取的答案综合成简洁且信息丰富的回答，以解答学生关于法律法规的问题。此外，我们在大学培训法规领域构建了ViRHE4QA数据集，包含9,758个问题-答案对，并经过了严格的构建过程。这是高等教育法规领域中首个包含多种类型答案（包括抽取式和生成式）的越南语数据集。此外，R2GQA系统是首个在越南语中提供生成式答案的系统。本文讨论了R2GQA系统在ViRHE4QA数据集上的每个模块的设计和实现，突出了它们的功能和交互。此外，我们展示了实验结果，证明了所提出系统在支持学生理解高等教育环境中的法律法规方面的有效性和实用性。总的来说，R2GQA系统和ViRHE4QA数据集有望对相关研究做出显著贡献，并帮助学生导航复杂的法律文件和法规，使他们能够做出明智的决策并有效遵守机构政策。我们的数据集可供研究使用。 | Phuc-Tinh Pham Do | [PDF](http://arxiv.org/pdf/2409.02840v1) | N/A | R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education | In this article, we propose the R2GQA system, a Retriever-Reader-Generator Question Answering system, consisting of three main components: Document Retriever, Machine Reader, and Answer Generator. The Retriever module employs advanced information retrieval techniques to extract the context of articles from a dataset of legal regulation documents. The Machine Reader module utilizes state-of-the-art natural language understanding algorithms to comprehend the retrieved documents and extract answers. Finally, the Generator module synthesizes the extracted answers into concise and informative responses to questions of students regarding legal regulations. Furthermore, we built the ViRHE4QA dataset in the domain of university training regulations, comprising 9,758 question-answer pairs with a rigorous construction process. This is the first Vietnamese dataset in the higher regulations domain with various types of answers, both extractive and abstractive. In addition, the R2GQA system is the first system to offer abstractive answers in Vietnamese. This paper discusses the design and implementation of each module within the R2GQA system on the ViRHE4QA dataset, highlighting their functionalities and interactions. Furthermore, we present experimental results demonstrating the effectiveness and utility of the proposed system in supporting the comprehension of students of legal regulations in higher education settings. In general, the R2GQA system and the ViRHE4QA dataset promise to contribute significantly to related research and help students navigate complex legal documents and regulations, empowering them to make informed decisions and adhere to institutional policies effectively. Our dataset is available for research purposes. |
| 通过大型语言模型的小样本学习，探索加密货币讨论中的情感动态和预测行为 | 本研究利用先进的自然语言处理技术，对加密货币相关讨论中的预测性陈述、希望言论和后悔情绪检测行为进行了分析。我们引入了一种名为“预测性陈述”的新分类方案，将评论分为预测性增长、预测性下降、预测性中性或非预测性类别。采用尖端的大型语言模型GPT-4o，我们探索了五种主要加密货币（Cardano、Binance、Matic、Fantom和Ripple）的情感动态。分析结果显示，预测性情感呈现出不同的模式，其中Matic表现出显著更高的乐观预测倾向。此外，我们还研究了希望和后悔情绪，揭示了这些情感与预测行为之间的微妙互动。尽管在数据量和资源可用性方面遇到限制，我们的研究仍报告了关于加密货币市场中投资者行为和情感趋势的宝贵发现，为战略决策和未来研究提供了信息。 | Moein Shahiki Tash | [PDF](http://arxiv.org/pdf/2409.02836v1) | N/A | Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models | This study performs analysis of Predictive statements, Hope speech, and Regret Detection behaviors within cryptocurrency-related discussions, leveraging advanced natural language processing techniques. We introduce a novel classification scheme named "Prediction statements," categorizing comments into Predictive Incremental, Predictive Decremental, Predictive Neutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge large language model, we explore sentiment dynamics across five prominent cryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysis reveals distinct patterns in predictive sentiments, with Matic demonstrating a notably higher propensity for optimistic predictions. Additionally, we investigate hope and regret sentiments, uncovering nuanced interplay between these emotions and predictive behaviors. Despite encountering limitations related to data volume and resource availability, our study reports valuable discoveries concerning investor behavior and sentiment trends within the cryptocurrency market, informing strategic decision-making and future research endeavors. |
| CMM-Math：一个中文多模态数学数据集，用于评估和增强大型多模态模型的数学推理能力 | 大型语言模型（LLMs）在数学推理方面取得了令人瞩目的成果，这是人类智能的基础技能之一。大多数先前的研究集中在基于文本数学推理数据集（如MATH、GSM8K）改进和衡量LLMs的性能。最近，一些研究者发布了英语多模态数学数据集（如MATHVISTA和MATH-V），以评估大型多模态模型（LMMs）的有效性。在本文中，我们发布了一个中文多模态数学（CMM-Math）数据集，包括基准测试和训练部分，以评估和提升LMMs的数学推理能力。CMM-Math包含超过28,000个高质量样本，涵盖了从小学到高中12个年级各种问题类型（如选择题、填空题等），并附有详细的解答。具体来说，视觉上下文可能出现在问题或解答中，这使得该数据集更具挑战性。通过综合分析，我们发现CMM-Math数据集上的最先进LMMs面临挑战，强调了进一步改进LMM开发的必要性。我们还提出了一种多模态数学LMM（Math-LMM），用于处理包含多个图像和文本片段的混合输入问题。我们通过三个阶段训练我们的模型，包括基础预训练、基础微调和数学微调。广泛的实验表明，通过与三个多模态数学数据集上的最先进LMMs进行比较，我们的模型有效地提高了数学推理性能。 | Wentao Liu | [PDF](http://arxiv.org/pdf/2409.02834v1) | N/A | CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models | Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets. |
| 黑曜石：针对安全机器学习加速器的协同状态空间探索以实现高效推理 | 可信执行环境（TEEs）对于机器学习加速器的性能提升在安全且高效的机器学习推理中是不可或缺的。通过状态空间探索对加速器架构进行优化，可以提升性能和能耗表现。然而，由于搜索空间庞大，此类探索既耗费资源又耗时。当前的研究不得不依赖快速分析模型，这些模型舍弃了关键的硬件细节以及硬件安全原语独有的跨层机会。尽管周期精确模型理论上能实现更优的设计，但其高昂的运行成本限制了它们只能在较小的状态空间内应用。

我们提出了Obsidian，这是一个优化框架，旨在为机器学习内核到安全机器学习加速器的映射找到最优解。Obsidian通过协同使用分析模型和周期精确模型来探索状态空间，从而应对上述挑战。其两大主要探索组件包括：(1) 一个包含安全硬件影响的加速器分析模型，在遍历大规模映射状态空间时生成最佳的m个模型映射；(2) 在周期精确模型上的编译器剖析步骤，捕捉运行时瓶颈以进一步优化执行时间、能耗和资源利用率，并找到最优的模型映射。

我们将Obsidian的结果与一个基线安全加速器进行了对比，该基线加速器整合了从GuardNN [33] 和 Sesame [11] 获取的最新安全方案。分析模型在云端部署中将推理延迟降低了20.5%，在边缘部署中降低了8.4%，同时分别实现了24%和19%的能耗改进。周期精确模型在此基础上，进一步将云端延迟降低了9.1%，边缘延迟降低了12.2%，并分别带来了13.8%和13.1%的能耗提升。 | Sarbartha Banerjee | [PDF](http://arxiv.org/pdf/2409.02817v1) | N/A | Obsidian: Cooperative State-Space Exploration for Performant Inference on Secure ML Accelerators | Trusted execution environments (TEEs) for machine learning accelerators are indispensable in secure and efficient ML inference. Optimizing workloads through state-space exploration for the accelerator architectures improves performance and energy consumption. However, such explorations are expensive and slow due to the large search space. Current research has to use fast analytical models that forego critical hardware details and cross-layer opportunities unique to the hardware security primitives. While cycle-accurate models can theoretically reach better designs, their high runtime cost restricts them to a smaller state space.   We present Obsidian, an optimization framework for finding the optimal mapping from ML kernels to a secure ML accelerator. Obsidian addresses the above challenge by exploring the state space using analytical and cycle-accurate models cooperatively. The two main exploration components include: (1) A secure accelerator analytical model, that includes the effect of secure hardware while traversing the large mapping state space and produce the best m model mappings; (2) A compiler profiling step on a cycle-accurate model, that captures runtime bottlenecks to further improve execution runtime, energy and resource utilization and find the optimal model mapping.   We compare our results to a baseline secure accelerator, comprising of the state-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11]. The analytical model reduces the inference latency by 20.5% for a cloud and 8.4% for an edge deployment with an energy improvement of 24% and 19% respectively. The cycle-accurate model, further reduces the latency by 9.1% for a cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%. |
| MMMU-Pro：一个更强大的多学科多模态理解基准 | 本文介绍了MMMU-Pro，这是大规模多学科多模态理解和推理（MMMU）基准的一个稳健版本。MMMU-Pro通过基于MMMU的三步流程，严格评估多模态模型的真正理解和推理能力：（1）过滤掉仅通过文本模型可回答的问题，（2）增加候选选项，（3）引入仅视觉输入设置，其中问题嵌入在图像中。这种设置挑战AI同时“看”和“读”，测试了无缝整合视觉和文本信息的基本人类认知技能。结果显示，模型在MMMU-Pro上的表现显著低于在MMMU上的表现，跨模型范围从16.8%到26.9%。我们探讨了OCR提示和思维链（CoT）推理的影响，发现OCR提示影响最小，而CoT通常能提高性能。MMMU-Pro提供了一个更严格的评估工具，紧密模拟现实世界场景，并为多模态AI的未来研究提供了宝贵的方向。 | Xiang Yue | [PDF](http://arxiv.org/pdf/2409.02813v1) | N/A | MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark | This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly "see" and "read" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI. |
| 提升时间序列分类中证书的鲁棒性：高效自集成方法 | 近期，时间序列领域的对抗鲁棒性问题引起了广泛关注。然而，现有的防御机制仍然有限，对抗训练是主要的方法，尽管它没有提供理论保证。随机平滑（Randomized Smoothing）因其能够在 $\ell_p$-ball 攻击下证明鲁棒性半径的可证明下界而脱颖而出。鉴于其成功，时间序列领域的研究开始关注这些方面。然而，现有研究主要集中在时间序列预测，或在时间序列分类（TSC）的统计特征增强中的非-$\ell_p$ 鲁棒性。我们的综述发现，随机平滑在 TSC 中表现一般，难以在鲁棒性较差的数据集上提供有效的保证。因此，我们提出了一种自集成方法，通过减少分类边际的方差来增强预测标签概率置信度的下界，从而证明更大的半径。该方法还解决了深度集成（DE）的计算开销问题，同时在鲁棒性方面保持竞争力，并在某些情况下超越了它。理论分析和实验结果都验证了我们方法的有效性，展示了在鲁棒性测试中优于基线方法的优越性能。 | Chang Dong | [PDF](http://arxiv.org/pdf/2409.02802v1) | N/A | Boosting Certificate Robustness for Time Series Classification with Efficient Self-Ensemble | Recently, the issue of adversarial robustness in the time series domain has garnered significant attention. However, the available defense mechanisms remain limited, with adversarial training being the predominant approach, though it does not provide theoretical guarantees. Randomized Smoothing has emerged as a standout method due to its ability to certify a provable lower bound on robustness radius under $\ell_p$-ball attacks. Recognizing its success, research in the time series domain has started focusing on these aspects. However, existing research predominantly focuses on time series forecasting, or under the non-$\ell_p$ robustness in statistic feature augmentation for time series classification~(TSC). Our review found that Randomized Smoothing performs modestly in TSC, struggling to provide effective assurances on datasets with poor robustness. Therefore, we propose a self-ensemble method to enhance the lower bound of the probability confidence of predicted labels by reducing the variance of classification margins, thereby certifying a larger radius. This approach also addresses the computational overhead issue of Deep Ensemble~(DE) while remaining competitive and, in some cases, outperforming it in terms of robustness. Both theoretical analysis and experimental results validate the effectiveness of our method, demonstrating superior performance in robustness testing compared to baseline approaches. |
| 迈向大语言模型偏好学习的统一视角：一项综述 | 大型语言模型（LLMs）展现出了极其强大的能力。实现成功的关键因素之一是使LLM的输出与人类偏好相一致。这一对齐过程通常只需要少量的数据就能有效地提升LLM的性能。尽管这种方法效果显著，但相关研究涉及多个领域，所涉及的方法相对复杂，不易理解。不同方法之间的关系尚未得到充分探索，这限制了偏好对齐的发展。鉴于此，我们将现有的流行对齐策略分解为不同的组成部分，并提供一个统一的框架来研究当前的对齐策略，从而在这些策略之间建立联系。在本综述中，我们将偏好学习中的所有策略分解为四个组成部分：模型、数据、反馈和算法。这种统一的视角不仅提供了对现有对齐算法的深入理解，还为不同策略的优势结合开辟了可能性。此外，我们提供了现有流行算法的详细工作示例，以帮助读者全面理解。最后，基于我们的统一视角，我们探讨了将大型语言模型与人类偏好对齐所面临的挑战和未来的研究方向。 | Bofei Gao | [PDF](http://arxiv.org/pdf/2409.02795v1) | N/A | Towards a Unified View of Preference Learning for Large Language Models: A Survey | Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences. |
| 从经验中“反学习”以避免虚假关联 | 尽管深度神经网络在许多任务中能够达到最先进的性能，但这些模型实际上比它们表现出来的更为脆弱。它们容易从训练数据中学习到虚假的相关性，从而导致令人惊讶的失败案例。在本文中，我们提出了一种新的方法来解决虚假相关性的问题：从经验中“遗忘”（UnLearning from Experience，简称ULE）。我们的方法基于使用两个并行训练的分类模型：学生模型和教师模型。两个模型接收相同的训练数据批次。学生模型在没有约束的情况下进行训练，并追求数据中的虚假相关性。教师模型则被训练来解决相同的分类问题，同时避免学生模型的错误。由于训练是并行进行的，学生模型对虚假相关性学得越好，教师模型就越鲁棒。教师模型利用学生输出相对于其输入的梯度来“遗忘”学生所犯的错误。我们在Waterbirds、CelebA、Spawrious和UrbanCars数据集上展示了我们方法的有效性。 | Jeff Mitchell | [PDF](http://arxiv.org/pdf/2409.02792v1) | N/A | UnLearning from Experience to Avoid Spurious Correlations | While deep neural networks can achieve state-of-the-art performance in many tasks, these models are more fragile than they appear. They are prone to learning spurious correlations in their training data, leading to surprising failure cases. In this paper, we propose a new approach that addresses the issue of spurious correlations: UnLearning from Experience (ULE). Our method is based on using two classification models trained in parallel: student and teacher models. Both models receive the same batches of training data. The student model is trained with no constraints and pursues the spurious correlations in the data. The teacher model is trained to solve the same classification problem while avoiding the mistakes of the student model. As training is done in parallel, the better the student model learns the spurious correlations, the more robust the teacher model becomes. The teacher model uses the gradient of the student's output with respect to its input to unlearn mistakes made by the student. We show that our method is effective on the Waterbirds, CelebA, Spawrious and UrbanCars datasets. |
| 具有领域适应性的正则化多输出高斯卷积过程 | 多输出高斯过程（MGP）作为一种建模多个输出的迁移学习方法，正受到越来越多的关注。尽管MGP具有高度的灵活性和通用性，但在应用于迁移学习时，仍面临两个关键挑战。第一个挑战是负迁移，即当输出之间不存在共享信息时发生的情况。第二个挑战是输入域不一致，这在迁移学习中常见，但在MGP中尚未得到探讨。本文提出了一种带有域适应的正则化MGP建模框架，以克服这些挑战。更具体地说，通过使用卷积过程，提出了一种稀疏的MGP协方差矩阵，其中加入了惩罚项，以自适应地选择最具信息量的输出进行知识迁移。为了处理域不一致问题，提出了一种域适应方法，通过边缘化不一致特征和扩展缺失特征来对齐不同输出之间的输入域。提供了所提出方法的统计特性，以保证其在实际应用和渐近情况下的性能。在综合模拟研究和陶瓷制造过程的一个真实案例研究中，所提出的框架优于最先进的基准方法。结果表明，我们的方法在处理负迁移和域不一致方面具有有效性。 | Wang Xinming | [PDF](http://arxiv.org/pdf/2409.02778v1) | N/A | Regularized Multi-output Gaussian Convolution Process with Domain Adaptation | Multi-output Gaussian process (MGP) has been attracting increasing attention as a transfer learning method to model multiple outputs. Despite its high flexibility and generality, MGP still faces two critical challenges when applied to transfer learning. The first one is negative transfer, which occurs when there exists no shared information among the outputs. The second challenge is the input domain inconsistency, which is commonly studied in transfer learning yet not explored in MGP. In this paper, we propose a regularized MGP modeling framework with domain adaptation to overcome these challenges. More specifically, a sparse covariance matrix of MGP is proposed by using convolution process, where penalization terms are added to adaptively select the most informative outputs for knowledge transfer. To deal with the domain inconsistency, a domain adaptation method is proposed by marginalizing inconsistent features and expanding missing features to align the input domains among different outputs. Statistical properties of the proposed method are provided to guarantee the performance practically and asymptotically. The proposed framework outperforms state-of-the-art benchmarks in comprehensive simulation studies and one real case study of a ceramic manufacturing process. The results demonstrate the effectiveness of our method in dealing with both the negative transfer and the domain inconsistency. |
| 将因果表征学习与不变性原则统一起来 | 因果表示学习旨在从高维观测中恢复潜在的因果变量，以解决因果下游任务，例如预测新干预措施的效果或进行更稳健的分类。已经开发了大量方法，每种方法都针对精心设计的问题设置，从而导致不同类型的可识别性。有一种普遍的看法是，这些不同的设置很重要，因为它们通常与Pearl因果层次结构的不同层次相关联，尽管并非所有都完全吻合。我们的主要贡献在于展示了现有许多因果表示学习方法在方法论上将表示与已知的数据对称性对齐。变量的识别是通过不同数据口袋之间的等价类来指导的，这些等价类不一定具有因果性。这一结果具有重要的意义，使我们能够将许多现有方法统一在一个单一的方法中，该方法可以根据与我们应用相关的恒定性混合和匹配不同的假设，包括非因果性的假设。这还大大提高了适用性，我们通过改进现实世界高维生态数据上的治疗效果估计来展示这一点。总的来说，本文阐明了因果假设在因果变量发现中的作用，并将重点转移到保留数据对称性上。 | Dingling Yao | [PDF](http://arxiv.org/pdf/2409.02772v1) | N/A | Unifying Causal Representation Learning with the Invariance Principle | Causal representation learning aims at recovering latent causal variables from high-dimensional observations to solve causal downstream tasks, such as predicting the effect of new interventions or more robust classification. A plethora of methods have been developed, each tackling carefully crafted problem settings that lead to different types of identifiability. The folklore is that these different settings are important, as they are often linked to different rungs of Pearl's causal hierarchy, although not all neatly fit. Our main contribution is to show that many existing causal representation learning approaches methodologically align the representation to known data symmetries. Identification of the variables is guided by equivalence classes across different data pockets that are not necessarily causal. This result suggests important implications, allowing us to unify many existing approaches in a single method that can mix and match different assumptions, including non-causal ones, based on the invariances relevant to our application. It also significantly benefits applicability, which we demonstrate by improving treatment effect estimation on real-world high-dimensional ecological data. Overall, this paper clarifies the role of causality assumptions in the discovery of causal variables and shifts the focus to preserving data symmetries. |
| 预训练与自训练的比较研究 | 预训练和自训练是两种半监督学习的方法。关于预训练和自训练的比较已有研究，但以往的研究结果令人困惑：在某些计算机视觉任务中，自训练优于预训练；而在某些自然语言处理任务中，预训练在不可比较的设置条件下优于自训练。我们提出了一种综合性的集成方法，以经验研究所有可行的训练范式，这些范式结合了预训练、自训练和微调，并在与数据增强可比的基础设置下进行。我们在六个数据集、四种数据增强方法以及情感分析和自然语言推理任务的不平衡数据上进行了实验。我们的研究结果证实，预训练和微调的范式总体表现最佳。此外，当与半监督预训练结合时，自训练并未带来额外的好处。 | Yiheng Wang | [PDF](http://arxiv.org/pdf/2409.02751v1) | N/A | A Comparative Study of Pre-training and Self-training | Pre-training and self-training are two approaches to semi-supervised learning. The comparison between pre-training and self-training has been explored. However, the previous works led to confusing findings: self-training outperforms pre-training experienced on some tasks in computer vision, and contrarily, pre-training outperforms self-training experienced on some tasks in natural language processing, under certain conditions of incomparable settings. We propose, comparatively and exhaustively, an ensemble method to empirical study all feasible training paradigms combining pre-training, self-training, and fine-tuning within consistent foundational settings comparable to data augmentation. We conduct experiments on six datasets, four data augmentation, and imbalanced data for sentiment analysis and natural language inference tasks. Our findings confirm that the pre-training and fine-tuning paradigm yields the best overall performances. Moreover, self-training offers no additional benefits when combined with semi-supervised pre-training. |
| 可处理的正则决策过程的离线学习 | 本研究探讨了在称为正则决策过程（RDPs）的一类非马尔可夫环境中进行离线强化学习（RL）的问题。在RDPs中，未来观测和奖励对过去交互的未知依赖性可以通过某个隐藏的有限状态自动机来捕捉。因此，许多RDP算法首先使用自动机学习技术来重建这种未知的依赖关系。本文表明，可以通过引入两种原创技术来克服先前RDP离线RL算法的两个强限制，特别是RegORL。这些技术包括：基于形式语言开发一种新的伪度量，消除了对$L_\infty^\mathsf{p}$-可区分性参数的依赖性问题；以及采用Count-Min-Sketch（CMS）而非简单的计数方法。前者减少了在语言理论术语中具有低复杂度环境所需的样本数量，后者缓解了长规划范围的内存需求。我们推导了与这些技术相关的PAC样本复杂度界限，并通过实验验证了该方法。 | Ahana Deb | [PDF](http://arxiv.org/pdf/2409.02747v1) | N/A | Tractable Offline Learning of Regular Decision Processes | This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we show that it is possible to overcome two strong limitations of previous offline RL algorithms for RDPs, notably RegORL. This can be accomplished via the introduction of two original techniques: the development of a new pseudometric based on formal languages, which removes a problematic dependency on $L_\infty^\mathsf{p}$-distinguishability parameters, and the adoption of Count-Min-Sketch (CMS), instead of naive counting. The former reduces the number of samples required in environments that are characterized by a low complexity in language-theoretic terms. The latter alleviates the memory requirements for long planning horizons. We derive the PAC sample complexity bounds associated to each of these techniques, and we validate the approach experimentally. |
| 卷积神经网络用于自动化元胞自动机分类 | 时空图中的细胞自动机（CA）的涌现动力学通常通过若干行为类别来组织。虽然基本CA的分类是可行的且已有深入研究，但非基本CA通常过于多样和繁多，难以手动进行详尽分类。在本章中，我们将时空图视为数字图像，并实施简单的计算机视觉技术，以自动将基本细胞自动机分类为五个Li-Packard类别。特别是，我们向卷积神经网络提出一个监督学习任务，使其能够推广到非基本CA。如果我们希望这样做，我们必须将算法的关注点从底层“微观”局部更新转移开。我们首先展示，先前开发的深度学习方法实际上已被训练用于识别局部更新规则，而不是直接关注与特定行为类别相关的中观模式。通过合理设计的神经网络架构以及多种数据增强技术，我们随后提出一个卷积神经网络，该网络几乎完美地识别行为类别，而不必首先识别底层微观动力学。 | Michiel Rollier | [PDF](http://arxiv.org/pdf/2409.02740v1) | N/A | Convolutional Neural Networks for Automated Cellular Automaton Classification | The emergent dynamics in spacetime diagrams of cellular automata (CAs) is often organised by means of a number of behavioural classes. Whilst classification of elementary CAs is feasible and well-studied, non-elementary CAs are generally too diverse and numerous to exhaustively classify manually. In this chapter we treat the spacetime diagram as a digital image, and implement simple computer vision techniques to perform an automated classification of elementary cellular automata into the five Li-Packard classes. In particular, we present a supervised learning task to a convolutional neural network, in such a way that it may be generalised to non-elementary CAs. If we want to do so, we must divert the algorithm's focus away from the underlying 'microscopic' local updates. We first show that previously developed deep learning approaches have in fact been trained to identify the local update rule, rather than directly focus on the mesoscopic patterns that are associated with the particular behavioural classes. By means of a well-argued neural network design, as well as a number of data augmentation techniques, we then present a convolutional neural network that performs nearly perfectly at identifying the behavioural class, without necessarily first identifying the underlying microscopic dynamics. |
| 完整且高效的3D点配置协变量及其在分子量子性质学习中的应用 | 在利用机器学习模型化分子的物理性质时，引入$SO(3)$协变性是可取的。尽管基于低体阶特征的此类模型并不完备，我们为高阶方法制定了并证明了普遍的完备性性质，并表明这些特征中的$6k-5$个足以涵盖最多$k$个原子。我们还发现，这些方法中常用的Clebsch--Gordan运算可以被矩阵乘法所取代，而不会牺牲完备性，从而将特征度数的缩放从$O(l^6)$降低到$O(l^3)$。我们将此应用于量子化学，但所提出的方法普遍适用于涉及三维点配置的问题。 | Hartmut Maennel | [PDF](http://arxiv.org/pdf/2409.02730v1) | N/A | Complete and Efficient Covariants for 3D Point Configurations with Application to Learning Molecular Quantum Properties | When modeling physical properties of molecules with machine learning, it is desirable to incorporate $SO(3)$-covariance. While such models based on low body order features are not complete, we formulate and prove general completeness properties for higher order methods, and show that $6k-5$ of these features are enough for up to $k$ atoms. We also find that the Clebsch--Gordan operations commonly used in these methods can be replaced by matrix multiplications without sacrificing completeness, lowering the scaling from $O(l^6)$ to $O(l^3)$ in the degree of the features. We apply this to quantum chemistry, but the proposed methods are generally applicable for problems involving 3D point configurations. |
| 面向任务的图数据通信：一种图信息瓶颈方法 | 图数据在知识表示和社会网络等领域至关重要，通常涉及具有大量节点和边的庞大网络。由于其规模和特定任务中的冗余性，传输这些图数据可能效率极低。本文介绍了一种方法，用于提取一个更小、专注于任务的子图，该子图在减少通信开销的同时保留关键信息。我们的方法利用图神经网络（GNNs）和图信息瓶颈（GIB）原理，创建适合传输的紧凑、信息丰富且稳健的图表示。挑战在于图数据的不规则结构，使得GIB优化变得复杂。我们通过推导目标函数的一个可处理的变分上界来解决这一问题。此外，我们提出了VQ-GIB机制，结合向量量化（VQ）将子图表示转换为离散的码本序列，与现有的数字通信系统兼容。我们的实验表明，这种基于GIB的方法显著降低了通信成本，同时保留了与任务相关的关键信息。该方法在各种通信信道上表现出稳健的性能，适用于连续和离散系统。 | Shujing Li | [PDF](http://arxiv.org/pdf/2409.02728v1) | N/A | Task-Oriented Communication for Graph Data: A Graph Information Bottleneck Approach | Graph data, essential in fields like knowledge representation and social networks, often involves large networks with many nodes and edges. Transmitting these graphs can be highly inefficient due to their size and redundancy for specific tasks. This paper introduces a method to extract a smaller, task-focused subgraph that maintains key information while reducing communication overhead. Our approach utilizes graph neural networks (GNNs) and the graph information bottleneck (GIB) principle to create a compact, informative, and robust graph representation suitable for transmission. The challenge lies in the irregular structure of graph data, making GIB optimization complex. We address this by deriving a tractable variational upper bound for the objective function. Additionally, we propose the VQ-GIB mechanism, integrating vector quantization (VQ) to convert subgraph representations into a discrete codebook sequence, compatible with existing digital communication systems. Our experiments show that this GIB-based method significantly lowers communication costs while preserving essential task-related information. The approach demonstrates robust performance across various communication channels, suitable for both continuous and discrete systems. |
| 池化和注意力：基于大型语言模型的嵌入模型有哪些有效设计？ | 大型语言模型（LLMs）在生成任务中的显著进展，催生了大量基于LLM的嵌入模型研究。尽管这些模型通过采用不同的池化和注意力策略，在公开的嵌入基准测试中达到了最先进的性能，但关于什么构成了有效的LLM嵌入模型设计的疑问依然存在。然而，这些模型往往在不同的数据集上进行训练，使用不同的LLM基础模型或训练设置。此外，公开嵌入基准测试中的评估往往未能报告统计显著性，这使得确定哪些设计真正有助于最终性能变得困难。这给寻求优化LLM嵌入模型训练方案的从业者带来了复杂性。在本研究中，我们通过使用相同的训练数据和基础模型，但采用不同的池化和注意力策略，训练了一系列LLM嵌入模型，进行了一次大规模实验。结果显示，没有一种策略是万能的：尽管双向注意力和额外的可训练池化层在文本相似性和信息检索任务中表现优异，但在聚类和分类任务中，它们并未显著超越像EOS-last token池化和默认因果注意力这样的简单设计。此外，我们提出了一种新的池化策略——多层可训练池化，该策略通过交叉注意力网络转换所有隐藏层的输出，而不仅仅是最后一层。这种方法在文本相似性和检索任务中，相较于现有的池化方法，显示出统计上的优越性。总体而言，本文揭示了LLM嵌入模型的有效训练策略。 | Yixuan Tang | [PDF](http://arxiv.org/pdf/2409.02727v1) | N/A | Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models? | The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models. |
| 利用期刊影响指标进行生物医学领域适应的预训练数据选择 | 领域适应是自然语言处理（NLP）中广泛使用的方法，旨在提升语言模型在特定领域的表现。这种方法在生物医学领域尤为常见，该领域定期发布大量科学文章。PubMed作为一个重要的文本语料库，在生物医学领域中经常被使用。本研究的主要目标是探讨通过使用特定的质量指标对预训练数据集进行优化，是否能提升最终模型的性能。为此，我们采用了两种简单的期刊影响指标，并通过在PubMed训练集的不同子集上持续预训练BERT，进行了一系列实验。随后，我们在BLURB基准的生物医学语言理解任务上评估了所得模型。研究结果表明，使用期刊影响指标进行剪枝并不高效。但我们也发现，使用较少的摘要进行预训练（但保持相同的训练步骤）并不一定会降低最终模型的性能。 | Mathieu Laï-king | [PDF](http://arxiv.org/pdf/2409.02725v1) | N/A | Pre-training data selection for biomedical domain adaptation using journal impact metrics | Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular publication of numerous scientific articles. PubMed, a significant corpus of text, is frequently used in the biomedical domain. The primary objective of this study is to explore whether refining a pre-training dataset using specific quality metrics for scientific papers can enhance the performance of the resulting model. To accomplish this, we employ two straightforward journal impact metrics and conduct experiments by continually pre-training BERT on various subsets of the complete PubMed training set, we then evaluate the resulting models on biomedical language understanding tasks from the BLURB benchmark. Our results show that pruning using journal impact metrics is not efficient. But we also show that pre-training using fewer abstracts (but with the same number of training steps) does not necessarily decrease the resulting model's performance. |
| 针对大型语言模型的对齐感知模型提取攻击 | 针对大型语言模型（LLMs）的模型提取攻击（MEAs）近期受到了越来越多的研究关注。现有的对LLMs的攻击方法继承了为深度神经网络（DNNs）设计的提取策略，但却忽略了MEA与LLMs对齐任务之间的不一致性，从而导致攻击效果不佳。为了解决这一问题，我们提出了局部强化蒸馏（Locality Reinforced Distillation, LoRD），这是一种专为LLMs设计的新型模型提取攻击算法。具体而言，我们设计了一种策略梯度风格训练任务，利用受害模型的响应作为信号来指导局部模型偏好的构建。理论分析表明：i) LoRD在MEA中的收敛过程与LLMs的对齐一致；ii) LoRD通过基于探索的窃取，能够在降低查询复杂度的同时缓解水印保护。在特定领域的提取实验中，通过考察对各种最先进的商业LLMs的提取效果，证明了我们方法的优越性。 | Zi Liang | [PDF](http://arxiv.org/pdf/2409.02718v1) | N/A | Alignment-Aware Model Extraction Attacks on Large Language Models | Model extraction attacks (MEAs) on large language models (LLMs) have received increasing research attention lately. Existing attack methods on LLMs inherit the extraction strategies from those designed for deep neural networks (DNNs) yet neglect the inconsistency of training tasks between MEA and LLMs' alignments. As such, they result in poor attack performances. To tackle this issue, we present Locality Reinforced Distillation (LoRD), a novel model extraction attack algorithm specifically for LLMs. In particular, we design a policy-gradient-style training task, which utilizes victim models' responses as a signal to guide the crafting of preference for the local model. Theoretical analysis has shown that i) LoRD's convergence procedure in MEAs is consistent with the alignments of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions demonstrate the superiority of our method by examining the extraction of various state-of-the-art commercial LLMs. |
| 一种利用跨语言句子表示提升低资源机器翻译的数据选择方法 | 低资源语言对的机器翻译面临显著挑战，主要源于平行语料库和语言资源的匮乏。本研究聚焦于英语-马拉地语对的案例，现有数据集存在显著噪音，严重影响了机器翻译模型的性能。为缓解数据质量问题的影响，我们提出了一种基于跨语言句子表示的数据过滤方法。我们的方法利用多语言SBERT模型来筛选训练数据中的问题翻译。具体而言，我们采用IndicSBERT相似性模型评估原始句子和翻译句子之间的语义等价性，从而保留语言上正确的翻译，同时剔除存在重大偏差的实例。结果表明，使用IndicSBERT进行过滤后，翻译质量相较于基线有显著提升。这展示了跨语言句子表示如何在资源有限的机器翻译场景中减少错误。通过将多语言句子BERT模型整合到翻译流程中，本研究推动了低资源环境下机器翻译技术的进步。所提出的方法不仅解决了英语-马拉地语对的挑战，还为其他低资源语言翻译任务提升翻译质量提供了宝贵的框架。 | Nidhi Kowtal | [PDF](http://arxiv.org/pdf/2409.02712v1) | N/A | A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations | Machine translation in low-resource language pairs faces significant challenges due to the scarcity of parallel corpora and linguistic resources. This study focuses on the case of English-Marathi language pairs, where existing datasets are notably noisy, impeding the performance of machine translation models. To mitigate the impact of data quality issues, we propose a data filtering approach based on cross-lingual sentence representations. Our methodology leverages a multilingual SBERT model to filter out problematic translations in the training data. Specifically, we employ an IndicSBERT similarity model to assess the semantic equivalence between original and translated sentences, allowing us to retain linguistically correct translations while discarding instances with substantial deviations. The results demonstrate a significant improvement in translation quality over the baseline post-filtering with IndicSBERT. This illustrates how cross-lingual sentence representations can reduce errors in machine translation scenarios with limited resources. By integrating multilingual sentence BERT models into the translation pipeline, this research contributes to advancing machine translation techniques in low-resource environments. The proposed method not only addresses the challenges in English-Marathi language pairs but also provides a valuable framework for enhancing translation quality in other low-resource language translation tasks. |
| 少样本多任务学习线性不变特征与元子空间追踪 | 数据稀缺对现代机器学习和人工智能构成了严重威胁，因为它们的实际成功通常依赖于大数据集的可用性。缓解数据不足问题的一个有效策略是，首先在研究设计阶段利用来自其他在研究设计上具有一定相似性的数据源的信息，然后在分析阶段采用多任务或元学习框架。本文重点研究了多任务（或多源）线性模型，这些模型的系数在不同任务之间共享一个不变的低秩分量，这是近期多任务或元学习文献中考虑的一种流行结构假设。在此假设下，我们提出了一种新的算法，称为元子空间追踪（简称Meta-SP），该算法能够证明性地学习不同任务共享的不变子空间。在多任务或元学习的这种理想化设置下，我们建立了所提出方法的算法和统计保证。进行了广泛的数值实验，将Meta-SP与几种竞争方法进行了比较，包括流行的、现成的模型不可知元学习算法，如ANIL。这些实验表明，Meta-SP在各个方面都优于竞争方法。 | Chaozhi Zhang | [PDF](http://arxiv.org/pdf/2409.02708v1) | N/A | Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit | Data scarcity poses a serious threat to modern machine learning and artificial intelligence, as their practical success typically relies on the availability of big datasets. One effective strategy to mitigate the issue of insufficient data is to first harness information from other data sources possessing certain similarities in the study design stage, and then employ the multi-task or meta learning framework in the analysis stage. In this paper, we focus on multi-task (or multi-source) linear models whose coefficients across tasks share an invariant low-rank component, a popular structural assumption considered in the recent multi-task or meta learning literature. Under this assumption, we propose a new algorithm, called Meta Subspace Pursuit (abbreviated as Meta-SP), that provably learns this invariant subspace shared by different tasks. Under this stylized setup for multi-task or meta learning, we establish both the algorithmic and statistical guarantees of the proposed method. Extensive numerical experiments are conducted, comparing Meta-SP against several competing methods, including popular, off-the-shelf model-agnostic meta learning algorithms such as ANIL. These experiments demonstrate that Meta-SP achieves superior performance over the competing methods in various aspects. |
| 用于增强作业车间调度问题中神经局部搜索的决策变换器 | 作业车间调度问题（JSSP）及其求解算法多年来在学术界和工业界一直备受关注。近年来，机器学习（ML）在推进现有和构建新的JSSP启发式解决方案方面发挥着越来越重要的作用，旨在在更短的计算时间内找到更好的解决方案。在本文中，我们基于一种最先进的深度强化学习（DRL）代理，称为神经局部搜索（NLS），该代理能够高效且有效地控制JSSP上的大规模局部邻域搜索。特别地，我们开发了一种方法，通过训练有素的NLS代理所采取的搜索轨迹来训练决策转换器（DT）算法，以进一步改进所学到的决策序列。我们的实验表明，DT成功地学习了与NLS代理不同的局部搜索策略，并且在许多情况下，这些策略更为有效。在解决方案质量和可接受的搜索所需计算时间之间的权衡方面，DT在允许更长计算时间的应用场景中表现尤为优越。在这种情况下，它通过每一步更好的决策质量弥补了每个搜索步骤所需更长的推理时间，这是由于更大的神经网络架构所导致的。因此，DT在ML增强搜索的JSSP求解中实现了最先进的结果。 | Constantin Waubert de Puiseau | [PDF](http://arxiv.org/pdf/2409.02697v1) | N/A | Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling Problem | The job shop scheduling problem (JSSP) and its solution algorithms have been of enduring interest in both academia and industry for decades. In recent years, machine learning (ML) is playing an increasingly important role in advancing existing and building new heuristic solutions for the JSSP, aiming to find better solutions in shorter computation times. In this paper we build on top of a state-of-the-art deep reinforcement learning (DRL) agent, called Neural Local Search (NLS), which can efficiently and effectively control a large local neighborhood search on the JSSP. In particular, we develop a method for training the decision transformer (DT) algorithm on search trajectories taken by a trained NLS agent to further improve upon the learned decision-making sequences. Our experiments show that the DT successfully learns local search strategies that are different and, in many cases, more effective than those of the NLS agent itself. In terms of the tradeoff between solution quality and acceptable computational time needed for the search, the DT is particularly superior in application scenarios where longer computational times are acceptable. In this case, it makes up for the longer inference times required per search step, which are caused by the larger neural network architecture, through better quality decisions per step. Thereby, the DT achieves state-of-the-art results for solving the JSSP with ML-enhanced search. |
| 检测多模态内容中的行动呼吁：对2021年德国联邦选举在Instagram上的竞选活动分析 | 本研究探讨了2021年德国Instagram选举活动中行动号召（CTAs）的自动化分类，以增进对社交媒体动员策略的理解。我们使用经过微调的BERT模型和OpenAI的GPT-4模型，分析了超过2,208个Instagram故事和712篇帖子。经过合成训练数据微调的BERT模型达到了0.93的宏观F1分数，显示出强大的分类性能。分析结果显示，49.58%的Instagram帖子和10.64%的故事包含CTAs，突显了这两种内容类型在动员策略上的显著差异。此外，我们发现FDP和绿党在帖子中CTAs的普遍性最高，而CDU和CSU则在故事中的CTAs占比领先。 | Michael Achmann-Denkler | [PDF](http://arxiv.org/pdf/2409.02690v1) | N/A | Detecting Calls to Action in Multimodal Content: Analysis of the 2021 German Federal Election Campaign on Instagram | This study investigates the automated classification of Calls to Action (CTAs) within the 2021 German Instagram election campaign to advance the understanding of mobilization in social media contexts. We analyzed over 2,208 Instagram stories and 712 posts using fine-tuned BERT models and OpenAI's GPT-4 models. The fine-tuned BERT model incorporating synthetic training data achieved a macro F1 score of 0.93, demonstrating a robust classification performance. Our analysis revealed that 49.58% of Instagram posts and 10.64% of stories contained CTAs, highlighting significant differences in mobilization strategies between these content types. Additionally, we found that FDP and the Greens had the highest prevalence of CTAs in posts, whereas CDU and CSU led in story CTAs. |
| 去混淆因果感知参数高效微调以提升大型语言模型的问题解决能力 | 大型语言模型（LLMs）在根据人类指令处理各种任务方面展示了显著的效率，但最近的研究表明，这些模型在涉及推理的问题上，如数学或物理问题，往往无法取得令人满意的结果。这种现象通常归因于这些模型是否真正理解了嵌入在文本中的知识，还是仅仅学会了复制令牌分布而没有真正理解内容的不确定性。在本文中，我们深入探讨了这个问题，并旨在提升LLMs的推理能力。首先，我们通过在注意力和表示层面可视化文本生成过程，来探究模型是否具有真正的推理能力。接着，我们将LLMs的推理过程形式化为一个因果框架，为我们在可视化中观察到的问题提供了一个正式的解释。最后，基于这个因果框架，我们提出了去混淆因果适应（Deconfounded Causal Adaptation, DCA），这是一种新颖的参数高效微调（PEFT）方法，通过鼓励模型提取一般的问题解决技能并将其应用于不同问题，来增强模型的推理能力。实验表明，我们的方法在多个基准测试中始终优于基线，并且仅用1.2M可调参数，我们就能取得优于或与其他微调方法相当的结果。这证明了我们的方法在提高LLMs整体准确性和可靠性方面的有效性和效率。 | Ruoyu Wang | [PDF](http://arxiv.org/pdf/2409.02686v1) | N/A | Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs | Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but recent studies reveal that these models often fail to achieve satisfactory results on questions involving reasoning, such as mathematics or physics questions. This phenomenon is usually attributed to the uncertainty regarding whether these models could genuinely comprehend the knowledge embedded in the text or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs. |
| 从计算角度看神经时间尺度 | 神经活动的时间尺度在不同脑区和同一脑区内部都表现出多样性，实验观察表明神经时间尺度反映了动态环境中的信息。然而，这些观察并未明确指出神经时间尺度是如何形成的，也没有说明特定的时间尺度是否对神经计算和脑功能是必要的。在此，我们从一个互补的角度出发，综合了计算方法的三个方向，将广泛的经验观察提炼成定量且可验证的理论：我们回顾了（i）数据分析方法如何让我们捕捉不同记录模式下神经动力学的不同时间尺度，（ii）计算模型如何为多样时间尺度的出现提供机制性解释，以及（iii）机器学习中的任务优化模型如何揭示神经时间尺度的功能相关性。这种综合的计算方法，结合实证发现，将提供一个更全面的理解，即神经时间尺度如何捕捉脑结构、动态和行为之间的关系。 | Roxana Zeraati | [PDF](http://arxiv.org/pdf/2409.02684v1) | N/A | Neural timescales from a computational perspective | Timescales of neural activity are diverse across and within brain areas, and experimental observations suggest that neural timescales reflect information in dynamic environments. However, these observations do not specify how neural timescales are shaped, nor whether particular timescales are necessary for neural computations and brain function. Here, we take a complementary perspective and synthesize three directions where computational methods can distill the broad set of empirical observations into quantitative and testable theories: We review (i) how data analysis methods allow us to capture different timescales of neural dynamics across different recording modalities, (ii) how computational models provide a mechanistic explanation for the emergence of diverse timescales, and (iii) how task-optimized models in machine learning uncover the functional relevance of neural timescales. This integrative computational approach, combined with empirical findings, would provide a more holistic understanding of how neural timescales capture the relationship between brain structure, dynamics, and behavior. |
| 使用LSTM和GRU神经网络在亚马逊地区建模活跃火灾 | 本研究提出了一种综合方法，用于建模和预测巴西亚马逊地区由AQUA_M-T卫星检测到的火点历史时间序列。该方法采用混合循环神经网络（RNN）模型，结合长短期记忆（LSTM）和门控循环单元（GRU）架构，以预测每日检测到的火点的月度累积量。数据概述显示，随着时间的推移，存在一致的季节性，每年的火点最大值和最小值往往在同一时期重复出现。主要目标是验证预测是否通过严格的统计分析捕捉到这种固有的季节性。该方法包括仔细的数据准备、模型配置和使用两个种子的交叉验证进行训练，确保数据能够很好地泛化到测试和验证集，并确认模型参数的收敛性。结果表明，混合LSTM和GRU模型在提前12个月的预测中提供了更高的准确性，展示了其在捕捉复杂时间模式和建模观测时间序列方面的有效性。这项研究显著促进了深度学习技术在环境监测中的应用，特别是在火点预测方面。除了提高预测准确性外，所提出的方法还突显了适应其他时间序列预测挑战的潜力，为机器学习和自然现象预测的研究和开发开辟了新的途径。关键词：时间序列预测，循环神经网络，深度学习。 | Ramon Tavares | [PDF](http://arxiv.org/pdf/2409.02681v1) | N/A | Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon | This study presents a comprehensive methodology for modeling and forecasting the historical time series of fire spots detected by the AQUA_M-T satellite in the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures to predict monthly accumulations of daily detected fire spots. A summary of the data revealed a consistent seasonality over time, with annual maximum and minimum fire spot values tending to repeat at the same periods each year. The primary objective is to verify whether the forecasts capture this inherent seasonality through rigorous statistical analysis. The methodology involved careful data preparation, model configuration, and training using cross-validation with two seeds, ensuring that the data generalizes well to the test and validation sets, and confirming the convergence of the model parameters. The results indicate that the mixed LSTM and GRU model offers improved accuracy in forecasting 12 months ahead, demonstrating its effectiveness in capturing complex temporal patterns and modeling the observed time series. This research significantly contributes to the application of deep learning techniques in environmental monitoring, specifically in fire spot forecasting. In addition to improving forecast accuracy, the proposed approach highlights the potential for adaptation to other time series forecasting challenges, opening new avenues for research and development in machine learning and natural phenomenon prediction. Keywords: Time Series Forecasting, Recurrent Neural Networks, Deep Learning. |
| 基于超声传感器和速率编码的低成本实时尖峰障碍物检测系统 | 自移动机器人问世以来，障碍物检测一直是一个备受关注的课题。在神经科学领域，障碍物检测也是一个研究对象，其中飞行的昆虫和蝙蝠分别被认为是基于视觉和基于声音的障碍物检测机制中最有趣的两个案例。目前，许多研究集中在基于视觉的障碍物检测上，但关于基于声音的障碍物检测的研究却不多见。本研究专注于后者，并利用脉冲神经网络来发挥这些架构的优势，实现更接近生物学的方法。通过一系列实验测试了整个系统，确认了脉冲架构在障碍物检测中的有效性。实证表明，当机器人与障碍物之间的距离减小时，系统的输出放电率如预期般增加，反之亦然。因此，这两者之间存在直接关系。此外，本研究还实证测量了可检测与不可检测物体之间的距离阈值。对基于脉冲间间隔概念的低层次系统工作原理进行了深入研究，这可能对未来基于脉冲滤波器的应用开发有所帮助。 | Alvaro Ayuso-Martinez | [PDF](http://arxiv.org/pdf/2409.02680v1) | N/A | A Low-Cost Real-Time Spiking System for Obstacle Detection based on Ultrasonic Sensors and Rate Coding | Since the advent of mobile robots, obstacle detection has been a topic of great interest. It has also been a subject of study in neuroscience, where flying insects and bats could be considered two of the most interesting cases in terms of vision-based and sound-based mechanisms for obstacle detection, respectively. Currently, many studies focus on vision-based obstacle detection, but not many can be found regarding sound-based obstacle detection. This work focuses on the latter approach, which also makes use of a Spiking Neural Network to exploit the advantages of these architectures and achieve an approach closer to biology. The complete system was tested through a series of experiments that confirm the validity of the spiking architecture for obstacle detection. It is empirically demonstrated that, when the distance between the robot and the obstacle decreases, the output firing rate of the system increases in response as expected, and vice versa. Therefore, there is a direct relation between the two. Furthermore, there is a distance threshold between detectable and undetectable objects which is also empirically measured in this work. An in-depth study on how this system works at low level based on the Inter-Spike Interval concept was performed, which may be useful in the future development of applications based on spiking filters. |
| 从认识论角度探讨独立约束下的解耦表示学习 | 解耦表示学习旨在通过训练数据编码器来提高深度学习方法的可解释性，该编码器能够识别数据生成过程中具有语义意义的潜在变量。然而，对于解耦表示学习的目标，目前尚无普遍接受的定义。特别是，关于潜在变量是否应相互独立的讨论相当多。在本文中，我们首先通过在认识论与解耦表示学习之间建立概念桥梁，探讨了潜在变量之间相互关系的这些论点。然后，受这些跨学科概念的启发，我们引入了一个双层潜在空间框架，为先前关于此问题的争论提供了一个通用解决方案。最后，我们在生成对抗网络（GAN）框架内结合互信息约束和独立性约束，提出了一种新的解耦表示学习方法。实验结果表明，我们提出的方法在定量和定性评估中始终优于基线方法。该方法在多种常用指标上表现出色，并展示了强大的能力来解耦各种语义因素，从而提高了可控生成的质量，进而增强了算法的可解释性。 | Ruoyu Wang | [PDF](http://arxiv.org/pdf/2409.02672v1) | N/A | Independence Constrained Disentangled Representation Learning from Epistemological Perspective | Disentangled Representation Learning aims to improve the explainability of deep learning methods by training a data encoder that identifies semantically meaningful latent variables in the data generation process. Nevertheless, there is no consensus regarding a universally accepted definition for the objective of disentangled representation learning. In particular, there is a considerable amount of discourse regarding whether should the latent variables be mutually independent or not. In this paper, we first investigate these arguments on the interrelationships between latent variables by establishing a conceptual bridge between Epistemology and Disentangled Representation Learning. Then, inspired by these interdisciplinary concepts, we introduce a two-level latent space framework to provide a general solution to the prior arguments on this issue. Finally, we propose a novel method for disentangled representation learning by employing an integration of mutual information constraint and independence constraint within the Generative Adversarial Network (GAN) framework. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches in both quantitative and qualitative evaluations. The method exhibits strong performance across multiple commonly used metrics and demonstrates a great capability in disentangling various semantic factors, leading to an improved quality of controllable generation, which consequently benefits the explainability of the algorithm. |
| 因果感知型Transformer网络在机器人导航中的应用 | 近期机器学习算法的进展引发了开发多功能具身智能系统的日益增长的关注。然而，当前该领域的研究揭示了改进的机会。首先，直接采用RNN和Transformer往往忽视了具身智能与传统序列数据建模之间的具体差异，这可能限制其在具身智能任务中的表现。其次，依赖于任务特定的配置，如预训练模块和数据集特定的逻辑，损害了这些方法的通用性。我们通过首先从因果关系的角度探讨具身智能任务与其他序列数据任务的独特差异，来解决这些限制，并提出一个因果框架以阐明传统序列方法在具身智能中的不足。通过利用这种因果视角，我们提出了用于导航的因果感知Transformer（CAT）网络，其特点是因果理解模块，以增强模型的环境理解能力。同时，我们的方法不包含任务特定的归纳偏差，并且可以以端到端的方式进行训练，这增强了方法在各种情境下的通用性。实证评估表明，我们的方法在各种设置、任务和模拟环境中始终超越基准性能。广泛的消融研究表明，性能提升可归因于因果理解模块，该模块在强化学习和监督学习设置中均表现出有效性和效率。 | Ruoyu Wang | [PDF](http://arxiv.org/pdf/2409.02669v1) | N/A | Causality-Aware Transformer Networks for Robotic Navigation | Recent advances in machine learning algorithms have garnered growing interest in developing versatile Embodied AI systems. However, current research in this domain reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Embodied AI tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Embodied AI. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings. |
| 机器学习简介 | 这本书介绍了数学基础和技巧，这些基础和技巧导致了机器学习中使用的许多算法的开发和分析。它从一个介绍性的章节开始，描述了贯穿全书的符号，并作为对微积分、线性代数和概率论中基本概念的提醒，同时也介绍了一些测度论术语，这些术语可以作为使用这些工具的章节的阅读指南。介绍性章节还提供了矩阵分析和优化的背景材料。后面的章节为书中使用的许多算法提供了理论支持，包括随机梯度下降、近端方法等。在讨论了统计预测的基本概念之后，本书介绍了再生核理论和希尔伯特空间技术，这些技术在许多地方都有应用，然后介绍了各种监督统计学习算法的描述，包括线性方法、支持向量机、决策树、提升法或神经网络。主题随后转向生成方法，从介绍采样方法和马尔可夫链理论的章节开始。接下来的章节描述了图形模型的理论，潜在变量模型的变分方法介绍，以及基于深度学习的生成模型。接下来的章节专注于无监督学习方法，用于聚类、因子分析和流形学习。本书的最后一章是理论导向的，讨论了集中不等式和泛化界限。 | Laurent Younes | [PDF](http://arxiv.org/pdf/2409.02668v1) | N/A | Introduction to Machine Learning | This book introduces the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used in machine learning. It starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability and also introduces some measure theoretic terminology, which can be used as a reading guide for the sections that use these tools. The introductory chapters also provide background material on matrix analysis and optimization. The latter chapter provides theoretical support to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc. After discussing basic concepts for statistical prediction, the book includes an introduction to reproducing kernel theory and Hilbert space techniques, which are used in many places, before addressing the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks. The subject then switches to generative methods, starting with a chapter that presents sampling methods and an introduction to the theory of Markov chains. The following chapter describe the theory of graphical models, an introduction to variational methods for models with latent variables, and to deep-learning based generative models. The next chapters focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds. |
| 创建特定领域的翻译记忆以进行机器翻译微调：TRENCARD双语心脏病学语料库 | 本文探讨了译者或其他语言专业人士如何创建翻译记忆（TM），以编译特定领域的平行语料库，这些语料库可用于不同场景，如机器翻译训练和微调、TM利用以及大型语言模型微调。文章介绍了一种半自动化的TM准备方法，主要利用译者常用的翻译工具，以确保数据质量和译者的控制。随后，该半自动化方法被用于构建基于心脏病学的土耳其语到英语语料库，该语料库来自土耳其心脏病学期刊的双语摘要。最终形成的语料库名为TRENCARD Corpus，包含约80万个源词和5万个句子。通过这种方法，译者可以在合理时间内构建自定义TM，并在需要双语数据的任务中使用它们。 | Gokhan Dogru | [PDF](http://arxiv.org/pdf/2409.02667v1) | N/A | Creating Domain-Specific Translation Memories for Machine Translation Fine-tuning: The TRENCARD Bilingual Cardiology Corpus | This article investigates how translation memories (TM) can be created by translators or other language professionals in order to compile domain-specific parallel corpora , which can then be used in different scenarios, such as machine translation training and fine-tuning, TM leveraging, and/or large language model fine-tuning. The article introduces a semi-automatic TM preparation methodology leveraging primarily translation tools used by translators in favor of data quality and control by the translators. This semi-automatic methodology is then used to build a cardiology-based Turkish -> English corpus from bilingual abstracts of Turkish cardiology journals. The resulting corpus called TRENCARD Corpus has approximately 800,000 source words and 50,000 sentences. Using this methodology, translators can build their custom TMs in a reasonable time and use them in their bilingual data requiring tasks. |
| OpenFact 在 CheckThat! 2024：结合多种攻击方法实现有效的对抗性文本生成 | 本文介绍了在CLEF 2024任务6：对抗样本下的可信度评估鲁棒性（InCrediblAE）中CheckThat!实验室的实验和结果。该任务的主要目标是生成五个问题领域的对抗样本，以评估广泛使用的文本分类方法（微调的BERT、BiLSTM和RoBERTa）在应用于可信度评估问题时的鲁棒性。本研究探讨了将集成学习应用于增强对自然语言处理（NLP）模型的对抗攻击。我们在五个数据集上系统地测试和优化了几种对抗攻击方法，包括BERT-Attack、遗传算法、TextFooler和CLARE，这些方法涉及各种错误信息任务。通过开发BERT-Attack的改进版本和混合方法，我们在攻击效果上取得了显著提升。我们的结果展示了通过修改和结合多种方法来创建更复杂和有效的对抗攻击策略的潜力，有助于开发更鲁棒和安全的系统。 | Włodzimierz Lewoniewski | [PDF](http://arxiv.org/pdf/2409.02649v1) | N/A | OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation | This paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE). The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BiLSTM, and RoBERTa) when applied to credibility assessment issues.   This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems. |
| 基于学习的先进车辆仪表集群渲染错误检测系统 | 汽车行业正在通过每一款新上市的车型不断扩展数字显示选项。这不仅涉及尺寸、分辨率和定制选择的扩展，还包括在组装显示集群内容时采用新颖的显示效果，如叠加。遗憾的是，这也带来了对适当监控系统的需求，这些系统能够在检测到渲染错误时及时采取适当的应对措施。传统的解决方案，如循环冗余校验（CRC），很快将不再适用，因为任何形式的阿尔法混合、内容扭曲或缩放都可能导致不必要的CRC违规。因此，我们提出了一种新的监控方法，使用指示灯（例如警告标志）作为示例，来验证显示内容的正确性。该方法采用基于学习的方式，区分“良好”的指示灯，即人类驾驶员能够正确理解的指示灯，以及“损坏”的指示灯，即那些无法正确显示或被正确感知的指示灯。因此，它具有对单个像素错误的内在抗性，并隐含地支持背景变化、叠加或缩放效果。我们的实验研究表明，所有“损坏”的测试模式都被正确分类，同时没有触发任何误报，这进一步强调了其有效性。 | Cornelius Bürkle | [PDF](http://arxiv.org/pdf/2409.02647v1) | N/A | Learning-Based Error Detection System for Advanced Vehicle Instrument Cluster Rendering | The automotive industry is currently expanding digital display options with every new model that comes onto the market. This entails not just an expansion in dimensions, resolution, and customization choices, but also the capability to employ novel display effects like overlays while assembling the content of the display cluster. Unfortunately, this raises the need for appropriate monitoring systems that can detect rendering errors and apply appropriate countermeasures when required. Classical solutions such as Cyclic Redundancy Checks (CRC) will soon be no longer viable as any sort of alpha blending, warping of scaling of content can cause unwanted CRC violations. Therefore, we propose a novel monitoring approach to verify correctness of displayed content using telltales (e.g. warning signs) as example. It uses a learning-based approach to separate "good" telltales, i.e. those that a human driver will understand correctly, and "corrupted" telltales, i.e. those that will not be visible or perceived correctly. As a result, it possesses inherent resilience against individual pixel errors and implicitly supports changing backgrounds, overlay or scaling effects. This is underlined by our experimental study where all "corrupted" test patterns were correctly classified, while no false alarms were triggered. |
| 关于新兴语言的调查 | 新兴语言领域代表了人工智能领域内的一个新颖研究方向，特别是在多智能体强化学习的背景下。尽管研究语言出现的概念并不新鲜，但早期的方法主要关注解释人类语言的形成，对其在人工代理中的潜在用途考虑较少。相比之下，基于强化学习的研究旨在开发出与人类语言相当甚至更优的代理通信能力。因此，它们超越了自然语言处理研究中常见的学习统计表示。这引发了一系列基本问题，从语言出现的先决条件到衡量其成功的标准。本文通过全面综述181篇关于人工智能中新兴语言的科学出版物来回答这些问题。其目的是为对该领域感兴趣或精通的研究人员提供参考。因此，主要贡献包括对流行术语的定义和概述、现有评估方法和指标的分析，以及所识别研究空白的描述。 | Jannik Peters | [PDF](http://arxiv.org/pdf/2409.02645v1) | N/A | A Survey on Emergent Language | The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps. |
| 动态生物系统中的共形预测 | 不确定性量化（UQ）是指系统性地确定和表征计算模型预测中置信度的过程。在系统生物学领域，尤其是动态模型中，UQ至关重要，因为它解决了由非线性和参数敏感性带来的挑战，使我们能够正确理解和推断复杂生物系统的行为。在此，我们专注于由确定性非线性常微分方程表示的动态模型。当前许多UQ方法依赖于贝叶斯统计方法，尽管这些方法功能强大，但它们通常需要强先验假设，并做出可能不适用于生物系统的参数假设。此外，这些方法在样本量有限且统计推断受到限制的领域面临挑战，计算速度成为大型生物系统模型中的瓶颈。作为替代方案，我们提出使用保形推断方法，并引入了两种新算法，这些算法在某些情况下提供了非渐近保证，增强了在各种应用中的鲁棒性和可扩展性。我们通过多个场景展示了所提出算法的有效性，突显了它们相对于传统贝叶斯方法的优势。所提出的方法在多样化的生物数据结构和场景中显示出有前景的结果，为生物系统动态模型的不确定性量化提供了一个通用框架。该方法的软件及结果复现代码可在https://zenodo.org/doi/10.5281/zenodo.13644870获取。 | Alberto Portela | [PDF](http://arxiv.org/pdf/2409.02644v1) | N/A | Conformal Prediction in Dynamic Biological Systems | Uncertainty quantification (UQ) is the process of systematically determining and characterizing the degree of confidence in computational model predictions. In the context of systems biology, especially with dynamic models, UQ is crucial because it addresses the challenges posed by nonlinearity and parameter sensitivity, allowing us to properly understand and extrapolate the behavior of complex biological systems. Here, we focus on dynamic models represented by deterministic nonlinear ordinary differential equations. Many current UQ approaches in this field rely on Bayesian statistical methods. While powerful, these methods often require strong prior specifications and make parametric assumptions that may not always hold in biological systems. Additionally, these methods face challenges in domains where sample sizes are limited, and statistical inference becomes constrained, with computational speed being a bottleneck in large models of biological systems. As an alternative, we propose the use of conformal inference methods, introducing two novel algorithms that, in some instances, offer non-asymptotic guarantees, enhancing robustness and scalability across various applications. We demonstrate the efficacy of our proposed algorithms through several scenarios, highlighting their advantages over traditional Bayesian approaches. The proposed methods show promising results for diverse biological data structures and scenarios, offering a general framework to quantify uncertainty for dynamic models of biological systems.The software for the methodology and the reproduction of the results is available at https://zenodo.org/doi/10.5281/zenodo.13644870. |
| AdvSecureNet：一个用于对抗机器学习的Python工具包 | 机器学习模型容易受到对抗攻击。虽然已经开发了多种工具来研究这些漏洞，但它们通常缺乏全面的功能和灵活性。我们引入了AdvSecureNet，这是一个基于PyTorch的对抗机器学习工具包，首次原生支持多GPU设置用于攻击、防御和评估。它也是首个同时支持CLI和API接口以及外部YAML配置文件的工具包，以增强多功能性和可重复性。该工具包包含多种攻击、防御和评估指标。遵循严格的软件工程实践，以确保高代码质量和可维护性。该项目作为开源项目在GitHub上提供，地址为https://github.com/melihcatal/advsecurenet，并且可以通过PyPI安装。 | Melih Catal | [PDF](http://arxiv.org/pdf/2409.02629v1) | N/A | AdvSecureNet: A Python Toolkit for Adversarial Machine Learning | Machine learning models are vulnerable to adversarial attacks. Several tools have been developed to research these vulnerabilities, but they often lack comprehensive features and flexibility. We introduce AdvSecureNet, a PyTorch based toolkit for adversarial machine learning that is the first to natively support multi-GPU setups for attacks, defenses, and evaluation. It is the first toolkit that supports both CLI and API interfaces and external YAML configuration files to enhance versatility and reproducibility. The toolkit includes multiple attacks, defenses and evaluation metrics. Rigiorous software engineering practices are followed to ensure high code quality and maintainability. The project is available as an open-source project on GitHub at https://github.com/melihcatal/advsecurenet and installable via PyPI. |
| （隐式）集成中的集成：大型模型中的认知不确定性崩溃 | 认知不确定性对安全关键型应用和分布外检测任务至关重要。然而，我们在深度学习模型中发现了一个矛盾现象：随着模型复杂性的增加，认知不确定性出现了崩溃，这挑战了更大模型必然提供更好不确定性量化的假设。我们认为，这是由于大型模型内部的隐式集成所导致的。为了支持这一假设，我们在多种架构上进行了实证演示，从显式集成和简单多层感知器到最先进的视觉模型，包括ResNet和Vision Transformers。对于后者，我们研究了隐式集成提取，并将大型模型分解为多样化的子模型，从而恢复了认知不确定性。我们为这些现象提供了理论依据，并探讨了它们对不确定性估计的影响。 | Andreas Kirsch | [PDF](http://arxiv.org/pdf/2409.02628v1) | N/A | (Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models | Epistemic uncertainty is crucial for safety-critical applications and out-of-distribution detection tasks. Yet, we uncover a paradoxical phenomenon in deep learning models: an epistemic uncertainty collapse as model complexity increases, challenging the assumption that larger models invariably offer better uncertainty quantification. We propose that this stems from implicit ensembling within large models. To support this hypothesis, we demonstrate epistemic uncertainty collapse empirically across various architectures, from explicit ensembles of ensembles and simple MLPs to state-of-the-art vision models, including ResNets and Vision Transformers -- for the latter, we examine implicit ensemble extraction and decompose larger models into diverse sub-models, recovering epistemic uncertainty. We provide theoretical justification for these phenomena and explore their implications for uncertainty estimation. |
