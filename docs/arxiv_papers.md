# Arxiv Papers

| 标题  | 摘要 | 作者 | PDF链接 | 代码仓库 | Title | Abstract | 
|-------|---------|----------|-----------|------------------|--------------------|---------|
| 跨越情节与语义：一种理解长篇视频的新框架 | 尽管现有研究往往将长视频视为扩展的短视频，我们提出了一种新颖的方法，更准确地反映了人类认知。本文介绍了BREASE：用于长视频理解的桥接剧集与语义，该模型模拟了情景记忆的积累，以捕捉动作序列，并通过分散在整个视频中的语义知识对其进行强化。我们的工作做出了两个关键贡献：首先，我们开发了一种情景压缩器（ECO），能够从微观到半宏观层面高效地聚合关键表征。其次，我们提出了一种语义检索器（SeTR），通过关注更广泛的上下文，用语义信息增强这些聚合表征，显著降低特征维度，同时保留相关的宏观级别信息。广泛的实验表明，BREASE在零样本和完全监督设置下的多个长视频理解基准测试中达到了最先进的性能。项目页面和代码位于：https://joslefaure.github.io/assets/html/hermes.html。 | Gueter Josmy Faure | [PDF](http://arxiv.org/pdf/2408.17443v1) | N/A | Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding | While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And SEmantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html. |
| SYNTHEVAL：结合合成检查清单对NLP模型进行混合行为测试 | 传统的自然语言处理（NLP）基准测试通常涉及使用静态的保留测试集。然而，这种方法往往导致性能的高估，并且缺乏提供全面、可解释和动态评估NLP模型的能力。最近，像DynaBench（Kiela等人，2021）和CheckList（Ribeiro等人，2020）这样的工作通过多步骤人工标注流程生成测试类型，对NLP模型进行行为测试，解决了这些局限性。不幸的是，手动创建多种测试类型需要大量人力，成本往往高得令人望而却步。在这项工作中，我们提出了SYNTHEVAL，一个混合行为测试框架，它利用大型语言模型（LLMs）生成广泛的测试类型，以全面评估NLP模型。SYNTHEVAL首先通过LLMs使用受控生成生成句子，然后通过比较LLMs与特定任务NLP模型的预测来识别具有挑战性的示例。在最后阶段，人类专家调查这些具有挑战性的示例，手动设计模板，并识别特定任务模型持续表现出的失败类型。我们将SYNTHEVAL应用于两个分类任务：情感分析和有毒语言检测，并展示了我们的框架在识别这些任务上强大模型的弱点方面的有效性。我们在https://github.com/Loreley99/SynthEval_CheckList分享了我们的代码。 | Raoyuan Zhao | [PDF](http://arxiv.org/pdf/2408.17437v1) | N/A | SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists | Traditional benchmarking in NLP typically involves using static held-out test sets. However, this approach often results in an overestimation of performance and lacks the ability to offer comprehensive, interpretable, and dynamic assessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021) and CheckList (Ribeiro et al., 2020) have addressed these limitations through behavioral testing of NLP models with test types generated by a multistep human-annotated pipeline. Unfortunately, manually creating a variety of test types requires much human labor, often at prohibitive cost. In this work, we propose SYNTHEVAL, a hybrid behavioral testing framework that leverages large language models (LLMs) to generate a wide range of test types for a comprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via LLMs using controlled generation, and then identifies challenging examples by comparing the predictions made by LLMs with task-specific NLP models. In the last stage, human experts investigate the challenging examples, manually design templates, and identify the types of failures the taskspecific models consistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment analysis and toxic language detection, and show that our framework is effective in identifying weaknesses of strong models on these tasks. We share our code in https://github.com/Loreley99/SynthEval_CheckList. |
| SelectTTS：通过基于离散单元的帧选择合成任何人的声音 | 合成未见说话者的声音是多说话者文本到语音（TTS）中持续存在的挑战。大多数多说话者TTS模型依赖于在训练期间通过说话者调节来建模说话者特征。通过这种方法建模未见说话者属性需要增加模型复杂性，这使得重现结果和改进结果变得困难。我们设计了一种简单的替代方案。我们提出了SelectTTS，一种新颖的方法，从目标说话者中选择适当的帧，并使用帧级自监督学习（SSL）特征进行解码。我们展示了这种方法能够有效地捕捉未见说话者的说话者特征，并在客观和主观指标上与其他多说话者TTS框架取得可比的结果。通过SelectTTS，我们展示了从目标说话者的语音中选择帧是实现未见说话者泛化的直接方式，且模型复杂度较低。我们的说话者相似性性能优于SOTA基线XTTS-v2和VALL-E，模型参数减少了8倍以上，训练数据减少了270倍。 | Ismail Rasim Ulgen | [PDF](http://arxiv.org/pdf/2408.17432v1) | N/A | SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection | Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. We design a simple alternative to this. We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data |
| CLOCR-C：利用预训练语言模型进行上下文增强的OCR校正 | 历史印刷媒体档案的数字化对于提高当代记录的可访问性至关重要。然而，将实体记录转换为数字文本的光学字符识别（OCR）过程容易出错，尤其是报纸和期刊因其复杂的版面设计。本文介绍了利用基于变换器的语言模型（LMs）的填补和上下文自适应能力来提高OCR质量的上下文利用OCR校正（CLOCR-C）方法。研究旨在确定LMs是否能够执行OCR后校正、改善下游NLP任务，以及在修正过程中提供社会文化背景的价值。实验使用了七种LMs在三个数据集上进行：19世纪期刊版（NCSE）和Overproof收藏的两个数据集。结果显示，某些LMs能够显著降低错误率，表现最佳的模型在NCSE数据集上实现了超过60%的字符错误率降低。OCR改进延伸至下游任务，如命名实体识别，提高了余弦命名实体相似度。此外，研究表明，在提示中提供社会文化背景可以提升性能，而误导性提示则会降低性能。除了这些发现，本研究还发布了来自NCSE的91篇转录文章的数据集，共包含4万个单词，以支持该领域的进一步研究。研究结果表明，CLOCR-C是一种有前景的方法，通过利用LMs中嵌入的社会文化信息和需要校正的文本，可以提高现有数字档案的质量。 | Jonathan Bourne | [PDF](http://arxiv.org/pdf/2408.17428v1) | N/A | CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models | The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction. |
| 公平意识下的图形模型估计 | 本文探讨了图模型（GMs）估计中的公平性问题，尤其是高斯模型、协方差模型和伊辛模型。这些模型在理解高维数据中的复杂关系方面发挥着至关重要的作用。然而，标准的图模型可能会导致偏见结果，特别是在基础数据涉及敏感特征或受保护群体时。为了解决这一问题，我们引入了一个全面的框架，旨在减少与受保护属性相关的图模型估计中的偏见。我们的方法涉及将成对图差异误差和定制损失函数整合到一个非光滑多目标优化问题中，力求在不同敏感群体中实现公平性，同时保持图模型的有效性。对合成数据集和真实世界数据集的实验评估表明，我们的框架能有效减轻偏见，而不损害图模型的性能。 | Zhuoping Zhou | [PDF](http://arxiv.org/pdf/2408.17396v1) | N/A | Fairness-Aware Estimation of Graphical Models | This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance. |
| 神经切线集合的持续学习 | 持续学习的一种自然策略是权衡一组固定函数的贝叶斯集成。这表明，如果一个（单一的）神经网络可以被解释为一个集成，那么我们可以设计出不遗忘的有效学习算法。为了实现这一可能性，我们观察到，具有N个参数的神经网络分类器可以被解释为N个分类器的加权集成，并且在懒惰学习机制的极限情况下，这些分类器在整个学习过程中是固定的。我们称这些分类器为神经切线专家，并证明它们输出的标签概率分布是有效的。然后，我们推导出给定过去数据时每个专家的似然和后验概率。令人惊讶的是，我们发现这些专家的后验更新等同于网络权重上随机梯度下降（SGD）的缩放和投影形式。在远离懒惰机制的情况下，网络可以被视为随时间改进的自适应专家的集成。这些结果为神经网络提供了一种新的解释，即作为专家的贝叶斯集成，为理解和缓解持续学习环境中的灾难性遗忘提供了一个原则性的框架。 | Ari S. Benjamin | [PDF](http://arxiv.org/pdf/2408.17394v1) | N/A | Continual learning with the neural tangent ensemble | A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We term these classifiers the neural tangent experts and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, we learn that the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings. |
| 非凸两阶段随机优化问题的贝叶斯优化 | 贝叶斯优化是一种解决昂贵、黑箱优化问题的样本高效方法。随机规划关注在不确定性下的优化问题，通常情况下，平均性能是关注的量。在两阶段问题的第一阶段，必须在这种不确定性下做出即刻决定，而在第二阶段，则在不确定性解决后做出观望决定。许多随机规划方法假设目标函数的评估成本低廉且为线性或凸性。在本研究中，我们应用贝叶斯优化来解决评估成本高昂的非凸两阶段随机规划问题。我们构建了一个基于知识梯度的采集函数来联合优化第一阶段和第二阶段的变量，确立了渐进一致性的保证，并提供了计算效率高的近似方法。我们展示了与另一种我们构建的交替关注两种变量类型的方法相比拟的经验结果，以及优于标准、朴素的两步基准的优越经验结果。我们表明，变量类型之间的维度差异和长度尺度可能导致两步算法的效率低下，而联合和交替采集函数在所有测试问题中表现良好。实验在合成数据和真实世界示例上进行。 | Jack M. Buckingham | [PDF](http://arxiv.org/pdf/2408.17387v1) | N/A | Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization Problems | Bayesian optimization is a sample-efficient method for solving expensive, black-box optimization problems. Stochastic programming concerns optimization under uncertainty where, typically, average performance is the quantity of interest. In the first stage of a two-stage problem, here-and-now decisions must be made in the face of this uncertainty, while in the second stage, wait-and-see decisions are made after the uncertainty has been resolved. Many methods in stochastic programming assume that the objective is cheap to evaluate and linear or convex. In this work, we apply Bayesian optimization to solve non-convex, two-stage stochastic programs which are expensive to evaluate. We formulate a knowledge-gradient-based acquisition function to jointly optimize the first- and second-stage variables, establish a guarantee of asymptotic consistency and provide a computationally efficient approximation. We demonstrate comparable empirical results to an alternative we formulate which alternates its focus between the two variable types, and superior empirical results over the standard, naive, two-step benchmark. We show that differences in the dimension and length scales between the variable types can lead to inefficiencies of the two-step algorithm, while the joint and alternating acquisition functions perform well in all problems tested. Experiments are conducted on both synthetic and real-world examples. |
| LASSO-MOGAT：一种用于癌症分类的多组学图注意力框架 | 将机器学习方法应用于分析基因表达模式的变化，最近在癌症研究中崭露头角，成为一种强大的研究途径，加深了我们对癌症发展和进展背后分子机制的理解。结合基因表达数据与其他类型的组学数据，已被众多研究证实能够改善癌症分类结果。尽管取得了这些进展，有效整合高维多组学数据并捕捉不同生物层面间的复杂关系仍然充满挑战。本文介绍了LASSO-MOGAT（LASSO-多组学门控注意力），这是一种新颖的基于图的深度学习框架，它整合了信使RNA、微小RNA和DNA甲基化数据，用于分类31种癌症类型。利用LIMMA进行差异表达分析和LASSO回归进行特征选择，并借助图注意力网络（GATs）纳入蛋白质-蛋白质相互作用（PPI）网络，LASSO-MOGAT有效地捕捉了多组学数据中的复杂关系。通过五折交叉验证的实验验证，展示了该方法的精确性、可靠性和提供癌症分子机制全面洞察的能力。提出的基于蛋白质-蛋白质相互作用的图注意力架构对图中边进行注意力系数的计算，证实了这对于识别多组学数据中癌症分类的协同作用是有益的。 | Fadi Alharbi | [PDF](http://arxiv.org/pdf/2408.17384v1) | N/A | LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer Classification | The application of machine learning methods to analyze changes in gene expression patterns has recently emerged as a powerful approach in cancer research, enhancing our understanding of the molecular mechanisms underpinning cancer development and progression. Combining gene expression data with other types of omics data has been reported by numerous works to improve cancer classification outcomes. Despite these advances, effectively integrating high-dimensional multi-omics data and capturing the complex relationships across different biological layers remains challenging. This paper introduces LASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep learning framework that integrates messenger RNA, microRNA, and DNA methylation data to classify 31 cancer types. Utilizing differential expression analysis with LIMMA and LASSO regression for feature selection, and leveraging Graph Attention Networks (GATs) to incorporate protein-protein interaction (PPI) networks, LASSO-MOGAT effectively captures intricate relationships within multi-omics data. Experimental validation using five-fold cross-validation demonstrates the method's precision, reliability, and capacity for providing comprehensive insights into cancer molecular mechanisms. The computation of attention coefficients for the edges in the graph by the proposed graph-attention architecture based on protein-protein interactions proved beneficial for identifying synergies in multi-omics data for cancer classification. |
| 使用10倍少得多的参数进行更多微调 | 参数高效的微调（PEFT）技术已经释放了大型预训练模型廉价且易于专用的潜力。然而，最突出的方法，如低秩适配器（LoRA），依赖于启发式或经验法则来做出其架构选择——这可能限制了它们对于新模型和架构的性能。这一局限性表明，可以利用神经架构搜索的技术来获得最优的适配器架构，但这些技术往往成本高昂且难以实施。我们通过Monarch矩形微调（MoRe）这一简单框架来应对这一挑战，该框架依赖于Monarch矩阵类别来搜索适配器架构。理论上，我们证明了MoRe比LoRA更具表达力。实证上，我们的方法在各种任务和模型上比最先进的PEFTs更加参数高效且性能优越，仅需LoRA参数的5%。 | Wenxuan Tan | [PDF](http://arxiv.org/pdf/2408.17383v1) | N/A | MoRe Fine-Tuning with 10x Fewer Parameters | Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential to cheaply and easily specialize large pretrained models. However, the most prominent approaches, like low-rank adapters (LoRA), depend on heuristics or rules-of-thumb for their architectural choices -- potentially limiting their performance for new models and architectures. This limitation suggests that techniques from neural architecture search could be used to obtain optimal adapter architectures, but these are often expensive and difficult to implement. We address this challenge with Monarch Rectangular Fine-tuning (MoRe), a simple framework to search over adapter architectures that relies on the Monarch matrix class. Theoretically, we show that MoRe is more expressive than LoRA. Empirically, our approach is more parameter-efficient and performant than state-of-the-art PEFTs on a range of tasks and models, with as few as 5\% of LoRA's parameters. |
| 交通专业知识遇上残差强化学习：基于知识的模型化残差强化学习在CAV轨迹控制中的应用 | 基于模型的强化学习（RL）通过利用虚拟环境模型，预计比无模型RL展现出更高的样本效率。然而，由于复杂系统和环境中的不确定性，获取足够准确的环境动力学表示是具有挑战性的。不准确的环境模型可能会降低基于模型的RL的样本效率和性能。此外，尽管基于模型的RL可以提高样本效率，但它通常仍然需要大量的训练时间从头开始学习，这可能限制其相对于无模型方法的优势。为了应对这些挑战，本文引入了一种知识启发的基于模型的残差强化学习框架，旨在通过将已建立的专家知识融入学习过程并避免从零开始的问题，来提高学习效率。我们的方法将交通专家知识整合到虚拟环境模型中，采用智能驾驶员模型（IDM）作为基础动力学，神经网络作为残差动力学，从而确保适应复杂场景的能力。我们提出了一种将传统控制方法与残差RL相结合的新策略，便于高效学习和策略优化，无需从头开始学习。所提出的方法应用于CAV轨迹控制任务，以消散混合交通流中的停车-启动波。实验结果表明，我们提出的方法使CAV代理在轨迹控制方面相对于基准代理在样本效率、交通流平滑度和交通流动性方面实现了更优越的性能。源代码和补充材料可在https://github.com/zihaosheng/traffic-expertise-RL/获取。 | Zihao Sheng | [PDF](http://arxiv.org/pdf/2408.17380v1) | N/A | Traffic expertise meets residual RL: Knowledge-informed model-based residual reinforcement learning for CAV trajectory control | Model-based reinforcement learning (RL) is anticipated to exhibit higher sample efficiency compared to model-free RL by utilizing a virtual environment model. However, it is challenging to obtain sufficiently accurate representations of the environmental dynamics due to uncertainties in complex systems and environments. An inaccurate environment model may degrade the sample efficiency and performance of model-based RL. Furthermore, while model-based RL can improve sample efficiency, it often still requires substantial training time to learn from scratch, potentially limiting its advantages over model-free approaches. To address these challenges, this paper introduces a knowledge-informed model-based residual reinforcement learning framework aimed at enhancing learning efficiency by infusing established expert knowledge into the learning process and avoiding the issue of beginning from zero. Our approach integrates traffic expert knowledge into a virtual environment model, employing the Intelligent Driver Model (IDM) for basic dynamics and neural networks for residual dynamics, thus ensuring adaptability to complex scenarios. We propose a novel strategy that combines traditional control methods with residual RL, facilitating efficient learning and policy optimization without the need to learn from scratch. The proposed approach is applied to CAV trajectory control tasks for the dissipation of stop-and-go waves in mixed traffic flow. Experimental results demonstrate that our proposed approach enables the CAV agent to achieve superior performance in trajectory control compared to the baseline agents in terms of sample efficiency, traffic flow smoothness and traffic mobility. The source code and supplementary materials are available at https://github.com/zihaosheng/traffic-expertise-RL/. |
| NDP：下一个分配预测作为更广泛的靶标 | 经过在下一个标记预测（NTP）范式上训练的大型语言模型（LLMs）已经展现出了强大的能力。然而，现有的NTP范式存在若干局限性，特别是在处理计划任务复杂性和推理过程中的错误传播方面。在我们的工作中，我们扩展了对NTP的批评，强调其局限性也源于训练时采用的狭窄目标：预测一个次优的一热分布。为了支持这一批评，我们进行了一项预实验，将强大LLMs的输出分布视为高效的世界数据压缩。通过评估$n$-gram分布与LLMs的一热分布之间的相似性，我们观察到$n$-gram分布与LLMs的输出分布更为接近。基于这一洞察，我们引入了下一个分布预测（NDP），它使用$n$-gram分布来替代一热目标，从而在不增加在线训练时间的情况下增强学习。我们在翻译、通用任务、语言迁移和医学领域适应等多个领域进行了实验。与NTP相比，NDP在翻译任务中可实现高达+2.97的COMET改进，在通用任务中平均提升+0.61，而在医学领域平均提升惊人地达到+10.75。这表明解决目标狭窄问题具有实质性的好处，并为未来改进NTP的工作指明了新的方向。 | Junhao Ruan | [PDF](http://arxiv.org/pdf/2408.17377v1) | N/A | NDP: Next Distribution Prediction as a More Broad Target | Large language models (LLMs) trained on next-token prediction (NTP) paradigm have demonstrated powerful capabilities. However, the existing NTP paradigm contains several limitations, particularly related to planned task complications and error propagation during inference. In our work, we extend the critique of NTP, highlighting its limitation also due to training with a narrow objective: the prediction of a sub-optimal one-hot distribution. To support this critique, we conducted a pre-experiment treating the output distribution from powerful LLMs as efficient world data compression. By evaluating the similarity between the $n$-gram distribution and the one-hot distribution with LLMs, we observed that the $n$-gram distributions align more closely with the output distribution of LLMs. Based on this insight, we introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions to replace the one-hot targets, enhancing learning without extra online training time. We conducted experiments across translation, general task, language transfer, and medical domain adaptation. Compared to NTP, NDP can achieve up to +2.97 COMET improvement in translation tasks, +0.61 average improvement in general tasks, and incredible +10.75 average improvement in the medical domain. This demonstrates the concrete benefits of addressing the target narrowing problem, pointing to a new direction for future work on improving NTP. |
| 探索环境污染物对多发性硬化症进展的影响 | 多发性硬化症（MS）是一种慢性自身免疫性和炎症性神经系统疾病，其特点是症状加剧的发作，称为复发。在本研究中，我们利用H2020 BRAINTEASER项目的数据，探讨环境因素在MS患者复发发生中的作用。我们采用了包括随机森林（RF）和逻辑回归（LR）在内的预测模型，并使用不同集合的输入特征，基于一周内收集的临床和污染物数据来预测复发的发生。随机森林模型取得了最佳结果，AUC-ROC得分为0.713。环境变量，如降水量、二氧化氮（NO2）、细颗粒物（PM2.5）、湿度和温度，被发现与预测相关。 | Elena Marinello | [PDF](http://arxiv.org/pdf/2408.17376v1) | N/A | Exploring the Impact of Environmental Pollutants on Multiple Sclerosis Progression | Multiple Sclerosis (MS) is a chronic autoimmune and inflammatory neurological disorder characterised by episodes of symptom exacerbation, known as relapses. In this study, we investigate the role of environmental factors in relapse occurrence among MS patients, using data from the H2020 BRAINTEASER project. We employed predictive models, including Random Forest (RF) and Logistic Regression (LR), with varying sets of input features to predict the occurrence of relapses based on clinical and pollutant data collected over a week. The RF yielded the best result, with an AUC-ROC score of 0.713. Environmental variables, such as precipitation, NO2, PM2.5, humidity, and temperature, were found to be relevant to the prediction. |
| 利用图神经网络来预测电力消耗 | 精确的电力需求预测对于多个方面至关重要，尤其是在可再生能源的整合以及向去中心化网络范式的过渡引入了更大的复杂性和不确定性之际。所提出的方法利用基于图的表示法，有效地捕捉这种去中心化网络结构中固有的空间分布和关系复杂性。本研究工作提供了一种新颖的方法，超越了传统的广义加性模型框架，考虑了诸如图卷积网络或图SAGE等模型。这些基于图的模型使得能够纳入节点间不同层次的互联性和信息共享，其中每个节点对应于一部分消费者（例如国家各地区）的综合负荷（即消费量）。更具体地说，我们引入了一系列针对消费预测定制图推理的方法，以及一个评估所开发模型在性能和可解释性方面的框架。我们在合成和真实框架下进行了电力预测实验，考虑了法国本土地区，并讨论了我们方法的性能和优点。 | Eloi Campagne | [PDF](http://arxiv.org/pdf/2408.17366v1) | N/A | Leveraging Graph Neural Networks to Forecast Electricity Consumption | Accurate electricity demand forecasting is essential for several reasons, especially as the integration of renewable energy sources and the transition to a decentralized network paradigm introduce greater complexity and uncertainty. The proposed methodology leverages graph-based representations to effectively capture the spatial distribution and relational intricacies inherent in this decentralized network structure. This research work offers a novel approach that extends beyond the conventional Generalized Additive Model framework by considering models like Graph Convolutional Networks or Graph SAGE. These graph-based models enable the incorporation of various levels of interconnectedness and information sharing among nodes, where each node corresponds to the combined load (i.e. consumption) of a subset of consumers (e.g. the regions of a country). More specifically, we introduce a range of methods for inferring graphs tailored to consumption forecasting, along with a framework for evaluating the developed models in terms of both performance and explainability. We conduct experiments on electricity forecasting, in both a synthetic and a real framework considering the French mainland regions, and the performance and merits of our approach are discussed. |
| 评估生成式语言模型在分类任务中的表现：环境与气候变化领域的性能与自我评估能力 | 本文研究了两个大型语言模型（LLMs）GPT3.5和Llama2以及一个小型语言模型（SLM）Gemma在气候变化（CC）和环境领域内三个不同分类任务中的表现。采用基于BERT的模型作为基线，我们将它们与这些基于transformer的模型进行效能比较。此外，我们通过分析这些文本分类任务中口头表述的置信度分数的校准情况，评估了模型的自我评估能力。我们的研究发现，尽管基于BERT的模型总体上优于LLMs和SLM，但大型生成模型的表现仍然值得注意。进一步的校准分析显示，虽然Gemma在初始任务中校准良好，但随后产生了不一致的结果；Llama校准合理，而GPT则始终表现出强校准。通过这项研究，我们旨在为关于生成式LM在解决地球上一些最紧迫问题中的实用性和有效性的持续讨论做出贡献，强调它们在生态和CC背景下的优势和局限性。 | Francesca Grasso | [PDF](http://arxiv.org/pdf/2408.17362v1) | N/A | Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain | This paper examines the performance of two Large Language Models (LLMs), GPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three different classification tasks within the climate change (CC) and environmental domain. Employing BERT-based models as a baseline, we compare their efficacy against these transformer-based models. Additionally, we assess the models' self-evaluation capabilities by analyzing the calibration of verbalized confidence scores in these text classification tasks. Our findings reveal that while BERT-based models generally outperform both the LLMs and SLM, the performance of the large generative models is still noteworthy. Furthermore, our calibration analysis reveals that although Gemma is well-calibrated in initial tasks, it thereafter produces inconsistent results; Llama is reasonably calibrated, and GPT consistently exhibits strong calibration. Through this research, we aim to contribute to the ongoing discussion on the utility and effectiveness of generative LMs in addressing some of the planet's most urgent issues, highlighting their strengths and limitations in the context of ecology and CC. |
| 《紧紧相拥：语音增强的稳定编码器-解码器设计》 | 一维卷积层常被用作音频信号编码的前端，这些层使用一维滤波器。与固定的时间-频率表示不同，它们能够适应输入数据的局部特征。然而，在原始音频上训练一维滤波器较为困难，且常常面临不稳定性问题。在本文中，我们通过混合解决方案来应对这些问题，即结合理论驱动和数据驱动的方法。首先，我们通过听觉滤波器组对音频信号进行预处理，确保学习到的编码器具有良好的频率定位能力。其次，我们利用框架理论的结果定义了一个无监督学习目标，该目标鼓励能量守恒和完美重建。第三，我们将混合压缩谱范数作为学习目标应用于编码器系数。在低复杂度的编码器-掩码-解码器模型中应用这些解决方案，显著提升了语音增强中的语音质量感知评估（PESQ）。 | Daniel Haider | [PDF](http://arxiv.org/pdf/2408.17358v1) | N/A | Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement | Convolutional layers with 1-D filters are often used as frontend to encode audio signals. Unlike fixed time-frequency representations, they can adapt to the local characteristics of input data. However, 1-D filters on raw audio are hard to train and often suffer from instabilities. In this paper, we address these problems with hybrid solutions, i.e., combining theory-driven and data-driven approaches. First, we preprocess the audio signals via a auditory filterbank, guaranteeing good frequency localization for the learned encoder. Second, we use results from frame theory to define an unsupervised learning objective that encourages energy conservation and perfect reconstruction. Third, we adapt mixed compressed spectral norms as learning objectives to the encoder coefficients. Using these solutions in a low-complexity encoder-mask-decoder model significantly improves the perceptual evaluation of speech quality (PESQ) in speech enhancement. |
| C-RADAR：一种用于软件定义网络入侵检测的集中式深度学习系统 | 软件定义网络（SDN）近年来因其简化网络管理和提高网络灵活性的能力而广受欢迎。然而，这也使得它们容易受到各种类型的网络攻击。SDN采用集中式控制平面工作，这使得它们更容易遭受网络攻击。研究表明，深度学习（DL）方法在识别传统网络中的入侵方面可以取得成功，但其在SDN中的应用仍是一个开放的研究领域。在本研究中，我们提出了使用深度学习技术进行SDN入侵检测的方法。我们通过在一个网络流量数据集上进行实验，并将其与现有技术进行比较，来衡量我们方法的有效性。我们的结果显示，基于深度学习的方法在检测准确性和计算效率方面优于传统方法。本研究中使用的深度学习架构是长短期记忆网络和自注意力机制的结合，即LSTM-Attn，其F1分数达到了0.9721。此外，这种技术可以被训练来检测新的攻击模式，并提高SDN的整体安全性。 | Osama Mustafa | [PDF](http://arxiv.org/pdf/2408.17356v1) | N/A | C-RADAR: A Centralized Deep Learning System for Intrusion Detection in Software Defined Networks | The popularity of Software Defined Networks (SDNs) has grown in recent years, mainly because of their ability to simplify network management and improve network flexibility. However, this also makes them vulnerable to various types of cyber attacks. SDNs work on a centralized control plane which makes them more prone to network attacks. Research has demonstrated that deep learning (DL) methods can be successful in identifying intrusions in conventional networks, but their application in SDNs is still an open research area. In this research, we propose the use of DL techniques for intrusion detection in SDNs. We measure the effectiveness of our method by experimentation on a dataset of network traffic and comparing it to existing techniques. Our results show that the DL-based approach outperforms traditional methods in terms of detection accuracy and computational efficiency. The deep learning architecture that has been used in this research is a Long Short Term Memory Network and Self-Attention based architecture i.e. LSTM-Attn which achieves an Fl-score of 0.9721. Furthermore, this technique can be trained to detect new attack patterns and improve the overall security of SDNs. |
| 双向解码：通过闭环重采样改进动作分块 | 预测并执行一系列动作而不进行中间重规划，即所谓的动作分块，在从人类演示中学习机器人技能的过程中越来越常用。然而，动作分块对学习到的策略的影响仍是一个谜：一些研究表明它对于实现高性能至关重要，而其他研究则观察到其有害效果。在本文中，我们首先通过分析学习者与演示者之间的差异来剖析动作分块的作用。我们发现，较长的动作块能够让策略更好地捕捉时间依赖性，因为它考虑了块内更多的过去状态和动作。然而，这种优势是以在随机环境中加剧近期状态观察较少导致的错误为代价的。为了解决这一问题，我们提出了双向解码（Bidirectional Decoding, BID），这是一种测试时推理算法，它将动作分块与闭环操作结合起来。BID在每个时间步采样多个预测，并根据两个标准搜索最优的预测：(i) 向后一致性，倾向于与先前决策相符的样本；(ii) 向前对比，倾向于接近更强策略输出且远离较弱策略输出的样本。通过在动作块内和块间耦合决策，BID在延长序列中增强了时间一致性，同时使策略能够在随机环境中进行自适应重规划。实验结果显示，BID在七个模拟基准和两个真实世界任务上显著优于两种最先进的生成策略的传统闭环操作。 | Yuejiang Liu | [PDF](http://arxiv.org/pdf/2408.17355v1) | N/A | Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling | Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. However, its effects on learned policies remain puzzling: some studies highlight its importance for achieving strong performance, while others observe detrimental effects. In this paper, we first dissect the role of action chunking by analyzing the divergence between the learner and the demonstrator. We find that longer action chunks enable a policy to better capture temporal dependencies by taking into account more past states and actions within the chunk. However, this advantage comes at the cost of exacerbating errors in stochastic environments due to fewer observations of recent states. To address this, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop operations. BID samples multiple predictions at each time step and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples aligned with previous decisions, (ii) forward contrast, which favors samples close to outputs of a stronger policy and distant from those of a weaker policy. By coupling decisions within and across action chunks, BID enhances temporal consistency over extended sequences while enabling adaptive replanning in stochastic environments. Experimental results show that BID substantially outperforms conventional closed-loop operations of two state-of-the-art generative policies across seven simulation benchmarks and two real-world tasks. |
| 遗忘以繁荣：利用预训练语言模型上的机器遗忘技术防范隐私泄露 | 在下游应用中对大型语言模型进行私有数据微调，可能会暴露敏感信息，从而带来显著的隐私风险。目前，多个热门社区平台提供了便捷的预训练模型分发服务，允许任何人无需严格审核即可发布。这种情况造成了隐私威胁，因为预训练模型可能被故意设计来侵犯微调数据集的隐私。在本研究中，我们引入了一种新颖的中毒技术，利用模型遗忘作为攻击手段。该方法操纵预训练语言模型，在微调过程中增加私有数据的泄露。我们的方法既增强了成员推理和数据提取攻击，又保持了模型的实用性。在不同模型、数据集和微调设置下的实验结果表明，我们的攻击显著优于基准性能。这项工作对从非验证来源下载预训练模型的用户发出了警示，强调了潜在的风险。 | Md Rafi Ur Rashid | [PDF](http://arxiv.org/pdf/2408.17354v1) | N/A | Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage | Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved. |
| 评估医学深度神经网络的可靠性：基于特征和置信度的异常检测方法的关键分析 | 可靠地使用深度神经网络（DNNs）进行医学图像分析需要识别与训练数据显著不同的输入，即分布外（OOD）数据，以防止错误的预测。OOD检测方法可以分为基于置信度的（使用模型的输出层进行OOD检测）或基于特征的（不使用输出层）。我们通过将D7P（皮肤病学）和BreastMNIST（超声）数据集分成包含或不包含人工制品（分别是标尺或注释）的子集，创建了两个新的OOD基准。模型使用无人工制品的图像进行训练，而包含人工制品的图像则用作OOD测试集。对于每个OOD图像，我们通过手动去除人工制品来创建一个反事实图像，以评估人工制品对模型预测的影响。我们发现，由于训练数据中的相关性等因素，OOD人工制品可以提高模型对其预测的softmax置信度。这与通常认为OOD人工制品应导致更不确定输出的假设相矛盾，而大多数基于置信度的方法正是基于这一假设。我们利用这一点解释了为什么基于特征的方法（如马氏距离分数）通常比基于置信度的方法（如最大类间概率MCP）具有更好的OOD检测性能。然而，我们也表明，基于特征的方法在区分导致正确和错误预测的输入（无论是OOD还是ID数据）方面通常表现较差。基于这些见解，我们认为应在DNN流程中结合使用基于特征和基于置信度的方法，以缓解各自的弱点。该项目代码和OOD基准可在以下链接获取：https://github.com/HarryAnthony/Evaluating_OOD_detection。 | Harry Anthony | [PDF](http://arxiv.org/pdf/2408.17337v1) | N/A | Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature and Confidence-Based OOD Detection | Reliable use of deep neural networks (DNNs) for medical image analysis requires methods to identify inputs that differ significantly from the training data, called out-of-distribution (OOD), to prevent erroneous predictions. OOD detection methods can be categorised as either confidence-based (using the model's output layer for OOD detection) or feature-based (not using the output layer). We created two new OOD benchmarks by dividing the D7P (dermatology) and BreastMNIST (ultrasound) datasets into subsets which either contain or don't contain an artefact (rulers or annotations respectively). Models were trained with artefact-free images, and images with the artefacts were used as OOD test sets. For each OOD image, we created a counterfactual by manually removing the artefact via image processing, to assess the artefact's impact on the model's predictions. We show that OOD artefacts can boost a model's softmax confidence in its predictions, due to correlations in training data among other factors. This contradicts the common assumption that OOD artefacts should lead to more uncertain outputs, an assumption on which most confidence-based methods rely. We use this to explain why feature-based methods (e.g. Mahalanobis score) typically have greater OOD detection performance than confidence-based methods (e.g. MCP). However, we also show that feature-based methods typically perform worse at distinguishing between inputs that lead to correct and incorrect predictions (for both OOD and ID data). Following from these insights, we argue that a combination of feature-based and confidence-based methods should be used within DNN pipelines to mitigate their respective weaknesses. These project's code and OOD benchmarks are available at: https://github.com/HarryAnthony/Evaluating_OOD_detection. |
| 从心电图特征估算心脏与非心脏诊断 | 引言：确保医疗状况的及时准确诊断对于有效的患者护理至关重要。心电图（ECG）信号是评估患者心脏健康的基础，并且易于获取。尽管如此，ECG数据在检测非心脏状况方面的巨大潜力却鲜少受到关注。方法：在我们的研究中，我们使用了公开可用的数据集（MIMIC-IV-ECG-ICD和ECG-VIEW II）来探讨从ECG特征推断一般诊断状况的可行性。为此，我们基于ECG特征和基本人口统计特征训练了一个基于树的模型（XGBoost），以估计广泛范围的诊断，包括心脏和非心脏状况。结果：我们的结果显示，在广泛的生理类别中，以统计学上显著的方式，可靠地估计了23种心脏状况以及21种非心脏状况，AUROC值均超过0.7。我们的发现强调了ECG数据在识别已知心脏状况方面的预测潜力。然而，更为引人注目的是，这项研究代表了系统性地扩展基于ECG诊断范围到传统上不与心脏系统相关联的状况的开创性努力。 | Juan Miguel Lopez Alcaraz | [PDF](http://arxiv.org/pdf/2408.17329v1) | N/A | Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram Features | Introduction: Ensuring timely and accurate diagnosis of medical conditions is paramount for effective patient care. Electrocardiogram (ECG) signals are fundamental for evaluating a patient's cardiac health and are readily available. Despite this, little attention has been given to the remarkable potential of ECG data in detecting non-cardiac conditions.   Methods: In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD and ECG-VIEW II) to investigate the feasibility of inferring general diagnostic conditions from ECG features. To this end, we trained a tree-based model (XGBoost) based on ECG features and basic demographic features to estimate a wide range of diagnoses, encompassing both cardiac and non-cardiac conditions.   Results: Our results demonstrate the reliability of estimating 23 cardiac as well as 21 non-cardiac conditions above 0.7 AUROC in a statistically significant manner across a wide range of physiological categories. Our findings underscore the predictive potential of ECG data in identifying well-known cardiac conditions. However, even more striking, this research represents a pioneering effort in systematically expanding the scope of ECG-based diagnosis to conditions not traditionally associated with the cardiac system. |
| ChatGPT对凝聚态物理学家写作风格的影响 | 我们采用了一种先进的差异分析方法来评估ChatGPT发布对arXiv上凝聚态物理论文写作风格的影响。我们的分析显示，非英语母语者在撰写摘要时，英语质量有了统计学意义上的显著提升。重要的是，即使在考虑了其他潜在因素后，这一提升依然稳固，证实了这一变化可以归因于ChatGPT的发布。这表明该工具已被广泛采纳。ChatGPT发布后，独特词汇的使用显著增加，而罕见词汇的使用频率则有所下降。在不同语系中，写作风格的变化在拉丁语系和乌拉尔-阿尔泰语系的作者中尤为显著，而在日耳曼语系或其他印欧语系的作者中则不然。 | Shaojun Xu | [PDF](http://arxiv.org/pdf/2408.17325v1) | N/A | Impact of ChatGPT on the writing style of condensed matter physicists | We apply a state-of-the-art difference-in-differences approach to estimate the impact of ChatGPT's release on the writing style of condensed matter papers on arXiv. Our analysis reveals a statistically significant improvement in the English quality of abstracts written by non-native English speakers. Importantly, this improvement remains robust even after accounting for other potential factors, confirming that it can be attributed to the release of ChatGPT. This indicates widespread adoption of the tool. Following the release of ChatGPT, there is a significant increase in the use of unique words, while the frequency of rare words decreases. Across language families, the changes in writing style are significant for authors from the Latin and Ural-Altaic groups, but not for those from the Germanic or other Indo-European groups. |
| 变压器中的模块化：探究神经元分离性与专业化 | Transformer模型在各种应用中日益普及，然而我们对其内部工作机制的理解仍然有限。本文研究了Transformer架构中神经元的模块化和任务特化，重点关注视觉（ViT）和语言（Mistral 7B）模型。通过结合选择性剪枝和MoEfication聚类技术，我们分析了不同任务和数据子集之间神经元的重叠和特化情况。我们的研究发现，存在任务特定的神经元集群，相关任务之间存在不同程度的重叠。我们观察到，即使在随机初始化的模型中，神经元重要性模式在一定程度上也得以保持，这表明存在一种训练所精炼的固有结构。此外，我们发现通过MoEfication识别的神经元集群在模型的早期和晚期层中与任务特定的神经元有更强的对应关系。这项工作有助于更细致地理解Transformer的内部机制，并为提高模型解释性和效率的潜在途径提供了见解。 | Nicholas Pochinkov | [PDF](http://arxiv.org/pdf/2408.17324v1) | N/A | Modularity in Transformers: Investigating Neuron Separability & Specialization | Transformer models are increasingly prevalent in various applications, yet our understanding of their internal workings remains limited. This paper investigates the modularity and task specialization of neurons within transformer architectures, focusing on both vision (ViT) and language (Mistral 7B) models. Using a combination of selective pruning and MoEfication clustering techniques, we analyze the overlap and specialization of neurons across different tasks and data subsets. Our findings reveal evidence of task-specific neuron clusters, with varying degrees of overlap between related tasks. We observe that neuron importance patterns persist to some extent even in randomly initialized models, suggesting an inherent structure that training refines. Additionally, we find that neuron clusters identified through MoEfication correspond more strongly to task-specific neurons in earlier and later layers of the models. This work contributes to a more nuanced understanding of transformer internals and offers insights into potential avenues for improving model interpretability and efficiency. |
| 探究注意力头中神经元消融现象：峰值激活中心化案例 | 基于Transformer的模型在社会中的应用正在迅速增长。随着这种增长，理解它们的工作原理，特别是注意力机制如何表示概念，变得至关重要。尽管存在许多可解释性方法，但许多方法通过神经元激活来观察模型，而这些激活的机制尚未被充分理解。我们描述了不同的视角来观察神经元激活，并通过多种神经元消融方法，即零消融、均值消融、激活重采样以及我们称之为“峰值消融”的新方法，研究了其在语言模型和视觉Transformer中的有效性。通过实验分析，我们发现，在不同的模型和情况下，每种方法都能在相比其他方法中提供最低的模型性能退化，而重采样通常会导致最显著的性能下降。我们在https://github.com/nickypro/investigating-ablation提供了我们的代码。 | Nicholas Pochinkov | [PDF](http://arxiv.org/pdf/2408.17322v1) | N/A | Investigating Neuron Ablation in Attention Heads: The Case for Peak Activation Centering | The use of transformer-based models is growing rapidly throughout society. With this growth, it is important to understand how they work, and in particular, how the attention mechanisms represent concepts. Though there are many interpretability methods, many look at models through their neuronal activations, which are poorly understood. We describe different lenses through which to view neuron activations, and investigate the effectiveness in language models and vision transformers through various methods of neural ablation: zero ablation, mean ablation, activation resampling, and a novel approach we term 'peak ablation'. Through experimental analysis, we find that in different regimes and models, each method can offer the lowest degradation of model performance compared to other methods, with resampling usually causing the most significant performance deterioration. We make our code available at https://github.com/nickypro/investigating-ablation. |
| 利用大型语言模型连接领域知识和过程发现 | 发现良好的过程模型对于各种过程分析任务至关重要，如一致性检查和过程改进。自动化的过程发现方法往往忽视了宝贵的领域知识。这些知识，包括领域专家的见解和详细的流程文档，在流程发现过程中很大程度上未被利用。本文利用大型语言模型（LLMs）将此类知识直接整合到流程发现中。我们使用从LLMs中提取的规则来指导模型构建，确保与领域知识和实际流程执行的一致性。通过整合LLMs，我们在自然语言表达的流程知识和稳健流程模型的发现之间架起了一座桥梁，显著推进了流程发现方法论。为了展示我们框架的可用性，我们与UWV员工保险机构进行了案例研究，证明了其实际效益和有效性。 | Ali Norouzifar | [PDF](http://arxiv.org/pdf/2408.17316v1) | N/A | Bridging Domain Knowledge and Process Discovery Using Large Language Models | Discovering good process models is essential for different process analysis tasks such as conformance checking and process improvements. Automated process discovery methods often overlook valuable domain knowledge. This knowledge, including insights from domain experts and detailed process documentation, remains largely untapped during process discovery. This paper leverages Large Language Models (LLMs) to integrate such knowledge directly into process discovery. We use rules derived from LLMs to guide model construction, ensuring alignment with both domain knowledge and actual process executions. By integrating LLMs, we create a bridge between process knowledge expressed in natural language and the discovery of robust process models, advancing process discovery methodologies significantly. To showcase the usability of our framework, we conducted a case study with the UWV employee insurance agency, demonstrating its practical benefits and effectiveness. |
| 在固定置信度下进行最佳臂识别 | 在这项工作中，我们提出了一种新颖的框架，用于在公平性约束下的最佳臂识别（Best Arm Identification, BAI），我们称之为 \textit{F-BAI}（公平 BAI）。与传统 BAI 仅关注以最小样本复杂度识别最优臂不同，F-BAI 还包含一组公平性约束。这些约束对每个臂的选择率设定了下限，并且可以是模型无关的或模型依赖的。针对这一设定，我们建立了实例特定的样本复杂度下界，并分析了 \textit{公平性代价}，量化了公平性如何影响样本复杂度。基于样本复杂度下界，我们提出了 F-TaS 算法，该算法在确保满足公平性约束的同时，证明了与样本复杂度下界匹配。通过使用合成模型和实际无线调度应用进行的数值结果显示，F-TaS 在实现低公平性违规的同时，有效地最小化了样本复杂度。 | Alessio Russo | [PDF](http://arxiv.org/pdf/2408.17313v1) | N/A | Fair Best Arm Identification with Fixed Confidence | In this work, we present a novel framework for Best Arm Identification (BAI) under fairness constraints, a setting that we refer to as \textit{F-BAI} (fair BAI). Unlike traditional BAI, which solely focuses on identifying the optimal arm with minimal sample complexity, F-BAI also includes a set of fairness constraints. These constraints impose a lower limit on the selection rate of each arm and can be either model-agnostic or model-dependent. For this setting, we establish an instance-specific sample complexity lower bound and analyze the \textit{price of fairness}, quantifying how fairness impacts sample complexity. Based on the sample complexity lower bound, we propose F-TaS, an algorithm provably matching the sample complexity lower bound, while ensuring that the fairness constraints are satisfied. Numerical results, conducted using both a synthetic model and a practical wireless scheduling application, show the efficiency of F-TaS in minimizing the sample complexity while achieving low fairness violations. |
| 构建训练策略以通过现实图像增强来强化感知模型 | 推进基于机器学习（ML）的自主系统感知模型需要解决模型中的薄弱环节，尤其是在具有挑战性的操作设计域（ODD）中。这些是自动驾驶车辆的环境操作条件，可能包含困难的情况，例如夜间镜头眩光或湿滑街道上的物体反射。本报告介绍了一种新颖的训练方法，通过增强来提高模型在这些条件下的鲁棒性和性能。所提出的方法利用定制的基于物理的增强功能，生成模拟多样ODD场景的真实训练数据。

我们提出一个全面的框架，包括识别ML模型中的薄弱环节、选择合适的增强方法以及制定有效的训练策略。该方法集成了超参数优化和潜在空间优化，以微调增强参数，确保它们最大限度地提高ML模型的性能。实验结果显示，模型性能有所提升，通过常用的指标如平均精度（mAP）和平均交并比（mIoU）衡量，在开源对象检测和语义分割模型及数据集上表现出色。

我们的研究强调，最佳训练策略是模型和数据特定的，并突出了将增强集成到训练流程中的益处。通过加入增强，我们观察到基于ML的感知模型鲁棒性增强，使其更能适应现实世界ODD中遇到的边缘情况。这项工作强调了定制增强的重要性，并提供了一个有效解决方案，以提高自动驾驶功能的安全性和可靠性。 | Ahmed Hammam | [PDF](http://arxiv.org/pdf/2408.17311v1) | N/A | Structuring a Training Strategy to Robustify Perception Models with Realistic Image Augmentations | Advancing Machine Learning (ML)-based perception models for autonomous systems necessitates addressing weak spots within the models, particularly in challenging Operational Design Domains (ODDs). These are environmental operating conditions of an autonomous vehicle which can contain difficult conditions, e.g., lens flare at night or objects reflected in a wet street. This report introduces a novel methodology for training with augmentations to enhance model robustness and performance in such conditions. The proposed approach leverages customized physics-based augmentation functions, to generate realistic training data that simulates diverse ODD scenarios.   We present a comprehensive framework that includes identifying weak spots in ML models, selecting suitable augmentations, and devising effective training strategies. The methodology integrates hyperparameter optimization and latent space optimization to fine-tune augmentation parameters, ensuring they maximally improve the ML models' performance. Experimental results demonstrate improvements in model performance, as measured by commonly used metrics such as mean Average Precision (mAP) and mean Intersection over Union (mIoU) on open-source object detection and semantic segmentation models and datasets.   Our findings emphasize that optimal training strategies are model- and data-specific and highlight the benefits of integrating augmentations into the training pipeline. By incorporating augmentations, we observe enhanced robustness of ML-based perception models, making them more resilient to edge cases encountered in real-world ODDs. This work underlines the importance of customized augmentations and offers an effective solution for improving the safety and reliability of autonomous driving functions. |
| 朝向文学机器翻译中词汇多样性的定制化恢复 | 机器翻译被发现比人工翻译在词汇上更为贫乏。机器翻译过程中词汇多样性的丧失，在文学作品的自动翻译中成为一个问题，因为在文学翻译中，不仅关注写了什么，还关注是如何写的。目前增加机器翻译词汇多样性的方法较为僵化。然而，如我们所展示，不同小说之间的词汇多样性程度可以有显著差异。因此，我们不再追求僵化地增加词汇多样性，而是将任务重新定义为恢复机器翻译过程中丢失的内容。我们提出了一种新颖的方法，该方法包括使用一个区分原文和翻译文本的分类器对翻译候选进行重新排序。我们在31本英译荷的书籍翻译上评估了我们的方法，发现对于某些书籍，我们的方法恢复的词汇多样性得分接近人工翻译的水平。 | Esther Ploeger | [PDF](http://arxiv.org/pdf/2408.17308v1) | N/A | Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation | Machine translations are found to be lexically poorer than human translations. The loss of lexical diversity through MT poses an issue in the automatic translation of literature, where it matters not only what is written, but also how it is written. Current methods for increasing lexical diversity in MT are rigid. Yet, as we demonstrate, the degree of lexical diversity can vary considerably across different novels. Thus, rather than aiming for the rigid increase of lexical diversity, we reframe the task as recovering what is lost in the machine translation process. We propose a novel approach that consists of reranking translation candidates with a classifier that distinguishes between original and translated text. We evaluate our approach on 31 English-to-Dutch book translations, and find that, for certain books, our approach retrieves lexical diversity scores that are close to human translation. |
| 将基线2D-CNN模型与猫群优化混合，以增强高级持续威胁检测 | 在网络安全领域，由于高级持续性威胁（APT）的隐蔽性和复杂性，检测它们仍然是一个艰巨的挑战。本研究论文提出了一种创新方法，通过利用带有二维基准模型的卷积神经网络（CNN），并结合尖端的猫群优化（CSO）算法，显著提高了APT检测的准确性。通过将二维CNN基准模型与CSO无缝集成，我们开启了在APT检测中实现前所未有的准确性和效率的潜力。结果显示，检测准确率达到了惊人的98.4%，标志着在各种攻击阶段中APT检测的显著提升，为对抗这些持续且复杂的威胁指明了前进的道路。 | Ali M. Bakhiet | [PDF](http://arxiv.org/pdf/2408.17307v1) | N/A | Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for Enhanced Advanced Persistent Threat Detection | In the realm of cyber-security, detecting Advanced Persistent Threats (APTs) remains a formidable challenge due to their stealthy and sophisticated nature. This research paper presents an innovative approach that leverages Convolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the cutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve APT detection accuracy. By seamlessly integrating the 2D-CNN baseline model with CSO, we unlock the potential for unprecedented accuracy and efficiency in APT detection. The results unveil an impressive accuracy score of $98.4\%$, marking a significant enhancement in APT detection across various attack stages, illuminating a path forward in combating these relentless and sophisticated threats. |
| 利用机器学习加速行星内部动力学稳态的发现 | 模拟地幔对流通常需要达到计算成本高昂的稳态，这对于推导热力学和动力学流动特性的标度律以及基准数值解至关重要。地幔岩石流变性的强烈温度依赖性导致粘度变化高达几个数量级，从而形成一个缓慢演化的停滞盖层，其中热传导占主导地位，覆盖在一个快速演化且强烈对流的区域之上。尽管时间步进方法对于粘度恒定的流体有效，但由于库朗准则限制了时间步长基于系统的最大速度和网格大小，因此受到阻碍。因此，由于控制停滞和对流区域的截然不同的时间尺度，达到稳态需要大量时间步长。我们提出了一种利用机器学习加速地幔对流模拟的概念。我们生成了128个具有混合基底和内部加热以及压力和温度依赖粘度的二维模拟数据集。我们在97个模拟上训练了一个前馈神经网络，以预测稳态温度分布。然后可以将其用于初始化不同模拟参数的数值时间步进方法。与典型初始化相比，达到稳态所需的时间步长数量减少了中位数因子3.75。这种方法的好处在于只需要非常少的模拟进行训练，提供了一个无预测误差的解决方案，因为我们初始化了一种数值方法，并且在推理时计算开销最小。我们展示了我们方法的有效性，并讨论了加速模拟对推进地幔对流研究的潜在影响。 | Siddhant Agarwal | [PDF](http://arxiv.org/pdf/2408.17298v1) | N/A | Accelerating the discovery of steady-states of planetary interior dynamics with machine learning | Simulating mantle convection often requires reaching a computationally expensive steady-state, crucial for deriving scaling laws for thermal and dynamical flow properties and benchmarking numerical solutions. The strong temperature dependence of the rheology of mantle rocks causes viscosity variations of several orders of magnitude, leading to a slow-evolving stagnant lid where heat conduction dominates, overlying a rapidly-evolving and strongly convecting region. Time-stepping methods, while effective for fluids with constant viscosity, are hindered by the Courant criterion, which restricts the time step based on the system's maximum velocity and grid size. Consequently, achieving steady-state requires a large number of time steps due to the disparate time scales governing the stagnant and convecting regions.   We present a concept for accelerating mantle convection simulations using machine learning. We generate a dataset of 128 two-dimensional simulations with mixed basal and internal heating, and pressure- and temperature-dependent viscosity. We train a feedforward neural network on 97 simulations to predict steady-state temperature profiles. These can then be used to initialize numerical time stepping methods for different simulation parameters. Compared to typical initializations, the number of time steps required to reach steady-state is reduced by a median factor of 3.75. The benefit of this method lies in requiring very few simulations to train on, providing a solution with no prediction error as we initialize a numerical method, and posing minimal computational overhead at inference time. We demonstrate the effectiveness of our approach and discuss the potential implications for accelerated simulations for advancing mantle convection research. |
| 在风险规避型总奖励马尔可夫决策过程（MDPs）中，固定策略是最优的，其中使用了期望值-在风险（EVaR）的度量方法。 | 在折扣马尔可夫决策过程（MDPs）中优化风险规避目标具有挑战性，因为大多数模型不承认直接的动态规划方程，并且需要复杂的依赖历史的策略。在本文中，我们证明在熵风险度量（ERM）和熵在险价值（EVaR）风险度量下，风险规避的**总奖励准则**可以通过固定策略进行优化，从而使其易于分析、解释和部署。我们提出了指数值迭代、策略迭代和线性规划来计算最优策略。与先前的工作相比，我们的结果仅需要相对温和的瞬态MDPs条件，并且允许**同时**存在正负奖励。我们的研究结果表明，在广泛的规避风险强化学习领域中，总奖励准则可能优于折扣准则。 | Xihong Su | [PDF](http://arxiv.org/pdf/2408.17286v1) | N/A | Stationary Policies are Optimal in Risk-averse Total-reward MDPs with EVaR | Optimizing risk-averse objectives in discounted MDPs is challenging because most models do not admit direct dynamic programming equations and require complex history-dependent policies. In this paper, we show that the risk-averse {\em total reward criterion}, under the Entropic Risk Measure (ERM) and Entropic Value at Risk (EVaR) risk measures, can be optimized by a stationary policy, making it simple to analyze, interpret, and deploy. We propose exponential value iteration, policy iteration, and linear programming to compute optimal policies. In comparison with prior work, our results only require the relatively mild condition of transient MDPs and allow for {\em both} positive and negative rewards. Our results indicate that the total reward criterion may be preferable to the discounted criterion in a broad range of risk-averse reinforcement learning domains. |
| 图像中的完美瑕疵：在文本到图像模型演进之影下，安全、偏见与真实性的考量 | 文本到图像模型，如稳定扩散（SD），经历迭代更新以提升图像质量并解决如安全性等关切。图像质量的改进易于评估。然而，模型更新如何解决既有问题以及是否带来新的疑问仍待探索。本研究首次从安全性、偏见和真实性角度探讨了文本到图像模型的演变。以稳定扩散为中心的发现表明，模型更新呈现复杂态势。虽然更新逐步减少了不安全图像的生成，但偏见问题，尤其是性别偏见，却有所加剧。我们还发现，负面刻板印象要么在同一非白人种族群体内持续存在，要么通过SD更新转向其他非白人种族群体，而这些特质与白人种族群体的关联甚微。此外，我们的评估揭示了SD更新引发的新问题：最初为早期SD版本训练的尖端假图像检测器在识别更新版本生成的假图像时遇到困难。我们展示了，在更新版本生成的假图像上微调这些检测器，能在多个SD版本上达到至少96.6%的准确率，从而解决这一问题。我们的见解强调了持续努力以减轻演化中的文本到图像模型偏见和脆弱性的重要性。 | Yixin Wu | [PDF](http://arxiv.org/pdf/2408.17285v1) | N/A | Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution | Text-to-image models, such as Stable Diffusion (SD), undergo iterative updates to improve image quality and address concerns such as safety. Improvements in image quality are straightforward to assess. However, how model updates resolve existing concerns and whether they raise new questions remain unexplored. This study takes an initial step in investigating the evolution of text-to-image models from the perspectives of safety, bias, and authenticity. Our findings, centered on Stable Diffusion, indicate that model updates paint a mixed picture. While updates progressively reduce the generation of unsafe images, the bias issue, particularly in gender, intensifies. We also find that negative stereotypes either persist within the same Non-White race group or shift towards other Non-White race groups through SD updates, yet with minimal association of these traits with the White race group. Additionally, our evaluation reveals a new concern stemming from SD updates: State-of-the-art fake image detectors, initially trained for earlier SD versions, struggle to identify fake images generated by updated versions. We show that fine-tuning these detectors on fake images generated by updated versions achieves at least 96.6\% accuracy across various SD versions, addressing this issue. Our insights highlight the importance of continued efforts to mitigate biases and vulnerabilities in evolving text-to-image models. |
| 将大型语言模型灵活有效地融入领域专家混合体 | 我们提供了一套工具包，用于从训练好的模型中创建低成本的领域专家混合体（Mixture-of-Domain-Experts, MOE）。该工具包可用于从模型本身或从适配器创建混合体。我们进行了广泛的测试，并就如何使用该工具包定义生成的MOE架构提供指导。有一个公开的代码仓库可供使用。 | Rhui Dih Lee | [PDF](http://arxiv.org/pdf/2408.17280v1) | N/A | Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts | We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE) from trained models. The toolkit can be used for creating a mixture from models or from adapters. We perform extensive tests and offer guidance on defining the architecture of the resulting MOE using the toolkit. A public repository is available. |
| 最小化与高效通信的分布式最佳子集选择：具备预言机属性 | 金融、电子商务和社交媒体等领域大规模数据的爆炸性增长，已经超出了单机系统处理能力的范畴，从而推动了对分布式统计推断方法的需求。传统的分布式推断方法往往难以在高维数据集中实现真正的稀疏性，并且涉及高昂的计算成本。为此，我们提出了一种新颖的两阶段分布式最佳子集选择算法来解决这些问题。我们的方法首先通过遵循$\ell_0$范数约束的代理似然函数，高效地估计活跃集，从而有效降低维度并筛选出关键变量。随后，在活跃集内进行精细估计，确保稀疏估计值并匹配最小最大$\ell_2$误差界限。我们引入了一种新的拼接技术用于自适应参数选择，以应对在$\ell_0$约束和广义信息准则（GIC）下的子问题。我们的理论和数值研究表明，所提出的算法能够正确发现真实的稀疏模式，具有oracle属性，并显著降低了通信成本。这是分布式稀疏估计领域的一大进步。 | Jingguo Lan | [PDF](http://arxiv.org/pdf/2408.17276v1) | N/A | Minimax and Communication-Efficient Distributed Best Subset Selection with Oracle Property | The explosion of large-scale data in fields such as finance, e-commerce, and social media has outstripped the processing capabilities of single-machine systems, driving the need for distributed statistical inference methods. Traditional approaches to distributed inference often struggle with achieving true sparsity in high-dimensional datasets and involve high computational costs. We propose a novel, two-stage, distributed best subset selection algorithm to address these issues. Our approach starts by efficiently estimating the active set while adhering to the $\ell_0$ norm-constrained surrogate likelihood function, effectively reducing dimensionality and isolating key variables. A refined estimation within the active set follows, ensuring sparse estimates and matching the minimax $\ell_2$ error bound. We introduce a new splicing technique for adaptive parameter selection to tackle subproblems under $\ell_0$ constraints and a Generalized Information Criterion (GIC). Our theoretical and numerical studies show that the proposed algorithm correctly finds the true sparsity pattern, has the oracle property, and greatly lowers communication costs. This is a big step forward in distributed sparse estimation. |
| 下采样稀疏图卷积网络的可迁移性 | 本文提出了一种基于稀疏随机图模型的大规模稀疏图降采样方法，该方法允许调整不同的稀疏度水平。我们将稀疏性和拓扑相似性结合起来：稀疏图模型随着图规模的增大而降低节点连接概率，而降采样方法在此变化过程中保留特定的拓扑连接模式。基于降采样方法，我们推导了关于降采样稀疏图卷积网络（GCNs）的可迁移性界限，即更高的采样率、更大的平均度期望和更小的初始图规模会带来更好的降采样迁移性能。 | Qinji Shu | [PDF](http://arxiv.org/pdf/2408.17274v1) | N/A | The Transferability of Downsampling Sparse Graph Convolutional Networks | In this paper, we propose a large-scale sparse graph downsampling method based on a sparse random graph model, which allows for the adjustment of different sparsity levels. We combine sparsity and topological similarity: the sparse graph model reduces the node connection probability as the graph size increases, while the downsampling method preserves a specific topological connection pattern during this change. Based on the downsampling method, we derive a theoretical transferability bound about downsampling sparse graph convolutional networks (GCNs), that higher sampling rates, greater average degree expectations, and smaller initial graph sizes lead to better downsampling transferability performance. |
| 通过物理信息神经网络进行流体流动的方程识别 | 科学机器学习（SciML）方法，如物理信息神经网络（PINNs），被用于根据控制方程和小量数据来估计感兴趣的参数。然而，对于跨广泛的数学科学领域内的控制方程，PINNs在逆问题上的表现如何进行评估的研究还很少。我们基于带有旋转流动的二维Burgers方程的参数扫描，提出了一个新的且具有挑战性的逆PINNs基准问题。我们展示了一种新颖的策略，该策略在第一和第二阶优化之间交替进行，证明其优于典型的第一阶策略用于参数估计。此外，我们提出了一种新颖的数据驱动方法来描述PINN在逆设置中的有效性。PINNs的物理信息正则化使它们能够比数据驱动的基线更有效地利用小量数据。然而，无论是PINNs还是基线，在处理高度无粘流动时都可能无法恢复参数，这促使了进一步发展PINN方法的需求。 | Alexander New | [PDF](http://arxiv.org/pdf/2408.17271v1) | N/A | Equation identification for fluid flows via physics-informed neural networks | Scientific machine learning (SciML) methods such as physics-informed neural networks (PINNs) are used to estimate parameters of interest from governing equations and small quantities of data. However, there has been little work in assessing how well PINNs perform for inverse problems across wide ranges of governing equations across the mathematical sciences. We present a new and challenging benchmark problem for inverse PINNs based on a parametric sweep of the 2D Burgers' equation with rotational flow. We show that a novel strategy that alternates between first- and second-order optimization proves superior to typical first-order strategies for estimating parameters. In addition, we propose a novel data-driven method to characterize PINN effectiveness in the inverse setting. PINNs' physics-informed regularization enables them to leverage small quantities of data more efficiently than the data-driven baseline. However, both PINNs and the baseline can fail to recover parameters for highly inviscid flows, motivating the need for further development of PINN methods. |
| 城市范围内配送需求的联合估计与预测：基于大型语言模型的图学习方法 | 电子商务的蓬勃发展和城市化进程的加速显著加剧了城市地区的配送操作，增加了配送需求的数量和复杂性。数据驱动的预测方法，特别是那些利用机器学习技术的方法，已经出现来应对这些城市配送需求管理问题中的复杂性。一个特别紧迫且尚未得到充分研究的问题是城市范围内配送需求的联合估计和预测。为此，我们将这个问题形式化为一个基于图的时空学习任务。首先，我们构建了一个消息传递神经网络模型，以捕捉相关地区需求模式之间的相互作用。其次，通过利用大型语言模型的最新进展，我们从非结构化的位置数据中提取通用地理空间知识编码，并将其整合到需求预测器中。最后，为了促进模型的跨城市可迁移性，我们开发了一种端到端的归纳训练方案。在两个真实世界的配送数据集（包括中国和美国的八个城市）上进行的广泛实证研究表明，我们的模型在这些具有挑战性的任务中显著优于最先进的基线模型。 | Tong Nie | [PDF](http://arxiv.org/pdf/2408.17258v1) | N/A | Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach | The proliferation of e-commerce and urbanization has significantly intensified delivery operations in urban areas, boosting the volume and complexity of delivery demand. Data-driven predictive methods, especially those utilizing machine learning techniques, have emerged to handle these complexities in urban delivery demand management problems. One particularly pressing problem that has not yet been sufficiently studied is the joint estimation and prediction of city-wide delivery demand. To this end, we formulate this problem as a graph-based spatiotemporal learning task. First, a message-passing neural network model is formalized to capture the interaction between demand patterns of associated regions. Second, by exploiting recent advances in large language models, we extract general geospatial knowledge encodings from the unstructured locational data and integrate them into the demand predictor. Last, to encourage the cross-city transferability of the model, an inductive training scheme is developed in an end-to-end routine. Extensive empirical results on two real-world delivery datasets, including eight cities in China and the US, demonstrate that our model significantly outperforms state-of-the-art baselines in these challenging tasks. |
| 通过去噪进行晶体性质预测的自监督学习 | 精确预测晶体材料的性质对于定向发现至关重要，而这种预测越来越多地采用数据驱动模型来完成。然而，对于许多感兴趣的性质，已确定特定性质的材料数量远少于已知材料的总数。为了克服这一差异，我们提出了一种新颖的自监督学习（SSL）策略，用于材料性质预测。我们的方法，即晶体去噪自监督学习（CDSSL），通过一个基于在给定这些结构的扰动版本时恢复有效材料结构的预文本任务，对预测模型（例如，图网络）进行预训练。我们证明，在不同材料类型、性质和数据集大小的情况下，CDSSL模型表现优于未采用SSL训练的模型。 | Alexander New | [PDF](http://arxiv.org/pdf/2408.17255v1) | N/A | Self-supervised learning for crystal property prediction via denoising | Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes. |
| 学习和验证最大泰勒-神经网络李亚普诺夫函数 | 我们提出了一种新颖的神经网络架构，称为泰勒神经李亚普诺夫函数（Taylor-neural Lyapunov functions），旨在通过形式认证来近似李亚普诺夫函数。该架构创新性地编码局部近似，并通过利用神经网络来近似残差，将这些局部近似扩展到全局。我们的方法将估计最大吸引域的问题——特别是针对最大李亚普诺夫函数——重新构建成一个学习问题，确保通过鲁棒控制理论在原点周围实现收敛。物理信息驱动的机器学习技术进一步细化了最大吸引域的估计。值得注意的是，这种方法具有广泛的适用性，即使在缺乏模拟数据点的情况下也能有效运作。我们通过在多个示例中提供收敛性的数值证书来验证我们方法的有效性。我们提出的方法不仅与最先进的方案（如平方和与LyZNet）紧密竞争，而且在没有模拟数据的情况下也能取得可比的结果。这项工作代表了控制理论的重大进展，具有广泛的应用潜力，不仅限于稳定控制系统的设计。 | Matthieu Barreau | [PDF](http://arxiv.org/pdf/2408.17246v1) | N/A | Learning and Verifying Maximal Taylor-Neural Lyapunov functions | We introduce a novel neural network architecture, termed Taylor-neural Lyapunov functions, designed to approximate Lyapunov functions with formal certification. This architecture innovatively encodes local approximations and extends them globally by leveraging neural networks to approximate the residuals. Our method recasts the problem of estimating the largest region of attraction - specifically for maximal Lyapunov functions - into a learning problem, ensuring convergence around the origin through robust control theory. Physics-informed machine learning techniques further refine the estimation of the largest region of attraction. Remarkably, this method is versatile, operating effectively even without simulated data points. We validate the efficacy of our approach by providing numerical certificates of convergence across multiple examples. Our proposed methodology not only competes closely with state-of-the-art approaches, such as sum-of-squares and LyZNet, but also achieves comparable results even in the absence of simulated data. This work represents a significant advancement in control theory, with broad potential applications in the design of stable control systems and beyond. |
| 深度脉冲神经网络的分步加权脉冲编码 | 脉冲神经网络（SNNs）旨在模拟生物神经元的脉冲行为，并有望在神经计算和人工智能的发展中扮演关键角色。SNNs的效率通常由神经编码方案决定。现有的编码方案要么导致巨大的延迟和能量消耗，要么需要复杂的神经元模型和训练技术。为了解决这些问题，我们提出了一种新颖的逐步加权脉冲（SWS）编码方案，以增强脉冲中的信息编码。该方法通过在神经计算的每一步中对脉冲的重要性进行加权来压缩脉冲，实现了高性能和低能耗。我们提出了一种具有静默期的三元自放大（TSA）神经元模型，以支持基于SWS的计算，旨在最小化神经计算中逐步加权产生的残余误差。我们的实验结果表明，SWS编码方案在非常深的SNNs中优于现有的神经编码方案，并显著减少了操作和延迟。 | Yiwen Gu | [PDF](http://arxiv.org/pdf/2408.17245v1) | N/A | Stepwise Weighted Spike Coding for Deep Spiking Neural Networks | Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological neurons and are expected to play a key role in the advancement of neural computing and artificial intelligence. The efficiency of SNNs is often determined by the neural coding schemes. Existing coding schemes either cause huge delays and energy consumption or necessitate intricate neuron models and training techniques. To address these issues, we propose a novel Stepwise Weighted Spike (SWS) coding scheme to enhance the encoding of information in spikes. This approach compresses the spikes by weighting the significance of the spike in each step of neural computation, achieving high performance and low energy consumption. A Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for supporting SWS-based computing, aimed at minimizing the residual error resulting from stepwise weighting in neural computation. Our experimental results show that the SWS coding scheme outperforms the existing neural coding schemes in very deep SNNs, and significantly reduces operations and latency. |
| 分类数据聚类：超越K-modes的25年发展 | 分类数据聚类是计算机科学中一项常见且重要的任务，它在众多应用领域中具有深远的意义。与纯数值数据集不同，分类数据往往缺乏像名义数据那样的固有顺序，或像有序数据那样具有不同层次的顺序，因此需要专门的聚类方法来进行有效的组织和分析。本篇综述从K-modes算法的引入开始，全面总结了过去二十五年中分类数据聚类的研究进展。它阐明了分类数据聚类在健康科学、自然科学、社会科学、教育、工程和经济等众多领域中的关键作用。对具有公开实现算法的实际比较分析，突出了不同的聚类方法，并揭示了近期算法在若干基准分类数据集上的表现。最后，探讨了该领域面临的挑战与机遇。 | Tai Dinh | [PDF](http://arxiv.org/pdf/2408.17244v1) | N/A | Categorical data clustering: 25 years beyond K-modes | The clustering of categorical data is a common and important task in computer science, offering profound implications across a spectrum of applications. Unlike purely numerical datasets, categorical data often lack inherent ordering as in nominal data, or have varying levels of order as in ordinal data, thus requiring specialized methodologies for efficient organization and analysis. This review provides a comprehensive synthesis of categorical data clustering in the past twenty-five years, starting from the introduction of K-modes. It elucidates the pivotal role of categorical data clustering in diverse fields such as health sciences, natural sciences, social sciences, education, engineering and economics. Practical comparisons are conducted for algorithms having public implementations, highlighting distinguishing clustering methodologies and revealing the performance of recent algorithms on several benchmark categorical datasets. Finally, challenges and opportunities in the field are discussed. |
| 利用量子求解深度玻尔兹曼机提高强化学习代理的数据效率 | 深度学习算法，例如在强化学习中使用的那些，通常需要大量数据才能有效训练。在大多数情况下，数据的可用性并不是一个重大问题。然而，对于某些情境，例如在自主网络防御中，我们需要数据高效的方法。最近，量子机器学习和玻尔兹曼机已被提出作为解决这一挑战的方案。在这项工作中，我们在现有工作的基础上，将深度玻尔兹曼机的应用扩展到强化学习网络防御环境中的前沿算法——近端策略优化。我们展示，当使用D-WAVE量子退火器解决时，这种方法可以导致数据效率的双倍提升。因此，我们预期它将被机器学习和量子社区采用，他们希望利用数据高效的强化学习方法来获得优势。 | Daniel Kent | [PDF](http://arxiv.org/pdf/2408.17240v1) | N/A | Using Quantum Solved Deep Boltzmann Machines to Increase the Data Efficiency of RL Agents | Deep Learning algorithms, such as those used in Reinforcement Learning, often require large quantities of data to train effectively. In most cases, the availability of data is not a significant issue. However, for some contexts, such as in autonomous cyber defence, we require data efficient methods. Recently, Quantum Machine Learning and Boltzmann Machines have been proposed as solutions to this challenge. In this work we build upon the pre-existing work to extend the use of Deep Boltzmann Machines to the cutting edge algorithm Proximal Policy Optimisation in a Reinforcement Learning cyber defence environment. We show that this approach, when solved using a D-WAVE quantum annealer, can lead to a two-fold increase in data efficiency. We therefore expect it to be used by the machine learning and quantum communities who are hoping to capitalise on data-efficient Reinforcement Learning methods. |
| 基于AI的入侵检测系统（IDS）在ROAD数据集上的比较分析：针对汽车控制器局域网络（CAN） | 现代车辆中数字设备的整合彻底改变了汽车技术，提升了安全性和整体驾驶体验。控制器局域网络（CAN）总线是管理车辆内电子控制单元（ECU）间通信的核心系统。然而，由于CAN协议固有的脆弱性，缺乏加密和认证机制，加之攻击面的不断扩大，需要强有力的安全措施。针对这一挑战，已经开发并部署了多种入侵检测系统（IDS）。尽管如此，现有文献中仍缺乏一个开放、全面且真实的测试IDS有效性的数据集。本文通过考虑最新的ROAD数据集来填补这一空白，该数据集包含了隐秘且复杂的注入攻击。研究方法涉及数据集标记以及实施最先进的深度学习模型和传统机器学习模型，以展示文献中最常用的数据集与更真实的ROAD数据集之间在性能上的差异。 | Lorenzo Guerra | [PDF](http://arxiv.org/pdf/2408.17235v1) | N/A | AI-Driven Intrusion Detection Systems (IDS) on the ROAD dataset: A Comparative Analysis for automotive Controller Area Network (CAN) | The integration of digital devices in modern vehicles has revolutionized automotive technology, enhancing safety and the overall driving experience. The Controller Area Network (CAN) bus is a central system for managing in-vehicle communication between the electronic control units (ECUs). However, the CAN protocol poses security challenges due to inherent vulnerabilities, lacking encryption and authentication, which, combined with an expanding attack surface, necessitates robust security measures. In response to this challenge, numerous Intrusion Detection Systems (IDS) have been developed and deployed. Nonetheless, an open, comprehensive, and realistic dataset to test the effectiveness of such IDSs remains absent in the existing literature. This paper addresses this gap by considering the latest ROAD dataset, containing stealthy and sophisticated injections. The methodology involves dataset labelling and the implementation of both state-of-the-art deep learning models and traditional machine learning models to show the discrepancy in performance between the datasets most commonly used in the literature and the ROAD dataset, a more realistic alternative. |
| 闪电自注意力几何：可识别性与维度 | 我们考虑由未经归一化的自注意力网络定义的函数空间，并从理论上分析其几何结构。由于这些网络是多项式的，我们借助代数几何的工具。具体而言，我们通过描述任意层数的参数化通用纤维，研究了深度注意力的可识别性，并由此计算了函数空间的维度。此外，对于单层模型，我们刻画了奇异点和边界点。最后，我们提出了将我们的结果推广到归一化自注意力网络的猜想，为单层情况提供了证明，并在深度情况下进行了数值验证。 | Nathan W. Henry | [PDF](http://arxiv.org/pdf/2408.17221v1) | N/A | Geometry of Lightning Self-Attention: Identifiability and Dimension | We consider function spaces defined by self-attention networks without normalization, and theoretically analyze their geometry. Since these networks are polynomial, we rely on tools from algebraic geometry. In particular, we study the identifiability of deep attention by providing a description of the generic fibers of the parametrization for an arbitrary number of layers and, as a consequence, compute the dimension of the function space. Additionally, for a single-layer model, we characterize the singular and boundary points. Finally, we formulate a conjectural extension of our results to normalized self-attention networks, prove it for a single layer, and numerically verify it in the deep case. |
| 在非洲普及人工智能：面向低资源边缘设备的联邦学习 | 非洲在医疗保健服务方面面临重大挑战，主要源于基础设施有限和缺乏先进的医疗技术。本研究探讨了利用联邦学习来克服这些障碍，特别是针对围产期健康领域。我们利用来自五个非洲国家（阿尔及利亚、加纳、埃及、马拉维和乌干达）以及西班牙医院的围产期数据，训练了一个胎儿平面分类器。为了考虑到计算资源的匮乏，我们在模型训练中采用了多种设备，包括树莓派和多台笔记本电脑。尽管存在计算限制，我们展示了集中式模型与联邦模型之间的性能比较，并且与仅在本地训练的模型相比，联邦模型在模型泛化性方面取得了显著提升。这些结果表明，未来有望大规模实施联邦学习平台，以弥合可及性差距并显著提升模型的泛化能力，而所需的条件极为有限。 | Jorge Fabila | [PDF](http://arxiv.org/pdf/2408.17216v1) | N/A | Democratizing AI in Africa: FL for Low-Resource Edge Devices | Africa faces significant challenges in healthcare delivery due to limited infrastructure and access to advanced medical technologies. This study explores the use of federated learning to overcome these barriers, focusing on perinatal health. We trained a fetal plane classifier using perinatal data from five African countries: Algeria, Ghana, Egypt, Malawi, and Uganda, along with data from Spanish hospitals. To incorporate the lack of computational resources in the analysis, we considered a heterogeneous set of devices, including a Raspberry Pi and several laptops, for model training. We demonstrate comparative performance between a centralized and a federated model, despite the compute limitations, and a significant improvement in model generalizability when compared to models trained only locally. These results show the potential for a future implementation at a large scale of a federated learning platform to bridge the accessibility gap and improve model generalizability with very little requirements. |
| 朝着符号化可解释人工智能（XAI）—— 通过特征间人类可理解的逻辑关系进行解释 | 可解释的人工智能（XAI）在促进AI系统的透明度和信任方面扮演着至关重要的角色。传统的XAI方法通常提供一个抽象层次的解释，这些解释往往以热图的形式突出显示单个或多个输入特征。然而，我们探讨了模型的抽象推理或问题解决策略是否同样重要，因为这些更接近人类解决问题的方式。我们提出了一种名为符号XAI的框架，该框架将相关性归因于表达输入特征之间逻辑关系的符号查询，从而捕捉模型预测背后的抽象推理。该方法建立在模型预测的简单而通用的多阶分解基础上。这种分解可以通过基于高阶传播的相关性方法（如GNN-LRP）或XAI中常用的基于扰动的解释方法来具体化。我们的框架在自然语言处理（NLP）、视觉和量子化学（QC）等领域展示了其有效性，这些领域拥有丰富的抽象符号领域知识，对用户具有重要意义。符号XAI框架提供了对模型决策过程的理解，既可以根据用户需求进行定制，又通过逻辑公式保持人类可读性。 | Thomas Schnake | [PDF](http://arxiv.org/pdf/2408.17198v1) | N/A | Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features | Explainable Artificial Intelligence (XAI) plays a crucial role in fostering transparency and trust in AI systems, where traditional XAI approaches typically offer one level of abstraction for explanations, often in the form of heatmaps highlighting single or multiple input features. However, we ask whether abstract reasoning or problem-solving strategies of a model may also be relevant, as these align more closely with how humans approach solutions to problems. We propose a framework, called Symbolic XAI, that attributes relevance to symbolic queries expressing logical relationships between input features, thereby capturing the abstract reasoning behind a model's predictions. The methodology is built upon a simple yet general multi-order decomposition of model predictions. This decomposition can be specified using higher-order propagation-based relevance methods, such as GNN-LRP, or perturbation-based explanation methods commonly used in XAI. The effectiveness of our framework is demonstrated in the domains of natural language processing (NLP), vision, and quantum chemistry (QC), where abstract symbolic domain knowledge is abundant and of significant interest to users. The Symbolic XAI framework provides an understanding of the model's decision-making process that is both flexible for customization by the user and human-readable through logical formulas. |
| 基于混合LSSVM-SVMD方法的智能电网短期风速预测与功率集成 | 由于其极低的污染和高效的能源利用，风能已成为最广泛开发的可再生能源之一。成功将风电并入电网系统取决于准确的风速预测模型。然而，风速预测任务具有挑战性，因为风速本身具有间歇性特征。本文开发了一种混合机器学习方法来预测短期风速。首先，使用连续变分模态分解（SVMD）将风数据分解为模态分量。然后，每个子信号被拟合到最小二乘支持向量机（LSSVM）模型中，其超参数通过量子行为粒子群优化（QPSO）的新变体——精英繁殖QPSO（EBQPSO）进行优化。其次，用于弥补原始风速序列与SVMD模态总和之间差异的残差，使用长短期记忆模型（LSTM）进行建模。然后，利用LSSVM和LSTM模型的总和计算整体预测值。最后，将所提出的模型与从当地风电场收集的两个独立数据集上进行风速预测的最先进基准模型进行性能比较。实证结果显示，所提出的方法在性能上有显著提升，与基准方法相比，实现了1.21%至32.76%的均方根误差（RMSE）降低和2.05%至40.75%的平均绝对误差（MAE）降低。这项工作的全部代码实现在Github上免费提供。 | Ephrem Admasu Yekun | [PDF](http://arxiv.org/pdf/2408.17185v1) | N/A | Short-term Wind Speed Forecasting for Power Integration in Smart Grids based on Hybrid LSSVM-SVMD Method | Owing to its minimal pollution and efficient energy use, wind energy has become one of the most widely exploited renewable energy resources. The successful integration of wind power into the grid system is contingent upon accurate wind speed forecasting models. However, the task of wind speed forecasting is challenging due to the inherent intermittent characteristics of wind speed. In this paper, a hybrid machine learning approach is developed for predicting short-term wind speed. First, the wind data was decomposed into modal components using Successive Variational Mode Decomposition (SVMD). Then, each sub-signal was fitted into a Least Squares Support Vector Machines (LSSVM) model, with its hyperparameter optimized by a novel variant of Quantum-behaved Particle Swarm Optimization (QPSO), QPSO with elitist breeding (EBQPSO). Second, the residuals making up for the differences between the original wind series and the aggregate of the SVMD modes were modeled using long short-term model (LSTM). Then, the overall predicted values were computed using the aggregate of the LSSVM and the LSTM models. Finally, the performance of the proposed model was compared against state-of-the-art benchmark models for forecasting wind speed using two separate data sets collected from a local wind farm. Empirical results show significant improvement in performance by the proposed method, achieving a 1.21% to 32.76% reduction in root mean square error (RMSE) and a 2.05% to 40.75% reduction in mean average error (MAE) compared to the benchmark methods. The entire code implementation of this work is freely available in Github. |
| 改进从电子健康记录中提取临床事件上下文属性的方法：一项比较研究 | 电子健康记录是包含大量有价值临床数据的大型存储库，其中很大一部分以非结构化文本格式存储。这些文本数据包括临床事件（例如，疾病、症状、发现、药物和程序），如果能够大规模准确提取，将能够解锁有价值的下游应用，如疾病预测。使用现有的命名实体识别和链接方法，MedCAT，这些识别的概念需要进一步分类（上下文化），以评估其对患者的相关性，以及它们的时间状态和否定状态，以便在下游应用中发挥作用。本研究对多种自然语言模型在医学文本分类方面进行了比较分析。广泛的实验表明，基于变换器的语言模型，特别是BERT，具有显著的有效性。当结合类别不平衡缓解技术时，BERT在少数类别的召回率上比Bi-LSTM模型高出最多28%，比基准BERT模型高出最多16%。该方法已作为CogStack/MedCAT框架的一部分实施，并向社区开放，供进一步研究使用。 | Shubham Agarwal | [PDF](http://arxiv.org/pdf/2408.17181v1) | N/A | Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study | Electronic Health Records are large repositories of valuable clinical data, with a significant portion stored in unstructured text format. This textual data includes clinical events (e.g., disorders, symptoms, findings, medications and procedures) in context that if extracted accurately at scale can unlock valuable downstream applications such as disease prediction. Using an existing Named Entity Recognition and Linking methodology, MedCAT, these identified concepts need to be further classified (contextualised) for their relevance to the patient, and their temporal and negated status for example, to be useful downstream. This study performs a comparative analysis of various natural language models for medical text classification. Extensive experimentation reveals the effectiveness of transformer-based language models, particularly BERT. When combined with class imbalance mitigation techniques, BERT outperforms Bi-LSTM models by up to 28% and the baseline BERT model by up to 16% for recall of the minority classes. The method has been implemented as part of CogStack/MedCAT framework and made available to the community for further research. |
| 识别和聚类PvP游戏中队伍构成的对抗关系，以实现高效的平衡分析 | 如何在游戏设置中量化平衡？这一问题对游戏设计师至关重要，尤其是在玩家对玩家（PvP）游戏中，分析预定义团队组合之间的力量关系——如多人在线战斗竞技场（MOBA）游戏中的英雄组合或卡牌游戏中的套牌——对于提升游戏体验和实现平衡至关重要。我们开发了两种高级度量方法，超越了简单的胜率，用于量化零和竞争场景中的平衡。这些度量方法源自胜价值估计，通过采用布拉德利-特里模型进行强度评级近似和通过向量量化进行对抗关系近似，显著降低了与传统胜价值估计相关的计算复杂性。在这些模型的学习过程中，我们识别了有用的组合类别并指出了它们的对抗关系，与人类玩家的体验相符，而无需特定游戏知识。我们的方法依赖于一种简单技术，通过确定性向量量化过程，在极小状态空间的离散表示中增强码本利用率。我们的框架已在多款热门在线游戏中得到验证，包括《帝国时代II》、《炉石传说》、《荒野乱斗》和《英雄联盟》。在这些游戏中观察到的强度关系准确性与传统成对胜价值预测相当，同时提供了更易于管理的分析复杂性。最终，我们的研究成果加深了对PvP游戏动态的理解，并提出了一种显著改进游戏平衡评估与设计的方法。 | Chiu-Chou Lin | [PDF](http://arxiv.org/pdf/2408.17180v1) | N/A | Identifying and Clustering Counter Relationships of Team Compositions in PvP Games for Efficient Balance Analysis | How can balance be quantified in game settings? This question is crucial for game designers, especially in player-versus-player (PvP) games, where analyzing the strength relations among predefined team compositions-such as hero combinations in multiplayer online battle arena (MOBA) games or decks in card games-is essential for enhancing gameplay and achieving balance. We have developed two advanced measures that extend beyond the simplistic win rate to quantify balance in zero-sum competitive scenarios. These measures are derived from win value estimations, which employ strength rating approximations via the Bradley-Terry model and counter relationship approximations via vector quantization, significantly reducing the computational complexity associated with traditional win value estimations. Throughout the learning process of these models, we identify useful categories of compositions and pinpoint their counter relationships, aligning with the experiences of human players without requiring specific game knowledge. Our methodology hinges on a simple technique to enhance codebook utilization in discrete representation with a deterministic vector quantization process for an extremely small state space. Our framework has been validated in popular online games, including Age of Empires II, Hearthstone, Brawl Stars, and League of Legends. The accuracy of the observed strength relations in these games is comparable to traditional pairwise win value predictions, while also offering a more manageable complexity for analysis. Ultimately, our findings contribute to a deeper understanding of PvP game dynamics and present a methodology that significantly improves game balance evaluation and design. |
| 编解码器确实重要：探究音频语言模型中编解码器的语义缺陷 | 近年来，音频生成领域的进展在很大程度上得益于大型语言模型（LLM）的能力。现有关于音频LLM的研究主要集中在增强音频语言模型的架构和规模，利用更大的数据集，并且通常使用声学编解码器，如EnCodec，来进行音频标记化。然而，这些编解码器最初设计用于音频压缩，这可能导致在音频LLM的背景下性能不佳。我们的研究旨在解决当前音频LLM编解码器的不足，特别是它们在生成音频中保持语义完整性方面的挑战。例如，现有的方法如VALL-E，在根据文本转录条件生成声学标记时，常常因声学标记的语义误解而导致内容不准确和单词错误率（WER）上升，出现跳词和错误。为了克服这些问题，我们提出了一种直接而有效的方法，称为X-Codec。X-Codec在残差向量量化（RVQ）阶段之前结合了预训练语义编码器的语义特征，并在RVQ之后引入了语义重建损失。通过增强编解码器的语义能力，X-Codec显著降低了语音合成任务中的WER，并将这些优势扩展到非语音应用，包括音乐和声音生成。我们在文本到语音、音乐延续和文本到声音任务中的实验表明，整合语义信息显著提高了语言模型在音频生成中的整体性能。我们的代码和演示已公开（演示：https://x-codec-audio.github.io 代码：https://github.com/zhenye234/xcodec）。 | Zhen Ye | [PDF](http://arxiv.org/pdf/2408.17175v1) | N/A | Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model | Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec) |
| SafeTail：通过计算冗余管理实现边缘服务调度中的高效尾部延迟优化 | 在边缘计算中，优化尾部延迟同时高效管理计算资源对于提供高性能、延迟敏感的服务至关重要。新兴应用，如增强现实，要求在用户设备上提供低延迟、高可靠性的计算服务，而这些设备往往计算能力有限。因此，这些设备依赖附近的边缘服务器进行处理。然而，由于无线网络的可变性和服务器负载的波动，网络和计算延迟存在固有的不确定性，使得按时交付服务变得具有挑战性。现有的方法通常专注于优化中位延迟，但在解决边缘环境中尾部延迟的具体挑战方面表现不足，尤其是在网络和计算条件不确定的情况下。尽管一些方法确实解决了尾部延迟问题，但它们通常依赖于固定或过度的冗余，缺乏对动态网络条件的适应性，并且往往是为云环境设计的，而不是针对边缘计算的独特需求。在本文中，我们介绍了SafeTail框架，该框架同时满足中位和尾部响应时间目标，其中尾部延迟定义为超过第90百分位阈值的延迟。SafeTail通过在多个边缘服务器上选择性复制服务来应对这一挑战，以满足目标延迟。SafeTail采用基于奖励的深度学习框架来学习最优放置策略，平衡实现目标延迟和最小化额外资源使用的需求。通过跟踪驱动的模拟，SafeTail展示了接近最优的性能，并在三种不同服务中优于大多数基线策略。 | Jyoti Shokhanda | [PDF](http://arxiv.org/pdf/2408.17171v1) | N/A | SafeTail: Efficient Tail Latency Optimization in Edge Service Scheduling via Computational Redundancy Management | Optimizing tail latency while efficiently managing computational resources is crucial for delivering high-performance, latency-sensitive services in edge computing. Emerging applications, such as augmented reality, require low-latency computing services with high reliability on user devices, which often have limited computational capabilities. Consequently, these devices depend on nearby edge servers for processing. However, inherent uncertainties in network and computation latencies stemming from variability in wireless networks and fluctuating server loads make service delivery on time challenging. Existing approaches often focus on optimizing median latency but fall short of addressing the specific challenges of tail latency in edge environments, particularly under uncertain network and computational conditions. Although some methods do address tail latency, they typically rely on fixed or excessive redundancy and lack adaptability to dynamic network conditions, often being designed for cloud environments rather than the unique demands of edge computing. In this paper, we introduce SafeTail, a framework that meets both median and tail response time targets, with tail latency defined as latency beyond the 90^th percentile threshold. SafeTail addresses this challenge by selectively replicating services across multiple edge servers to meet target latencies. SafeTail employs a reward-based deep learning framework to learn optimal placement strategies, balancing the need to achieve target latencies with minimizing additional resource usage. Through trace-driven simulations, SafeTail demonstrated near-optimal performance and outperformed most baseline strategies across three diverse services. |
